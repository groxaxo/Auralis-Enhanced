

================================================================================
# File: combine_files.py
# Path: /home/marco/PycharmProjects/betterVoiceCraft/Auralis/src/combine_files.py
================================================================================

import os
from pathlib import Path


def combine_python_files(root_dir: str, output_file: str):
    """
    Ricorsivamente trova tutti i file .py e li appende a un unico file
    con riferimento al path originale.

    Args:
        root_dir: Directory da cui partire per la ricerca
        output_file: File di output dove verranno combinati i risultati
    """
    with open(output_file, 'w', encoding='utf-8') as outfile:
        for root, _, files in os.walk(root_dir):
            for file in files:
                if file.endswith('.py'):
                    file_path = Path(root) / file
                    relative_path = file_path.relative_to(root_dir)

                    outfile.write(f"\n\n{'=' * 80}\n")
                    outfile.write(f"# File: {relative_path}\n")
                    outfile.write(f"# Path: {file_path}\n")
                    outfile.write('=' * 80 + "\n\n")

                    try:
                        with open(file_path, 'r', encoding='utf-8') as infile:
                            outfile.write(infile.read())
                    except Exception as e:
                        outfile.write(f"# Error reading file: {e}\n")


if __name__ == "__main__":
    # Esempio di utilizzo
    root_directory = "/home/marco/PycharmProjects/betterVoiceCraft/Auralis/src"  # Directory corrente
    output_file = "combined_python_files.txt"
    combine_python_files(root_directory, output_file)

================================================================================
# File: auralis/__init__.py
# Path: /home/marco/PycharmProjects/betterVoiceCraft/Auralis/src/auralis/__init__.py
================================================================================

#  Copyright (c) 2024 Astramind. Licensed under Apache License, Version 2.0.

from .common.definitions.dto.output import TTSOutput
from .common.definitions.dto.requests import TTSRequest
from .common.definitions.enhancer import AudioPreprocessingConfig
from .common.logging.logger import setup_logger, set_vllm_logging_level
from .core.tts import TTS



================================================================================
# File: auralis/common/__init__.py
# Path: /home/marco/PycharmProjects/betterVoiceCraft/Auralis/src/auralis/common/__init__.py
================================================================================

#  Copyright (c) 2024 Astramind. Licensed under Apache License, Version 2.0.



================================================================================
# File: auralis/common/utilities.py
# Path: /home/marco/PycharmProjects/betterVoiceCraft/Auralis/src/auralis/common/utilities.py
================================================================================

#  Copyright (c) 2024 Astramind. Licensed under Apache License, Version 2.0.

from typing import Union, Callable, Dict, Any

import fsspec
import torch
import torchaudio


def wav_to_mel_cloning(
        wav,
        mel_norms_file="../experiments/clips_mel_norms.pth",
        mel_norms=None,
        device=torch.device("cpu"),
        n_fft=4096,
        hop_length=1024,
        win_length=4096,
        power=2,
        normalized=False,
        sample_rate=22050,
        f_min=0,
        f_max=8000,
        n_mels=80,
):
    mel_stft = torchaudio.transforms.MelSpectrogram(
        n_fft=n_fft,
        hop_length=hop_length,
        win_length=win_length,
        power=power,
        normalized=normalized,
        sample_rate=sample_rate,
        f_min=f_min,
        f_max=f_max,
        n_mels=n_mels,
        norm="slaney",
    ).to(device)
    wav = wav.to(device)
    mel = mel_stft(wav)
    mel = torch.log(torch.clamp(mel, min=1e-5))
    if mel_norms is None:
        mel_norms = torch.load(mel_norms_file, map_location=device)
    mel = mel / mel_norms.unsqueeze(0).unsqueeze(-1)
    return mel


def load_audio(audiopath, sampling_rate):
    audio, lsr = torchaudio.load(audiopath)

    # Stereo to mono if needed
    if audio.size(0) != 1:
        audio = torch.mean(audio, dim=0, keepdim=True)

    if lsr != sampling_rate:
        audio = torchaudio.functional.resample(audio, lsr, sampling_rate)

    # Clip audio invalid values
    audio.clip_(-1, 1)
    return audio

def load_fsspec(
    path: str,
    map_location: Union[str, Callable, torch.device, Dict[Union[str, torch.device], Union[str, torch.device]]] = None,
    **kwargs,
) -> Any:
    """Like torch.load but can load from other locations (e.g. s3:// , gs://).

    Args:
        path: Any path or url supported by fsspec.
        map_location: torch.device or str.
        **kwargs: Keyword arguments forwarded to torch.load.

    Returns:
        Object stored in path.
    """
    with fsspec.open(path, "rb") as f:
            return torch.load(f, map_location=map_location, **kwargs)


================================================================================
# File: auralis/common/scheduling/scheduler.py
# Path: /home/marco/PycharmProjects/betterVoiceCraft/Auralis/src/auralis/common/scheduling/scheduler.py
================================================================================

import asyncio
import gc
from contextlib import asynccontextmanager
from typing import Callable, Union, List

import objgraph
from memory_profiler import profile
import torch

from auralis import setup_logger, TTSOutput
from auralis.common.definitions.scheduler.context import GenerationContext
from auralis.common.metrics.profiling import profile_cuda, CUDALineProfiler
from auralis.common.scheduling.collective_event import CounterEvent
from auralis.common.scheduling.queues import BatchQueue
from auralis.common.scheduling.dynamic_resource_lock import DynamicResourceLock

logger = setup_logger(__name__)

AURALIS_SCHEDULER_TIMEOUT = 30

class AsyncScheduler:
    def __init__(self, max_batch_size: int, processing_function: Callable, stage_name: str):
        """
        Initialize an AsyncScheduler.

        Parameters
        ----------
        max_batch_size : DynamicResourceLock
            The lock that controls the resources available to the scheduler.
        processing_function : Callable
            The function that will be called to process each item in the queue.
        stage_name : str
            The name of the stage that this scheduler represents.
        """
        self._max_batch_size = max_batch_size
        self._remaining_capacity = max_batch_size
        self._capacity_lock = asyncio.Lock() # Lock to ensure thread-safe batch retrieval

        self.processing_function = processing_function
        self.stage_name = stage_name
        self.queue = BatchQueue(stage_name)

        #self.profiler = CUDALineProfiler(__file__)
    @asynccontextmanager
    async def cuda_memory_manager(self):
        """
        Context manager to manage CUDA memory.

        This context manager ensures that the CUDA memory is deallocated
        after the code block is finished. It also ensures that the memory
        is deallocated even if an exception is raised.

        Notes:
            - This context manager is designed to be used with the `async with`
              statement. It is not thread-safe and should not be used with
              the `with` statement.
            - The `torch.cuda.empty_cache()` call is necessary to deallocate
              the memory, but it can take some time. To avoid blocking the
              event loop, we use `asyncio.sleep(0.1)` to give other tasks a
              chance to run.
        """
        try:
            yield
        finally:
            torch.cuda.synchronize()
            await asyncio.sleep(0.1)
            torch.cuda.empty_cache()

    def get_next_stage(self, stage: str):
        stages = ['conditioning', 'phonetic', 'synthesis', 'completed']
        return stages[stages.index(stage) + 1]

    def print_memory_statemts(self):
        curr_memory = torch.cuda.max_memory_allocated()
        print(f"{torch.cuda.max_memory_allocated() / 1024**3} GB | incremented by {curr_memory-self._last_memory_consumed / 1024**2} MB")
        self._last_memory_consumed = curr_memory


    async def _process_element_in_batch(self, input_data: GenerationContext, completion_event: CounterEvent) -> \
        Union[Union[GenerationContext, TTSOutput], List[Union[GenerationContext, TTSOutput]]]:
        """
        Process an element in a batch.

        Parameters
        ----------
        input_data : GenerationContext
            The data to be processed.

        Returns
        -------
        new_outputs : List[GenerationContext | TTSOutput] | GenerationContext | TTSOutput
            The output of the current stage.

        """

        new_outputs = await self.processing_function(input_data)

        async with self._capacity_lock:
            self._remaining_capacity += input_data.length(self.stage_name)
        if isinstance(new_outputs, list):
            for new_output in new_outputs:
                logger.debug(f"[{self.stage_name}] "
                             f"Finished processing request {input_data.request_id}"
                             f"(part of {input_data.parent_request_id})")
                new_output.stage = self.get_next_stage(input_data.stage)
                # We asser this since the TTSOutput obj does not have a completion event and
                # we must set hte input event in the orchestrator but in this case a single input
                # yields multiple outputs
                assert new_output.stage != "completed", "TTSOutputs should not be sent as a list"
            del input_data
            return new_outputs

        logger.debug(f"[{self.stage_name}] "
                         f"Finished processing request {input_data.request_id}"
                         f"(part of {input_data.parent_request_id})")

        new_outputs.stage = self.get_next_stage(input_data.stage)
        if new_outputs.stage == "completed":
            await completion_event.set()
        del input_data
        return new_outputs

    async def process(self, orchestrator: 'Orchestrator'):
        """
        Process the items in the input queue.

        This method runs indefinitely, pulling items from the input queue and
        processing them with the provided processing function. The output of the
        processing function is then put back into the main queue with the next
        stage and output.

        Parameters
        ----------
        orchestrator : Orchestrator
            The orchestrator that owns this scheduler.

        Returns
        -------
        None
        """
        logger.debug(f"Starting processing loop for stage {self.stage_name}")

        try:
            while True:
                async with self._capacity_lock:
                    batch_list, self._remaining_capacity = await self.queue.get_batch(self._remaining_capacity)

                if not batch_list:
                    await asyncio.sleep(0.1)
                    continue

                # Crea una lista di coroutine invece di creare manualmente i task
                coroutines = [
                    self._process_element_in_batch(input_data, completion_event) for
                    input_data, completion_event in batch_list
                ]

                # Esegui tutte le coroutine in parallelo con gather
                results = await asyncio.gather(*coroutines, return_exceptions=True)

                # Gestisci i risultati
                for new_outputs, (_, completion) in zip(results, batch_list):
                    if isinstance(new_outputs, Exception):
                        # Gestione dell'eccezione: log, skip, ecc.
                        logger.error(f"Processing failed with error: {new_outputs}")
                        continue

                    # Se Ã¨ una lista di output
                    if isinstance(new_outputs, list):
                        for output in new_outputs:
                            await orchestrator.queue.put((output, completion))
                            del output
                    else:
                        await orchestrator.queue.put((new_outputs, completion))

                    del new_outputs

                # Pulizia del batch
                for item in batch_list:
                    item = None
                batch_list.clear()

        except Exception as e:
            logger.error(f"Error in process loop: {e}")
            raise
        finally:
            gc.collect()


================================================================================
# File: auralis/common/scheduling/__init__.py
# Path: /home/marco/PycharmProjects/betterVoiceCraft/Auralis/src/auralis/common/scheduling/__init__.py
================================================================================

#  Copyright (c) 2024 Astramind. Licensed under Apache License, Version 2.0.



================================================================================
# File: auralis/common/scheduling/orchestrator.py
# Path: /home/marco/PycharmProjects/betterVoiceCraft/Auralis/src/auralis/common/scheduling/orchestrator.py
================================================================================

import asyncio
from collections import deque
from collections.abc import AsyncGenerator
from copy import deepcopy
from typing import Callable, Union

import torch

from auralis import TTSOutput, setup_logger
from auralis.common.definitions.dto.requests import TTSRequest
from auralis.common.definitions.scheduler.context import GenerationContext
from auralis.common.metrics.performance import track_generation
from auralis.common.scheduling.collective_event import CounterEvent
from auralis.common.scheduling.dynamic_resource_lock import DynamicResourceLock
from auralis.common.scheduling.profiler import Profiler
from auralis.common.scheduling.queues import QueueWithLeftPut
from auralis.common.scheduling.scheduler import AsyncScheduler
from auralis.models.base import BaseAsyncTTSEngine

logger = setup_logger(__name__)

class Orchestrator:
    def __init__(self, engine: BaseAsyncTTSEngine):
        """
        Initialize the Orchestrator with the given TTS engine.

        This constructor sets up the necessary schedulers and profiling for
        the various phases of text-to-speech processing, including conditioning,
        phonetics, and synthesis. It configures these schedulers based on the
        engine's configuration, which includes concurrency settings and maximum sizes.

        Args:
            engine (BaseAsyncTTSEngine): The engine used for TTS processing. It
                provides the functions for each processing phase and configuration
                details.

        Attributes:
            schedulers (list): A list of AsyncScheduler instances for managing
                different phases of TTS processing.
            preprocessing_phase_fn (Callable): Function to preprocess input data.
            queue (asyncio.Queue): Queue to manage requests for processing.
            scheduler_tasks (list): List of tasks handling scheduling.
            processing_task (Optional[asyncio.Task]): Task for processing the queue.
        """
        conditioning_phase_fn = engine.conditioning_phase
        phonetics_phase_fn = engine.phonetic_phase
        synthesis_phase_fn = engine.speech_phase
        eng_config = engine.config

        fake_data_factories = engine.info['fake_data_factories']
        concurrences = eng_config['concurrences']
        max_sizes = eng_config['max_sizes']

        self.schedulers = [
            AsyncScheduler(
                max_sizes[0] * concurrences[0],
                conditioning_phase_fn,
                "conditioning"
            ),
            AsyncScheduler(
                concurrences[1],
                phonetics_phase_fn,
                "phonetic"
            ),
            AsyncScheduler(
                max_sizes[2] * concurrences[2],
                synthesis_phase_fn,
                "synthesis"
            )
        ]
        self.preprocessing_phase_fn = engine.preprocess_inputs

        Profiler.profile(
            fake_data_factories,
            (conditioning_phase_fn, phonetics_phase_fn, synthesis_phase_fn),
            eng_config
        )

        self.queue: QueueWithLeftPut[Union[GenerationContext, TTSOutput]] = QueueWithLeftPut()  # Single queue for all requests
        self.scheduler_tasks = []
        self.processing_task = None  # Task for processing the queue

    async def start_schedulers(self):
        self.scheduler_tasks = [
            asyncio.create_task(s.process(self)) for s in self.schedulers
        ]

    @torch.inference_mode()
    async def run(self, request: TTSRequest) -> AsyncGenerator[TTSOutput, None]:
        if not self.scheduler_tasks:
            await self.start_schedulers()
        #asyncio.create_task(monitor_memory())
        logger.info(f"(Auralis) Starting request {request.request_id}")

        request_id = request.request_id
        input_data = await self.preprocessing_phase_fn(request)
        completion_event = CounterEvent()
        for input_element in input_data:
            await self.queue.put((input_element, deepcopy(completion_event)))

        if self.processing_task is None or self.processing_task.done():
            self.processing_task = asyncio.create_task(self.process_queue())

        try:
            while True:
                item, completion_event = await self.queue.get()

                if item.parent_request_id == request_id and completion_event.is_set():
                    yield item
                    self.queue.task_done()
                    if completion_event.is_collectively_set():
                        del completion_event # Clear the event
                        logger.info(f"Request {request_id} finished")
                        break
                else:
                    # reinsert the item at the front of the queue
                    await self.queue.put_front((item, completion_event))
                    await asyncio.sleep(0.1)

        except Exception as e:
            logger.error(f"Error in request {request_id}: {e}")
            raise

        finally:
            # empty the queue
            while not self.queue.empty():
                await self.queue.get()

    async def process_queue(self):
        while True:
            # Get the next item from the queue (non-blocking)
            item, completion_event = await self.queue.get()

            if not isinstance(item, TTSOutput):
                # Find the appropriate scheduler and put the item in its input queue
                for scheduler in self.schedulers:
                    if scheduler.stage_name == item.stage:
                        logger.debug(f"Forwarding request {item.request_id}"
                                     f"(part of {item.parent_request_id}) to scheduler '{scheduler.stage_name}'")
                        await scheduler.queue.put((item, completion_event))
                        break  # Important: break out of the inner loop after forwarding the item
                else:  # This 'else' clause is associated with the 'for' loop
                    # If no scheduler was found for the current stage, put the item back in the queue
                    await self.queue.put((item, completion_event))

            else:
                # If completed, put it back for the run() method to pick up
                await self.queue.put((item, completion_event))
                await asyncio.sleep(0.1) # Give it a chance to be picked up from run()

            self.queue.task_done()

    async def shutdown(self):
        if self.processing_task:
            self.processing_task.cancel()
            await asyncio.gather(self.processing_task, return_exceptions=True)
        for task in self.scheduler_tasks:
            task.cancel()
        await asyncio.gather(*self.scheduler_tasks, return_exceptions=True)

        self.scheduler_tasks.clear()
        self.processing_task = None
        torch.cuda.empty_cache()





================================================================================
# File: auralis/common/scheduling/two_phase_scheduler.py
# Path: /home/marco/PycharmProjects/betterVoiceCraft/Auralis/src/auralis/common/scheduling/two_phase_scheduler.py
================================================================================

#  Copyright (c) 2024 Astramind. Licensed under Apache License, Version 2.0.

import asyncio
import time
from contextlib import asynccontextmanager
from typing import Any, Dict, AsyncGenerator, Callable, Awaitable

from auralis.common.definitions.scheduler import QueuedRequest, TaskState
from auralis.common.logging.logger import setup_logger


class TwoPhaseScheduler:
    def __init__(
            self,
            second_phase_concurrency: int = 10,
            request_timeout: float = None,
            generator_timeout: float = None
    ):
        # Core configuration
        self.second_phase_concurrency = second_phase_concurrency
        self.request_timeout = request_timeout
        self.generator_timeout = generator_timeout
        self.logger = setup_logger(__file__)

        # State management
        self.is_running = False
        self.request_queue = None
        self.active_requests = {}
        self.queue_processor_tasks = []
        self.cancel_warning_issued = False

        # Concurrency controls
        self.second_phase_sem = None
        self.active_generator_count = 0
        self.generator_count_lock = asyncio.Lock()
        self.cleanup_lock = asyncio.Lock()

    async def start(self):
        if self.is_running:
            return

        self.request_queue = asyncio.Queue()
        self.second_phase_sem = asyncio.Semaphore(self.second_phase_concurrency)
        self.is_running = True
        self.queue_processor_tasks = [
            asyncio.create_task(self._process_queue())
            for _ in range(self.second_phase_concurrency)
        ]

    async def _process_queue(self):
        """Continuously process requests from the queue."""
        while self.is_running:
            try:
                request = await self.request_queue.get()
                if request.state == TaskState.QUEUED:
                    async with self._request_lifecycle(request.id):
                        self.active_requests[request.id] = request
                        await self._process_request(request)
            except asyncio.CancelledError:
                if not self.cancel_warning_issued:
                    self.logger.warning("Queue processing task cancelled")
                    self.cancel_warning_issued = True
                break
            except Exception as e:
                self.logger.error(f"Queue processing error: {e}")
                await asyncio.sleep(1)

    @asynccontextmanager
    async def _request_lifecycle(self, request_id: str):
        try:
            yield
        finally:
            async with self.cleanup_lock:
                self.active_requests.pop(request_id, None)

    async def _process_request(self, request: QueuedRequest):
        """Handle the two-phase processing of a request."""
        try:
            self.logger.info(f"Starting request {request.id}")
            # Phase 1: Initial processing
            await self._handle_first_phase(request)

            # Phase 2: Parallel processing
            await self._handle_second_phase(request)

            if not request.error:
                request.state = TaskState.COMPLETED
                self.logger.info(f"Request {request.id} completed")

        except Exception as e:
            request.error = e
            request.state = TaskState.FAILED
            self.logger.error(f"Request {request.id} failed: {e}")
        finally:
            request.completion_event.set()

    async def _handle_first_phase(self, request: QueuedRequest):
        """Execute the first phase of processing."""
        request.state = TaskState.PROCESSING_FIRST
        try:
            request.first_phase_result = await asyncio.wait_for(
                request.first_fn(request.input),
                timeout=self.request_timeout
            )
            request.generators_count = len(request.first_phase_result.get('parallel_inputs', []))
            # Initialize sequence_buffers here
            request.sequence_buffers = {i: [] for i in range(request.generators_count)}
            request.state = TaskState.PROCESSING_SECOND
        except asyncio.TimeoutError:
            raise TimeoutError(f"First phase timeout after {self.request_timeout}s")

    async def _handle_second_phase(self, request: QueuedRequest):
        """Execute the second phase of processing."""
        parallel_inputs = request.first_phase_result.get('parallel_inputs', [])
        generator_tasks = [
            asyncio.create_task(self._process_generator(request, gen_input, idx))
            for idx, gen_input in enumerate(parallel_inputs)
        ]

        try:
            await asyncio.wait_for(
                asyncio.gather(*generator_tasks, return_exceptions=True),
                timeout=self.request_timeout
            )
        except asyncio.TimeoutError:
            for task in generator_tasks:
                if not task.done():
                    task.cancel()
            raise TimeoutError(f"Second phase timeout after {self.request_timeout}s")

    async def _process_generator(
            self,
            request: QueuedRequest,
            generator_input: Any,
            sequence_idx: int,
    ):
        async with self.second_phase_sem:
            try:
                await self._init_generator(request, sequence_idx)
                await self._run_generator(request, generator_input, sequence_idx)
            except asyncio.CancelledError:
                self.logger.warning(f"Generator {sequence_idx} cancelled for request {request.id}")
                raise
            except Exception as e:
                self._handle_generator_error(request, sequence_idx, e)
            finally:
                await self._cleanup_generator(request, sequence_idx)

    async def _init_generator(self, request: QueuedRequest, sequence_idx: int):
        async with self.generator_count_lock:
            self.active_generator_count += 1
            if not hasattr(request, 'generator_events'):
                request.generator_events = {}
            request.generator_events[sequence_idx] = asyncio.Event()

    async def _run_generator(self, request: QueuedRequest, generator_input: Any, sequence_idx: int):
        generator = request.second_fn(generator_input)
        buffer = request.sequence_buffers[sequence_idx]

        while True:
            try:
                item = await asyncio.wait_for(
                    generator.__anext__(),
                    timeout=self.generator_timeout
                )

                event = asyncio.Event()
                event.set()
                buffer.append((item, event))
            except StopAsyncIteration:
                self.logger.debug(f"Generator {sequence_idx} completed for request {request.id}")
                break
            except asyncio.TimeoutError:
                raise TimeoutError(f"Generator {sequence_idx} timed out")

    def _handle_generator_error(self, request: QueuedRequest, sequence_idx: int, error: Exception):
        self.logger.error(f"Generator {sequence_idx} failed for request {request.id}: {error}")
        if request.error is None:
            request.error = error

    async def _cleanup_generator(self, request: QueuedRequest, sequence_idx: int):
        async with self.generator_count_lock:
            self.active_generator_count -= 1
            request.completed_generators += 1
            if sequence_idx in request.generator_events:
                request.generator_events[sequence_idx].set()

    async def _yield_ordered_outputs(self, request: QueuedRequest) -> AsyncGenerator[Any, None]:
        """Yield outputs in sequence order."""
        current_index = 0
        last_progress = time.time()

        while not self._is_processing_complete(request):
            if self._check_timeout(last_progress):
                raise TimeoutError("No progress in output generation")

            if request.error:
                raise request.error

            if current_index in request.sequence_buffers:
                buffer = request.sequence_buffers[current_index]
                if buffer:
                    item, event = buffer[0]
                    try:
                        await asyncio.wait_for(event.wait(), timeout=self.generator_timeout)
                        yield item
                        buffer.pop(0)
                        last_progress = time.time()
                    except asyncio.TimeoutError:
                        raise TimeoutError(f"Timeout waiting for item in sequence {current_index}")


                    current_index += 1


            await asyncio.sleep(0.01)

    def _is_processing_complete(self, request: QueuedRequest) -> bool:
        return (request.state in (TaskState.COMPLETED, TaskState.FAILED) and
                request.completed_generators >= request.generators_count and
                all(len(buffer) == 0 for buffer in request.sequence_buffers.values()))

    def _check_timeout(self, last_progress: float) -> bool:
        return self.request_timeout and time.time() - last_progress > self.request_timeout

    def _can_advance_sequence(self, request: QueuedRequest, current_index: int) -> bool:
        return (hasattr(request, 'generator_events') and
                current_index in request.generator_events and
                request.generator_events[current_index].is_set())

    async def run(
            self,
            inputs: Any,
            first_phase_fn: Callable[[Any], Awaitable[Any]],
            second_phase_fn: Callable[[Dict], AsyncGenerator],
            request_id: str = None,
    ) -> AsyncGenerator[Any, None]:
        if not self.is_running:
            await self.start()

        request = QueuedRequest(
            id=request_id,
            input=inputs,
            first_fn=first_phase_fn,
            second_fn=second_phase_fn
        )

        await self.request_queue.put(request)

        try:
            async for item in self._yield_ordered_outputs(request):
                yield item

            await asyncio.wait_for(
                request.completion_event.wait(),
                timeout=self.request_timeout
            )
            if request.error:
                raise request.error

        finally:
            async with self.cleanup_lock:
                self.active_requests.pop(request.id, None)

    async def shutdown(self):
        self.is_running = False

        for task in self.queue_processor_tasks:
            if task and not task.done():
                task.cancel()

        await asyncio.gather(*self.queue_processor_tasks, return_exceptions=True)

        if self.active_requests:
            await asyncio.gather(
                *(request.completion_event.wait() for request in self.active_requests.values()),
                return_exceptions=True
            )

================================================================================
# File: auralis/common/scheduling/profiler.py
# Path: /home/marco/PycharmProjects/betterVoiceCraft/Auralis/src/auralis/common/scheduling/profiler.py
================================================================================

import asyncio
import gc
import threading
import torch
from auralis import setup_logger
from auralis.common.definitions.types.scheduler import FakeFactoriesForSchedulerProfiling

logger = setup_logger(__file__)

class Profiler:
    @staticmethod
    def profile(fake_factories: FakeFactoriesForSchedulerProfiling, profiling_functions, config):

        async def consume_asyncgen(asyncgen):
            async for _ in asyncgen:
                pass

        async def run_profiling():
            logger.info("Starting Auralis profiling...")

            initial_memory = torch.cuda.memory_allocated()
            initial_memory_gb = initial_memory / (1024 ** 3)
            torch.cuda.reset_peak_memory_stats()
            torch.cuda.empty_cache()
            tasks = []
            for function_data, function in zip(fake_factories, profiling_functions):
                if not function_data:
                    # for vllm is useless
                    continue
                data = function_data(config)
                tasks.append( asyncio.create_task(function(data))) # start concurrently

            await asyncio.gather(*tasks)
            del tasks, function_data, function

            peak_memory = torch.cuda.max_memory_allocated()
            current_memory = torch.cuda.memory_allocated()

            peak_memory_gb = peak_memory / (1024 ** 3)
            current_memory_gb = current_memory / (1024 ** 3)

            logger.info(
                f"Initial CUDA memory usage: {initial_memory_gb:.2f}GB, "
                f"Peak CUDA memory usage: {peak_memory_gb:.2f}GB, "
                f"Final CUDA memory usage: {current_memory_gb:.2f}GB, "
                f"Memory increase: {(current_memory_gb - initial_memory_gb):.2f} GB"
            )


        # We run the profiling in a separate thread to avoid blocking the main thread

        def worker():
            try:
                asyncio.run(run_profiling())
            except torch.cuda.OutOfMemoryError:
                error_message = "Profiling failed: CUDA out of memory, try reducing the concurrency"
                logger.error(error_message)
                raise RuntimeError(error_message)


        t = threading.Thread(target=worker)
        t.start()
        t.join()

        del t
        gc.collect()
        torch.cuda.empty_cache()


================================================================================
# File: auralis/common/scheduling/dynamic_resource_lock.py
# Path: /home/marco/PycharmProjects/betterVoiceCraft/Auralis/src/auralis/common/scheduling/dynamic_resource_lock.py
================================================================================

import asyncio
from contextlib import asynccontextmanager

class DynamicResourceLock:
    def __init__(self, max_size):
        """
        This class represents a dynamic resource lock that controls the
        availability of a shared resource. It provides methods to acquire and
        release the resource, as well as the current number of occupied
        resources.

        Args:
            max_size (int): The maximum capacity of the resource lock.
                            If set to a negative value, the resource is considered unlimited.

        Attributes:
            _max_length (int): The maximum capacity.
            _current_occupied (int): The current occupied size of the resource.
            _lock (asyncio.Lock): An asyncio lock to ensure thread-safe operations.
            _condition (asyncio.Condition): A condition variable for managing wait/notify.
            _active_tasks (set): A set to track active tasks.
        """
        self._max_length = max_size
        self._current_occupied = 0
        self._lock = asyncio.Lock()
        self._condition = asyncio.Condition(self._lock)
        self._active_tasks = set()


    async def acquire(self, item_size):
        """
        Release the resource occupied by the given task ID.

        This method releases the resource previously allocated by the
        `acquire` method. It must be called with the same task ID returned
        by `acquire` to ensure the correct resource is released.

        Args:
            item_size (int): The size of the resource to be acquired.

        Returns:
            task_id (str): The task ID returned by `acquire`.
        """

        async with self._lock:
            if self._max_length < 0: # illimitate resourcea
                return None

            while self._current_occupied + item_size > self._max_length:
                await self._condition.wait()

            self._current_occupied += item_size
            task_id = id(asyncio.current_task())
            self._active_tasks.add((task_id, item_size))
            return task_id

    async def release(self, task_id):
        """
        Release the resource occupied by the given task ID.

        This method releases the resource previously allocated by the
        `acquire` method. It must be called with the same task ID returned
        by `acquire` to ensure the correct resource is released.

        Args:
            task_id (str): The task ID returned by `acquire`.

        Returns:
            None
        """
        async with self._lock:
            task_entry = next((t for t in self._active_tasks if t[0] == task_id), None)
            if task_entry:
                self._active_tasks.remove(task_entry)
                self._current_occupied -= task_entry[1]
                self._condition.notify_all()

    @asynccontextmanager
    async def lock_resource(self, item_size):

        """
        A context manager that ensures only a certain amount of resources are used.

        This context manager is used to ensure that only a certain amount of
        resources are used at any given time. It is used to limit the number of

        Args:
            item_size (int): The size of the resource to be acquired.

        Yields:
            None ( the normal function flow)

        Examples:
            async with dynamic_lock.lock_resource(item_size):
                # Do something
        """
        task_id = await self.acquire(item_size)
        try:
            yield
        finally:
            if task_id is not None:
                await self.release(task_id)


================================================================================
# File: auralis/common/scheduling/collective_event.py
# Path: /home/marco/PycharmProjects/betterVoiceCraft/Auralis/src/auralis/common/scheduling/collective_event.py
================================================================================

import asyncio
import weakref

class CounterEvent:
    def __init__(self, parent=None):
        if parent is None:
            # This is the main event
            self._event = asyncio.Event()
            self._counter = 0
            self._copies = 0
            self._lock = asyncio.Lock()
            self._parent = None
            self._individual_set = False
            self._children = weakref.WeakSet()  # Track children without creating ref cycles
        else:
            # This is a copy, use parent's references
            self._event = parent._event
            self._counter = parent._counter
            self._copies = parent._copies
            self._lock = parent._lock
            self._parent = parent
            self._individual_set = False
            # Parent keeps track of this child
            parent._children.add(self)

    def __deepcopy__(self, memo):
        # When a deep copy is made, create a new instance
        # that shares references with the parent
        if self._parent is None:
            # If this is the parent, increment the copy counter
            self._copies += 1
            return CounterEvent(parent=self)
        else:
            # If this is already a copy, create a new copy from the original parent
            self._parent._copies += 1
            return CounterEvent(parent=self._parent)

    async def set(self):
        self._individual_set = True
        parent = self._parent or self
        async with parent._lock:
            parent._counter += 1
            if parent._counter == parent._copies:
                parent._event.set()

    def is_set(self):
        """Returns whether this individual event is set"""
        return self._individual_set

    def is_collectively_set(self):
        """Returns whether all events are set"""
        parent = self._parent or self
        return parent._event.is_set()

    async def wait(self):
        parent = self._parent or self
        await parent._event.wait()

    async def delete(self):
        """Explicitly cleans up all references"""
        if self._parent is None:
            # If parent, clean up all children
            for child in list(self._children):
                await child.delete()
            self._children.clear()

            # Reset all counters
            self._counter = 0
            self._copies = 0
            self._individual_set = False

            # Clean up asyncio objects
            self._event.clear()
            self._event = None
            self._lock = None
        else:
            # If child, remove from parent
            if self._parent is not None and hasattr(self._parent, '_children'):
                self._parent._children.discard(self)

        # Clean up all references
        self._event = None
        self._counter = None
        self._copies = None
        self._lock = None
        self._parent = None
        self._individual_set = None

    def __del__(self):
        """Automatic cleanup when object is deallocated"""
        if hasattr(self, '_parent') and self._parent is not None:
            if hasattr(self._parent, '_children'):
                self._parent._children.discard(self)

================================================================================
# File: auralis/common/scheduling/queues.py
# Path: /home/marco/PycharmProjects/betterVoiceCraft/Auralis/src/auralis/common/scheduling/queues.py
================================================================================

import asyncio
import collections
from asyncio import QueueFull, Queue
from typing import Tuple

from auralis import setup_logger

logger = setup_logger(__name__)


class QueueWithLeftPut(Queue):
    async def put_front(self, item):
        """Put an item at the left (front) of the queue.

        If the queue is full, wait until a free slot is available.
        """
        while self.full():
            putter = self._get_loop().create_future()
            self._putters.append(putter)
            try:
                await putter
            except:
                putter.cancel()
                try:
                    self._putters.remove(putter)
                except ValueError:
                    pass
                if not self.full() and not putter.cancelled():
                    self._wakeup_next(self._putters)
                raise
        return self.put_left_nowait(item)

    def put_left_nowait(self, item):
        """Put an item at the left of the queue without blocking.

        If no free slot is immediately available, raise QueueFull.
        """
        if self.full():
            raise QueueFull
        self._put_left(item)
        self._unfinished_tasks += 1
        self._finished.clear()
        self._wakeup_next(self._getters)

    def _put_left(self, item):
        """Put an item at the left of the internal deque."""
        self._queue.appendleft(item)

class BatchQueue(asyncio.Queue):
    def __init__(self, stage_name: str, *args, **kwargs):
        """
        Initialize a BatchQueue.

        Parameters
        ----------
        stage_name : str
            The name of the stage that this batch queue belongs to.
        *args : tuple
            Additional arguments to pass to Queue.__init__.
        **kwargs : dict
            Additional keyword arguments to pass to Queue.__init__.

        """
        super().__init__(*args, **kwargs)
        self.stage_name = stage_name
        self._current_batch = []
        self._current_length = 0
        self._batch_lock = asyncio.Lock()

    def peek(self):
        if self._queue:
            return self._queue[0]
        return None

    async def get_batch(self, remaining_capacity: int) -> Tuple[list, int]:
        async with self._batch_lock:
            # wait till non empty queue
            while self.empty():
                await asyncio.sleep(0.1)

            batch = []
            current_length = 0

            while not self.empty():
                next_item = self.peek()[0]
                if not next_item:
                    break

                next_length = next_item.length(self.stage_name)

                if not batch and next_length > remaining_capacity:
                    logger.debug(f"[{self.stage_name}] Waiting for more capacity")
                    return [], remaining_capacity

                if current_length + next_length <= remaining_capacity or not batch:
                    item = self._get()  # Use of private method to avoid race conditions
                    self.task_done()
                    batch.append(item)
                    current_length += next_length
                else:
                    break

            logger.debug(f"[{self.stage_name}] Running with batch of {len(batch)} items "
                         f"| {round((current_length / remaining_capacity * 100), 2)}% capacity")
            return batch, remaining_capacity - current_length


================================================================================
# File: auralis/common/scheduling/memory_manager.py
# Path: /home/marco/PycharmProjects/betterVoiceCraft/Auralis/src/auralis/common/scheduling/memory_manager.py
================================================================================

# Copyright (c) 2024 Astramind.
# Licensed under the Apache License, Version 2.0.

from typing import Dict, List, Optional, Tuple
import torch
import numpy as np
import logging
from asyncio import Lock

from auralis import setup_logger
from auralis.common.definitions.scheduler.memory_manager import Block, PagedAllocation

logger = setup_logger(__name__)


class AuralisMemoryManager:
    """
    Buddy-based memory allocator to minimize fragmentation and maintain stable memory usage.
    Supports multiple predefined shapes (2 or 3) defined at initialization.
    """

    def __init__(
        self,
        shapes: List[Tuple[int, int, int]],  # e.g., [(batch, seq, hidden), ...]
        available_memory: int, # in bytes
        device: str = 'cuda',
        dtype: torch.dtype = torch.float32,
    ):
        self.device = torch.device(device)
        self.dtype = dtype
        self.shapes = shapes

        # Determine max size from given shapes
        self.bytes_per_element = torch.tensor([], dtype=dtype).element_size()
        max_size = 0
        for s in self.shapes:
            b, seq, h = s
            sz = b * seq * h * self.bytes_per_element
            if sz > max_size:
                max_size = sz

        # Round to next power of two
        if max_size == 0:
            max_size = 256
        self.total_size = 2 ** int(np.ceil(np.log2(max_size)))

        # Create a memory pool for the largest shape
        # Actual allocations can be smaller; buddy system splits blocks.
        # Using the largest shape here to ensure enough space.
        # If multiple shapes differ in size, the largest dimension ensures coverage.
        # Pool is just a buffer; shapes are applied at allocation time.
        max_b, max_seq, max_h = max(self.shapes, key=lambda x: x[0]*x[1]*x[2])
        self.memory_pool = torch.zeros(
            (max_b, max_seq, max_h),
            device=self.device,
            dtype=self.dtype
        )
        self.flat_pool = self.memory_pool.view(-1)

        # Buddy system free lists: size -> list of (offset, size)
        self.free_lists = {self.total_size: [(0, self.total_size)]}

        self.allocations: Dict[int, PagedAllocation] = {}
        self._lock = Lock()

        logger.info(
            f"AuralisMemoryManager initialized with total size {self.total_size} bytes"
        )

    async def allocate(self, logical_id: int, shape: Tuple[int, int, int]) -> Optional[List[torch.Tensor]]:
        size = np.prod(shape) * self.bytes_per_element
        if size == 0:
            return [self.flat_pool[:0].view(shape)]
        block_size = 2 ** int(np.ceil(np.log2(size)))

        async with self._lock:
            blk = await self._get_free_block(block_size)
            if blk is None:
                logger.error(f"Allocation failed for shape {shape}")
                return None

            offset, _ = blk
            block_obj = Block(id=offset, size=block_size, shape=shape, is_free=False)
            alloc = PagedAllocation(
                blocks=[block_obj],
                logical_id=logical_id,
                total_size=size,
                shape=shape
            )
            self.allocations[logical_id] = alloc

            start_idx = offset // self.bytes_per_element
            end_idx = start_idx + (size // self.bytes_per_element)
            return [self.flat_pool[start_idx:end_idx].view(shape)]

    async def free(self, logical_id: int):
        async with self._lock:
            allocation = self.allocations.pop(logical_id, None)
            if not allocation:
                return
            for block in allocation.blocks:
                await self._free_block(block.id, block.size)

    async def get_allocation_info(self, logical_id: int) -> Optional[List[torch.Tensor]]:
        async with self._lock:
            alloc = self.allocations.get(logical_id)
            if not alloc:
                return None
            views = []
            esize = self.bytes_per_element
            remaining_size = alloc.total_size
            for block in alloc.blocks:
                start_idx = block.id // esize
                blk_size = min(block.size, remaining_size)
                end_idx = start_idx + (blk_size // esize)
                view = self.flat_pool[start_idx:end_idx].view(alloc.shape)
                views.append(view)
                remaining_size -= blk_size
            return views

    async def get_stats(self) -> Dict:
        async with self._lock:
            free_memory = sum(size * len(blks) for size, blks in self.free_lists.items())
            largest_block = max((size for size, blks in self.free_lists.items() if blks), default=0)
            frag = 0.0 if free_memory == 0 else 1.0 - (largest_block / free_memory)
            return {
                'total_memory': self.total_size,
                'free_memory': free_memory,
                'used_memory': self.total_size - free_memory,
                'active_allocations': len(self.allocations),
                'fragmentation': frag
            }

    async def cleanup(self):
        async with self._lock:
            try:
                del self.memory_pool
                if self.device.type == 'cuda':
                    torch.cuda.empty_cache()
            except Exception as e:
                logger.error(f"Cleanup error: {e}")

    # Buddy allocator internals
    async def _get_free_block(self, size: int) -> Optional[Tuple[int, int]]:
        if size in self.free_lists and self.free_lists[size]:
            return self.free_lists[size].pop()
        bigger = self._find_bigger_block(size)
        if bigger is None:
            return None
        offset, bsize = self.free_lists[bigger].pop()
        while bsize > size:
            bsize //= 2
            self._add_free_block(offset + bsize, bsize)
        return (offset, bsize)

    async def _free_block(self, offset: int, size: int):
        while True:
            buddy_off = offset ^ size
            fl = self.free_lists.get(size, [])
            buddy_idx = None
            for i, (o, _) in enumerate(fl):
                if o == buddy_off:
                    buddy_idx = i
                    break
            if buddy_idx is not None:
                fl.pop(buddy_idx)
                offset = min(offset, buddy_off)
                size *= 2
            else:
                self._add_free_block(offset, size)
                break

    def _add_free_block(self, offset: int, size: int):
        if size not in self.free_lists:
            self.free_lists[size] = []
        self.free_lists[size].append((offset, size))

    def _find_bigger_block(self, size: int) -> Optional[int]:
        for s in sorted(self.free_lists.keys()):
            if s > size and self.free_lists[s]:
                return s
        return None

    def __del__(self):
        try:
            del self.memory_pool
        except:
            pass


================================================================================
# File: auralis/common/vllm/__init__.py
# Path: /home/marco/PycharmProjects/betterVoiceCraft/Auralis/src/auralis/common/vllm/__init__.py
================================================================================

#  Copyright (c) 2024 Astramind. Licensed under Apache License, Version 2.0.



================================================================================
# File: auralis/common/vllm/hijack.py
# Path: /home/marco/PycharmProjects/betterVoiceCraft/Auralis/src/auralis/common/vllm/hijack.py
================================================================================

#  Copyright (c) 2024 Astramind. Licensed under Apache License, Version 2.0.

from typing import List, Optional

import torch
from auralis.common.vllm.hidden_state_collector import HiddenStatesCollector
from vllm import SamplingParams


class ExtendedSamplingParams(SamplingParams, kw_only=True):
    """Extended sampling parameters that allows additional fields while maintaining compatibility with SamplingParams.

    This class inherits from SamplingParams and allows adding new required fields
    without conflicting with the base class's optional fields ordering.
    """
    hidden_state_collector: Optional[HiddenStatesCollector] = None  # New required field
    request_id: Optional[str] = None  # New required field


class LogitsRepetitionPenalizer:
    """A logits processor that applies repetition penalty to prevent repetitive text generation."""

    def __init__(self, repetition_penalty: float):
        if repetition_penalty < 0:
            raise ValueError("Repetition penalty must be non-negative")
        self.repetition_penalty = repetition_penalty

    def __call__(self, prompt_token_ids:List[int], token_ids: List[int], logits: torch.Tensor) -> torch.Tensor:
        """Apply repetition penalty to the logits based on previous tokens."""
        # If no repetition penalty or no tokens to check, return original logits
        if self.repetition_penalty == 1.0 or (not token_ids and not prompt_token_ids):
            return logits

        # Create a mask for the repeated tokens
        repeated_tokens = torch.tensor(prompt_token_ids + token_ids,
                                       device=logits.device,
                                       dtype=torch.long)

        # Get logits of repeated tokens
        repeated_logits = logits[repeated_tokens]

        # Apply penalty: divide positive logits by penalty, multiply negative logits by penalty
        repeated_logits = torch.where(
            repeated_logits > 0,
            repeated_logits / self.repetition_penalty,
            repeated_logits * self.repetition_penalty
        )

        # Update only the logits for repeated tokens
        logits[repeated_tokens] = repeated_logits

        return logits



================================================================================
# File: auralis/common/vllm/hidden_state_collector.py
# Path: /home/marco/PycharmProjects/betterVoiceCraft/Auralis/src/auralis/common/vllm/hidden_state_collector.py
================================================================================

#  Copyright (c) 2024 Astramind. Licensed under Apache License, Version 2.0.

import threading
from concurrent.futures import ThreadPoolExecutor
from queue import Queue
from typing import Optional, Dict, List, Callable

import torch

from auralis.common.logging.logger import setup_logger


class SyncCollectorWrapper:
    """Wrapper that provides a sync interface for collection while maintaining thread safety"""
    def __init__(self, collector_fn: Callable[[torch.Tensor, str], None], request_id: str):
        self.collector_fn = collector_fn
        self.request_id = request_id

    def __call__(self, hidden_states: Optional[torch.Tensor], request_id: Optional[str] = None):
        """Sync interface for VLLM - uses stored request_id if none provided"""
        self.collector_fn(hidden_states, request_id or self.request_id)

class HiddenStatesCollector:
    def __init__(self):
        self.outputs: Dict[str, List[torch.Tensor]] = {}
        self.collection_ready: Dict[str, threading.Event] = {}
        self.collection_complete: Dict[str, threading.Event] = {}
        self.locks: Dict[str, threading.Lock] = {}
        self.global_lock = threading.Lock()
        self.logger = setup_logger(__file__)
        self.states_count: Dict[str, int] = {}
        self.expected_states: Dict[str, int] = {}
        self.notifications: Dict[str, Queue] = {}
        self.executor = ThreadPoolExecutor(max_workers=4)

    def initialize_request(self, request_id: str):
        """Synchronous initialization for request"""
        with self.global_lock:
            if request_id not in self.locks:
                self.locks[request_id] = threading.Lock()
                self.collection_ready[request_id] = threading.Event()
                self.collection_complete[request_id] = threading.Event()
                self.outputs[request_id] = []
                self.states_count[request_id] = 0
                self.expected_states[request_id] = 1
                self.notifications[request_id] = Queue()
                self.collection_ready[request_id].set()
                self.logger.debug(f"Initialized collector for request {request_id}")

    def sync_collect(self, hidden_states: Optional[torch.Tensor], request_id: str):
        """Synchronous collection method for VLLM callback"""
        if request_id not in self.collection_ready:
            self.logger.error(f"Collector not initialized for request {request_id}")
            # Initialize on demand if needed
            self.initialize_request(request_id)
            return

        try:
            with self.locks[request_id]:
                if hidden_states is not None:
                    self.outputs[request_id].append(hidden_states.clone())
                    self.states_count[request_id] += 1
                    self.logger.debug(f"Collected state {self.states_count[request_id]} for request {request_id}")

                    if self.states_count[request_id] >= self.expected_states[request_id]:
                        self.collection_complete[request_id].set()
                        self.notifications[request_id].put(True)
                else:
                    self.logger.warning(f"Received None hidden states for request {request_id}")
        except Exception as e:
            self.logger.error(f"Error collecting hidden states: {e}")
            raise

    async def get_hidden_states(self, request_id: str, timeout: float = 3.0) -> Optional[torch.Tensor]:
        """Get hidden states for a request with timeout."""
        try:
            if request_id not in self.collection_ready:
                self.logger.error(f"Request {request_id} was never initialized")
                return None

            # Wait for completion using threading.Event
            if not self.collection_complete[request_id].wait(timeout):
                return None

            with self.locks[request_id]:
                outputs = self.outputs.get(request_id, [])
                if not outputs:
                    self.logger.critical(f"No hidden states found for request {request_id}") # most likely due to wrong profiling data dimensions
                    raise ValueError(f"No hidden states found for request {request_id}, "
                                     f"this should not happen, please open an issue on github")

                try:
                    result = torch.cat(outputs, dim=0)
                    self._cleanup_request(request_id)
                    return result
                except Exception as e:
                    self.logger.error(f"Error processing hidden states: {e}")
                    raise

        except Exception as e:
            self.logger.error(f"Error retrieving hidden states: {e}")
            return None

    def _cleanup_request(self, request_id: str):
        """Clean up resources for a request."""
        with self.global_lock:
            self.outputs.pop(request_id, None)
            self.collection_ready.pop(request_id, None)
            self.collection_complete.pop(request_id, None)
            self.locks.pop(request_id, None)
            self.states_count.pop(request_id, None)
            self.expected_states.pop(request_id, None)
            self.notifications.pop(request_id, None)
            self.logger.debug(f"Cleaned up request {request_id}")

    def bind_to_request(self, request_id: str) -> SyncCollectorWrapper:
        """Create a sync wrapper for VLLM callback."""
        # Synchronous initialization
        self.initialize_request(request_id)
        # Pass request_id to wrapper so it's available even if VLLM passes None
        return SyncCollectorWrapper(
            collector_fn=lambda hs, rid: self.sync_collect(hs, rid),
            request_id=request_id
        )

================================================================================
# File: auralis/common/definitions/__init__.py
# Path: /home/marco/PycharmProjects/betterVoiceCraft/Auralis/src/auralis/common/definitions/__init__.py
================================================================================

#  Copyright (c) 2024 Astramind. Licensed under Apache License, Version 2.0.



================================================================================
# File: auralis/common/definitions/enhancer.py
# Path: /home/marco/PycharmProjects/betterVoiceCraft/Auralis/src/auralis/common/definitions/enhancer.py
================================================================================

#  Copyright (c) 2024 Astramind. Licensed under Apache License, Version 2.0.

from dataclasses import dataclass

import librosa
import numpy as np
import pyloudnorm
import torch
import torchaudio


@dataclass
class AudioPreprocessingConfig:
    sample_rate: int = 22050
    normalize: bool = True
    trim_silence: bool = True
    remove_noise: bool = True
    enhance_speech: bool = True

    # VAD parameters
    vad_threshold: float = 0.02
    vad_frame_length: int = 1024*4

    # Noise reduction
    noise_reduce_margin: float = 1.0
    noise_reduce_frames: int = 25

    # Enhancement
    enhance_amount: float = 1.0

    # Normalization target
    target_lufs: float = -18.0


class EnhancedAudioProcessor:
    def __init__(self, config: AudioPreprocessingConfig):
        self.config = config
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    @staticmethod
    @torch.no_grad()
    def get_mel_spectrogram(audio: np.ndarray, sr: int) -> torch.Tensor:
        """Compute mel spectrogram efficiently using torch."""
        audio_tensor = torch.FloatTensor(audio).unsqueeze(0)
        mel_spec = torchaudio.transforms.MelSpectrogram(
            sample_rate=sr,
            n_fft=2048,
            hop_length=512,
            n_mels=80
        )(audio_tensor)
        return torch.log(torch.clamp(mel_spec, min=1e-5))

    def vad_split(self, audio: np.ndarray) -> np.ndarray:
        """Enhanced Voice Activity Detection using energy and spectral features."""
        # Compute short-time energy
        frame_length = self.config.vad_frame_length
        frames = librosa.util.frame(audio, frame_length=frame_length, hop_length=frame_length // 2)
        energy = np.sum(frames ** 2, axis=0)
        energy = energy / np.max(energy)

        # Compute spectral features
        mel_spec = self.get_mel_spectrogram(audio, self.config.sample_rate)
        spectral_sum = torch.sum(mel_spec, dim=1).numpy().squeeze()
        spectral_sum = spectral_sum / np.max(spectral_sum)

        # Resize signals to match
        if len(energy) > len(spectral_sum):
            # Interpolate spectral_sum to match energy length
            spectral_sum = np.interp(
                np.linspace(0, 1, len(energy)),
                np.linspace(0, 1, len(spectral_sum)),
                spectral_sum
            )
        else:
            # Interpolate energy to match spectral_sum length
            energy = np.interp(
                np.linspace(0, 1, len(spectral_sum)),
                np.linspace(0, 1, len(energy)),
                energy
            )

        # Combine features
        vad_signal = (energy + spectral_sum) / 2
        vad_mask = np.absolute(vad_signal) > self.config.vad_threshold

        # Apply mask (resizing to audio length)
        mask_upsampled = np.interp(
            np.linspace(0, 1, len(audio)),
            np.linspace(0, 1, len(vad_mask)),
            vad_mask.astype(float)
        )

        return audio * mask_upsampled

    def spectral_gating(self, audio: np.ndarray) -> np.ndarray:
        """Enhanced spectral noise reduction."""
        # Compute STFT
        D = librosa.stft(audio)
        mag, phase = librosa.magphase(D)

        # Estimate noise profile from lowest energy frames
        noise_profile = np.mean(np.sort(mag, axis=1)[:, :self.config.noise_reduce_frames], axis=1)
        noise_profile = noise_profile[:, None]

        # Create mask
        mask = (mag - noise_profile * self.config.noise_reduce_margin).clip(min=0)
        mask = mask / (mask + noise_profile)

        # Apply mask
        return librosa.istft(mask * D)

    def enhance_clarity(self, audio: np.ndarray) -> np.ndarray:
        """Enhance speech clarity using spectral shaping."""
        # Convert to frequency domain
        D = librosa.stft(np.nan_to_num(audio, nan=0.0, posinf=0.0, neginf=0.0))
        mag, phase = librosa.magphase(D)

        # Apply mild spectral shaping to enhance clarity
        freq_bins = np.fft.fftfreq(D.shape[0], 1 / self.config.sample_rate)
        clarity_boost = np.exp(-np.abs(freq_bins - 2000) / 1000) * self.config.enhance_amount
        clarity_boost = clarity_boost[:, None]

        mag_enhanced = mag * (1 + clarity_boost)

        return librosa.istft(mag_enhanced * phase)

    def normalize_loudness(self, audio: np.ndarray) -> np.ndarray:
        """Improved loudness normalization targeting LUFS."""
        # Compute current loudness
        meter = pyloudnorm.Meter(self.config.sample_rate)
        current_loudness = meter.integrated_loudness(audio)

        # Compute gain needed
        gain_db = self.config.target_lufs - current_loudness
        gain_linear = 10 ** (gain_db / 20)

        # Apply gain with soft clipping
        audio_normalized = audio * gain_linear
        return np.tanh(audio_normalized)  # Soft clipping

    def process(self, audio: np.ndarray) -> np.ndarray:
        """Apply all processing steps efficiently."""
        if self.config.trim_silence:
            audio = self.vad_split(audio)

        if self.config.remove_noise:
            audio = self.spectral_gating(audio)

        if self.config.enhance_speech:
            audio = self.enhance_clarity(audio)

        if self.config.normalize:
            audio = self.normalize_loudness(audio)

        return audio

================================================================================
# File: auralis/common/definitions/scheduler/context.py
# Path: /home/marco/PycharmProjects/betterVoiceCraft/Auralis/src/auralis/common/definitions/scheduler/context.py
================================================================================

#  Copyright (c) 2024 Astramind. Licensed under Apache License, Version 2.0.
import asyncio
import copy
import time
import uuid
from dataclasses import dataclass
from typing import Optional, Union, List

import torch

from auralis import TTSRequest
from auralis.common.definitions.batch.batchable_item import BatchableItem
from auralis.common.definitions.types.generator import Tokens, SpeakerEmbeddings, DecodingEmbeddingsModifier, \
    Spectrogram
from auralis.common.scheduling.collective_event import CounterEvent


@dataclass
class GenerationContext(BatchableItem):
    """
    Represents a context for generating text-to-speech output.

    Attributes:
        request_id (Optional[str]): Unique identifier for the request.
        start_time (Optional[float]): The start time of the request.
        text (Optional[str]): The input text for generation.
        language (Optional[str]): The language of the text.
        temperature (Optional[float]): Sampling temperature for generation.
        top_p (Optional[float]): Top-p sampling parameter for generation.
        top_k (Optional[int]): Top-k sampling parameter for generation.
        repetition_penalty (Optional[float]): Penalty for repetition during generation.
        tokens (Optional[Tokens]): Tokens generated from the input text.
        decoding_embeddings_modifier (Optional[DecodingEmbeddingsModifier]): Modifier for decoding embeddings.
        speaker_embeddings (Optional[SpeakerEmbeddings]): Embeddings for the speaker's voice.
        spectrogram (Optional[Spectrogram]): Spectrogram representation of the generated audio.
    """
    start_time: Optional[float] = None
    text: Optional[str] = None
    language: Optional[str] = None
    speaker_files: Union[Union[str,List[str]], Union[bytes,List[bytes]], Union[torch.Tensor, List[torch.Tensor]]] = None

    # Generation parameters shared with ttsrequest
    temperature: Optional[float] = 0.75
    top_p: Optional[float] = 0.85
    top_k: Optional[int] = 50
    repetition_penalty: Optional[float] = 5.0
    length_penalty: Optional[float] = 1.0
    do_sample: Optional[bool] = True
    stream: Optional[bool] = False

    # Shared voice parameters
    max_ref_length: Optional[int] = 60
    gpt_cond_len: Optional[int] = 30
    gpt_cond_chunk_len: Optional[int] = 4

    # Generated states
    tokens: Optional[Tokens] = None
    # this is a modifier which will condition the decoding process in a autoregressive decoder only model
    decoding_embeddings_modifier: Optional[DecodingEmbeddingsModifier] = None
    speaker_embeddings: Optional[SpeakerEmbeddings] = None
    spectrogram: Optional[Spectrogram] = None

    # Intra-generation-related states
    stage: Optional[str] = None

    # original request ref
    parent_request_id: Optional[str] = None
    request_id: Optional[str] = None

    def __post_init__(self):
        self.to('cpu')
        if not self.request_id:
            self.request_id = uuid.uuid4().hex
        if self.start_time is None:
            self.start_time = time.time()

    def to(self, *args, **kwargs):
        #easier to handle
        if torch_module := next((arg for arg in args if isinstance(arg, torch.nn.Module)), None):
            dtype = next(torch_module.parameters()).dtype
            device = next(torch_module.parameters()).device
            for k, v in self.__dict__.items():
                if isinstance(v, torch.Tensor):
                    self.__dict__[k] = v.to(dtype=dtype).to(device=device) # type: ignore
        else:
            for k, v in self.__dict__.items():
                if isinstance(v, torch.Tensor):
                     self.__dict__[k] = v.to(*args, **kwargs)


    def length(self, key: str):
        if key == 'conditioning':
            return sum(t.shape[-1] for t in self.tokens)
        elif key == 'phonetic':
            return 1 # TODO: migth change this, for xtts it works but it will probably be best to resiter a funct at a module level
        elif key == 'synthesis':
            return self.spectrogram.shape[0] * self.spectrogram.shape[1] # bs * seq_len

    @classmethod
    def from_request(cls, request: TTSRequest, **kwargs) -> 'GenerationContext':
        """
        Crea un GenerationContext da un TTSRequest.
        """
        shared_fields = {}
        self_keys = vars(cls).keys()
        for k, v in vars(request).items():
            if k in self_keys and k != 'request_id':
                shared_fields[k] = v

        shared_fields['parent_request_id'] = request.request_id
        shared_fields['stage'] = 'conditioning'
        # Add additional fields
        shared_fields.update(kwargs)

        return cls(**shared_fields)

    def update(self, **kwargs):
        """
        Updates the context with new values while maintaining dataclass integrity.

        Args:
            **kwargs: New values for fields
        """
        # Verify valid fields
        valid_fields = self.__dataclass_fields__.keys() # type: ignore
        for field_name in kwargs:
            if field_name not in valid_fields:
                raise ValueError(f"Invalid field: {field_name}")

        # Update fields
        for field_name, value in kwargs.items():
            setattr(self, field_name, value)

        # Recall __post_init__
        if hasattr(self, '__post_init__'):
            self.__post_init__()

        return self

    def copy(self):
        return copy.deepcopy(self)

================================================================================
# File: auralis/common/definitions/scheduler/__init__.py
# Path: /home/marco/PycharmProjects/betterVoiceCraft/Auralis/src/auralis/common/definitions/scheduler/__init__.py
================================================================================

#  Copyright (c) 2024 Astramind. Licensed under Apache License, Version 2.0.



================================================================================
# File: auralis/common/definitions/scheduler/memory_manager.py
# Path: /home/marco/PycharmProjects/betterVoiceCraft/Auralis/src/auralis/common/definitions/scheduler/memory_manager.py
================================================================================

#  Copyright (c) 2024 Astramind. Licensed under Apache License, Version 2.0.
from dataclasses import dataclass
from typing import List, Optional, Tuple

@dataclass
class Block:
    """Single memory block with linked list capability"""
    id: int
    size: int
    shape: Tuple[int, ...]
    is_free: bool = True
    next_block: Optional['Block'] = None

@dataclass
class PagedAllocation:
    """Logical allocation spanning multiple blocks"""
    blocks: List[Block]
    logical_id: int
    total_size: int
    shape: Tuple[int, ...]


================================================================================
# File: auralis/common/definitions/dto/__init__.py
# Path: /home/marco/PycharmProjects/betterVoiceCraft/Auralis/src/auralis/common/definitions/dto/__init__.py
================================================================================

#  Copyright (c) 2024 Astramind. Licensed under Apache License, Version 2.0.



================================================================================
# File: auralis/common/definitions/dto/output.py
# Path: /home/marco/PycharmProjects/betterVoiceCraft/Auralis/src/auralis/common/definitions/dto/output.py
================================================================================

#  Copyright (c) 2024 Astramind. Licensed under Apache License, Version 2.0.

import io
from dataclasses import dataclass
from pathlib import Path
from typing import Union, Optional, Tuple, List

import numpy as np
import sounddevice as sd
import torch
import torchaudio
from IPython.display import Audio, display
from torio.io import CodecConfig

from auralis.common.scheduling.collective_event import CounterEvent


@dataclass
class TTSOutput:
    array: Union[np.ndarray, bytes]
    request_id: Optional[str] = None  # internal
    parent_request_id: Optional[str] = None  # internal
    start_time: float = 0.0  # internal
    end_time: float = 0.0  # internal
    token_length: int = 0  # internal
    sample_rate: int = 24000
    bit_depth: int = 32
    bit_rate: int = 192 # kbps
    compression: int = 10 #
    channel: int = 1

    def __post_init__(self):
        if isinstance(self.array, bytes):
            self.array = np.frombuffer(self.array, dtype=np.float32)
            #normalize in the range
            self.array = self.array / 32768.0
            fade_length = 100
            fade_in = np.linspace(0, 1, fade_length)
            self.array[:fade_length] *= fade_in


    # Modifica dei metodi che creano nuove istanze
    def change_speed(self, speed_factor: float) -> 'TTSOutput':
        """
        Change audio speed while preserving quality and minimizing distortion.
        Uses phase vocoder for better quality at extreme speed changes.

        Args:
            speed_factor (float): Speed modification factor:
                                 > 1.0: speeds up (e.g. 1.2 = 20% faster)
                                 < 1.0: slows down (e.g. 0.8 = 20% slower)
                                 = 1.0: no change

        Returns:
            TTSOutput: New instance with speed-modified audio

        Example:
            # Speed up 20%
            faster = audio.change_speed(1.2)

            # Slow down 20%
            slower = audio.change_speed(0.8)

        Raises:
            ValueError: If speed_factor is <= 0
        """
        import librosa

        # Validate input
        if speed_factor <= 0:
            raise ValueError("Speed factor must be positive")

        if speed_factor == 1.0:
            return self

        # Ensure float32
        wav = self.array.astype(np.float32) if self.array.dtype != np.float32 else self.array

        # Parameters for STFT
        n_fft = 2048
        hop_length = 512

        # Compute STFT
        D = librosa.stft(wav, n_fft=n_fft, hop_length=hop_length)

        # Time-stretch using phase vocoder
        modified_stft = librosa.phase_vocoder(
            D,
            rate=speed_factor,
            hop_length=hop_length
        )

        # Inverse STFT
        modified = librosa.istft(
            modified_stft,
            hop_length=hop_length,
            length=len(wav)
        )

        # Normalize to prevent clipping
        modified = librosa.util.normalize(modified, norm=np.inf)

        return TTSOutput(
            array=modified,
            sample_rate=self.sample_rate,
        )

    @staticmethod
    def combine_outputs(outputs: List['TTSOutput']) -> 'TTSOutput':
        """Combine multiple TTSOutput instances into a single instance.

        Args:
            outputs: List of TTSOutput instances

        Returns:
            New TTSOutput instance with concatenated audio
        """
        # Concatenate audio
        combined_audio = np.concatenate([out.array for out in outputs])

        # Use sample rate of first output
        return TTSOutput(
            array=combined_audio,
            sample_rate=outputs[0].sample_rate,
        )

    def to_tensor(self) -> Union[torch.Tensor, np.ndarray]:
        """Convert numpy array to torch tensor"""
        if isinstance(self.array, np.ndarray):
            return torch.from_numpy(self.array)
        return self.array

    def to_bytes(self, format: str = 'wav', sample_width: int = 2) -> bytes:
        """Convert audio to bytes format.

        Args:
            format: Output format ('mp3', 'opus', 'aac', 'flac', 'wav', 'pcm')
            sample_width: Bit depth (1, 2, or 4 bytes per sample)

        Returns:
            Audio data as bytes
        """
        # Convert to tensor if needed
        wav_tensor = self.to_tensor().to(torch.float32)

        # Ensure correct shape (1, N) for torchaudio
        if wav_tensor.dim() == 1:
            wav_tensor = wav_tensor.unsqueeze(0)

        # Normalize to [-1, 1]
        wav_tensor = torch.clamp(wav_tensor, -1.0, 1.0)

        buffer = io.BytesIO()

        if format in ['wav', 'flac']:
            torchaudio.save(
                buffer,
                wav_tensor,
                self.sample_rate,
                format=format,
                encoding="PCM_S" if sample_width == 2 else "PCM_F",
                bits_per_sample=sample_width * 8,
                compression = CodecConfig(compression_level=min(8, self.compression)) if format == 'flac' else None
            )
        elif format == 'mp3':
            torchaudio.save(
                buffer,
                wav_tensor,
                self.sample_rate,
                format="mp3",
                compression = CodecConfig(bit_rate=self.bit_rate)
            )
        elif format == 'opus':
            torchaudio.save(
                buffer,
                wav_tensor,
                self.sample_rate,
                format="opus",
                compression = CodecConfig(compression_level=self.compression)
            )
        elif format == 'aac':
            torchaudio.save(
                buffer,
                wav_tensor,
                self.sample_rate,
                format="adts",
                compression = CodecConfig(bit_rate=self.bit_rate)
            )
        elif format == 'pcm':
            # Scale to appropriate range based on sample width
            if sample_width == 2:  # 16-bit
                wav_tensor = (wav_tensor * 32767).to(torch.int16)
            elif sample_width == 4:  # 32-bit
                wav_tensor = (wav_tensor * 2147483647).to(torch.int32)
            else:  # 8-bit
                wav_tensor = (wav_tensor * 127).to(torch.int8)
            return wav_tensor.cpu().numpy().tobytes()
        else:
            raise ValueError(f"Unsupported format: {format}. Supported formats are: mp3, opus, aac, flac, wav, pcm")

        return buffer.getvalue()

    def save(self,
             filename: Union[str, Path],
             sample_rate: Optional[int] = None,
             format: Optional[str] = None) -> None:
        """Save audio to file.

        Args:
            filename: Output filename
            sample_rate: Optional new sample rate for resampling
            format: Optional format override (default: inferred from extension)
        """
        wav_tensor = self.to_tensor()
        if wav_tensor.dim() == 1:
            wav_tensor = wav_tensor.unsqueeze(0)

        # Resample if needed
        if sample_rate and sample_rate != self.sample_rate:
            wav_tensor = torchaudio.functional.resample(
                wav_tensor,
                orig_freq=self.sample_rate,
                new_freq=sample_rate
            )
        else:
            sample_rate = self.sample_rate
        if wav_tensor.dtype != torch.float32:
            wav_tensor = wav_tensor.to(torch.float32)
        torchaudio.save(
            filename,
            wav_tensor,
            sample_rate,
            format=format,
            bits_per_sample=self.bit_depth,
            channels_first=self.channel
        )

    def resample(self, new_sample_rate: int) -> 'TTSOutput':
        """Create new TTSOutput with resampled audio.

        Args:
            new_sample_rate: Target sample rate

        Returns:
            New TTSOutput instance with resampled audio
        """
        wav_tensor = self.to_tensor()
        if wav_tensor.dim() == 1:
            wav_tensor = wav_tensor.unsqueeze(0)

        resampled = torchaudio.functional.resample(
            wav_tensor,
            orig_freq=self.sample_rate,
            new_freq=new_sample_rate
        )

        return TTSOutput(
            array=resampled.squeeze().numpy(),
            sample_rate=new_sample_rate
        )

    def get_info(self) -> Tuple[int, int, float]:
        """Get audio information.

        Returns:
            Tuple of (number of samples, sample rate, duration in seconds)
        """
        n_samples = len(self.array)
        duration = n_samples / self.sample_rate
        return n_samples, self.sample_rate, duration

    @classmethod
    def from_tensor(cls, tensor: torch.Tensor, sample_rate: int = 24000) -> 'TTSOutput':
        """Create TTSOutput from torch tensor.

        Args:
            tensor: Audio tensor
            sample_rate: Sample rate of the audio

        Returns:
            New TTSOutput instance
        """
        return cls(
            array=tensor.squeeze().cpu().numpy(),
            sample_rate=sample_rate
        )

    @classmethod
    def from_file(cls, filename: Union[str, Path]) -> 'TTSOutput':
        """Create TTSOutput from audio file.

        Args:
            filename: Path to audio file

        Returns:
            New TTSOutput instance
        """
        wav_tensor, sample_rate = torchaudio.load(filename)
        return cls.from_tensor(wav_tensor, sample_rate)

    def play(self) -> None:
        """Play the audio through the default sound device.
        For use in regular Python scripts/applications."""
        # Ensure the audio is in the correct format
        if isinstance(self.array, torch.Tensor):
            audio_data = self.array.cpu().numpy()
        else:
            audio_data = self.array

        # Ensure float32 and normalize
        if audio_data.dtype != np.float32:
            audio_data = audio_data.astype(np.float32)
        audio_data = np.clip(audio_data, -1.0, 1.0)

        # Play the audio
        sd.play(audio_data, self.sample_rate, blocksize=2048)
        sd.wait()  # Wait until the audio is finished playing

    def display(self) -> Optional[Audio]:
        """Display audio player in Jupyter notebook.
        Returns Audio widget if in notebook, None otherwise."""
        try:
            # Convert to bytes
            audio_bytes = self.to_bytes(format='wav')

            # Create and display audio widget
            audio_widget = Audio(audio_bytes, rate=self.sample_rate, autoplay=False)
            display(audio_widget)
            return audio_widget
        except Exception as e:
            print(f"Could not display audio widget: {str(e)}")
            print("Try using .play() method instead")
            return None

    def preview(self) -> None:
        """Smart play method that chooses appropriate playback method."""
        try:
            # Try notebook display first
            if self.display() is None:
                # Fall back to sounddevice if not in notebook
                self.play()
        except Exception as e:
            print(f"Error playing audio: {str(e)}")

    @property
    def duration(self) -> float:
        return self.end_time - self.start_time

================================================================================
# File: auralis/common/definitions/dto/requests.py
# Path: /home/marco/PycharmProjects/betterVoiceCraft/Auralis/src/auralis/common/definitions/dto/requests.py
================================================================================

#  Copyright (c) 2024 Astramind. Licensed under Apache License, Version 2.0.

import functools
import hashlib
import io
import json
import uuid
from dataclasses import asdict, field
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path
from typing import Union, AsyncGenerator, Optional, List, Literal, get_args, Callable, Dict

import langid
import librosa
import soundfile as sf
from cachetools import LRUCache

from auralis.common.definitions.batch.batchable_item import BatchableItem
from auralis.common.logging.logger import setup_logger
from auralis.common.definitions.enhancer import EnhancedAudioProcessor, AudioPreprocessingConfig

logger = setup_logger(__name__)

def hash_params(*args, **kwargs):
    """
    Convert args and kwargs to a JSON string and hash it.

    Parameters
    ----------
    *args : tuple
        Positional arguments to be hashed
    **kwargs : dict
        Keyword arguments to be hashed

    Returns
    -------
    str
        The hash string
    """
    params_str = json.dumps([str(arg) for arg in args], sort_keys=True)
    return hashlib.md5(params_str.encode()).hexdigest()



def cached_processing(maxsize=128):

    def decorator(func):
        # Create cache storage
        cache = LRUCache(maxsize=maxsize)
        @functools.wraps(func)
        def wrapper(self, audio_path: str, audio_config: AudioPreprocessingConfig, *args, **kwargs):
            # Create hash from the two parameters we care about
            params_dict = {
                'audio_path': audio_path,
                'config': asdict(audio_config)
            }
            cache_key = hash_params(params_dict)

            # Check cache
            if result := cache.get(cache_key):
                return result

            # If not in cache, process and store
            result = func(self, audio_path, audio_config, *args, **kwargs)
            cache.__setitem__(cache_key, result)
            return result

        return wrapper

    return decorator


SupportedLanguages = Literal[
        "en",
        "es",
        "fr",
        "de",
        "it",
        "pt",
        "pl",
        "tr",
        "ru",
        "nl",
        "cs",
        "ar",
        "zh-cn",
        "hu",
        "ko",
        "ja",
        "hi",
        "auto",
        ""
    ]

@lru_cache(maxsize=1024)
def get_language(text: str):
    """
    Detect the language of a given text using langid.

    Args:
        text (str): The text to detect the language from.

    Returns:
        str: The detected language as an ISO 639-1 language code.

    Notes:
        Langid is used to detect the language. If the detected language is Chinese
        ("zh"), it is replaced with "zh-cn" since we use Mandarin Chinese as our
        Chinese language variant.

    """
    detected_language =  langid.classify(text)[0].strip()
    if detected_language == "zh":
        # we use zh-cn
        detected_language = "zh-cn"
    return detected_language

def validate_language(language: str) -> SupportedLanguages:
    """
    Validate that the provided language is supported.

    Args:
        language (str): The language code to validate.

    Returns:
        SupportedLanguages: The validated language code.

    Raises:
        ValueError: If the language is not supported.
    """
    supported = get_args(SupportedLanguages)
    if language not in supported:
        raise ValueError(
            f"Language {language} not supported. Must be one of {supported}"
        )
    return language # type: ignore


@dataclass
class TTSRequest:
    """
    Data class representing a Text-to-Speech (TTS) request.

    Attributes:
        text (Union[AsyncGenerator[str, None], str, List[str]]): The text or texts to be converted into speech.
        speaker_files (Union[Union[str, List[str]], Union[bytes, List[bytes]]]): The speaker audio files or data.
        context_partial_function (Optional[Callable]): A partial function for additional context processing.
        start_time (Optional[float]): The start time for the TTS operation.
        enhance_speech (bool): Flag to indicate if speech enhancement should be applied.
        audio_config (AudioPreprocessingConfig): Configuration for audio preprocessing.
        language (SupportedLanguages): The language of the text, defaults to 'auto' for automatic detection.
        request_id (str): Unique identifier for the request.
        load_sample_rate (int): The sample rate for loading audio files.
        sound_norm_refs (bool): Flag to indicate if sound normalization references should be used.
        max_ref_length (int): Maximum reference length for voice conditioning.
        gpt_cond_len (int): Length of GPT conditioning.
        gpt_cond_chunk_len (int): Chunk length for GPT conditioning.
        stream (bool): Flag to indicate if the output should be streamed.
        temperature (float): Sampling temperature for generation.
        top_p (float): Top-p sampling parameter for generation.
        top_k (int): Top-k sampling parameter for generation.
    """
    # Request metadata
    text: Union[AsyncGenerator[str, None], str, List[str]]

    speaker_files: Union[Union[str,List[str]], Union[bytes,List[bytes]]] = None
    context_partial_function: Optional[Callable] = None

    start_time: Optional[float] = None
    split_text: bool = True
    enhance_speech: bool = False
    audio_config: AudioPreprocessingConfig = field(default_factory=AudioPreprocessingConfig)
    language: SupportedLanguages = "auto"
    request_id: str = field(default_factory=lambda: uuid.uuid4().hex)
    load_sample_rate: int = 22050
    sound_norm_refs: bool = False

    # Voice conditioning parameters
    max_ref_length: int = 60
    gpt_cond_len: int = 30
    gpt_cond_chunk_len: int = 4

    # Generation parameters
    stream: bool = False
    temperature: float = 0.75
    top_p: float = 0.85
    top_k: int = 50
    repetition_penalty: float = 5.0
    length_penalty: float = 1.0
    do_sample: bool = True


    def _validation_for_model(self):
         pass
        # for now just a placeholder, but it'll check form the model registry some predefined values

    def __post_init__(self):

        if self.language == 'auto' and len(self.text) > 0:
            self.language = get_language(self.text)

        validate_language(self.language)
        self.processor = EnhancedAudioProcessor(self.audio_config)

        if isinstance(self.speaker_files, list) and self.enhance_speech:
            if len(self.speaker_files) > 5:
                logger.warning(f"You provided alist of {len(self.speaker_files)} speaker files "
                               f"but only 5 are supported. we'll take the first 5.")
                self.speaker_files = self.speaker_files[:5]  # FIXME(mlinmg): for xttsv2 might need adjustments later

            self.speaker_files = [self.preprocess_audio(f, self.audio_config) for f in self.speaker_files]

        if self.max_ref_length > 60:
            logger.warning(f"Maximum reference length is set to {self.max_ref_length}. "
                           f"We hard limit this to 60 seconds.") # FIXME(mlinmg): for xttsv2 might need adjustments later
            self.max_ref_length = 60


    def infer_language(self):
        """
        Infer the language of the text if it is set to 'auto'.

        If the language is set to 'auto', this method will infer the language of the
        text using the `langid` library. The inferred language is then stored in the
        `language` attribute.

        Parameters
        ----------
        None

        Returns
        -------
        None
        """
        if self.language == 'auto':
            self.language = get_language(self.text)

    @cached_processing()
    def preprocess_audio(self, audio_source: Union[str, bytes], audio_config: AudioPreprocessingConfig) -> str:
        """
        Preprocesses an audio source (either a file path or a bytes object).

        The audio is processed using the `processor` attribute, which is an instance of
        `EnhancedAudioProcessor` with the `audio_config` parameter. The output is saved to
        a temporary file in `/tmp/auralis` and the path to the file is returned.

        If an error occurs during processing, the original file is returned.

        Parameters
        ----------
        audio_source : Union[str, bytes]
            The audio source to preprocess.
        audio_config : AudioPreprocessingConfig
            The configuration for the audio preprocessing.

        Returns
        -------
        str
            The path to the preprocessed audio file.
        """
        try:
            temp_dir = Path("/tmp/auralis")
            temp_dir.mkdir(exist_ok=True)
            if isinstance(audio_source, str):
                audio_source = Path(audio_source)
                audio, sr = librosa.load(audio_source, sr=self.audio_config.sample_rate)
            else:
                audio, sr = librosa.load(io.BytesIO(audio_source), sr=self.audio_config.sample_rate)
            processed = self.processor.process(audio)

            output_path = temp_dir / (f"{hash(audio_source) if isinstance(audio_source, bytes) else audio_source.stem}"
                                      f"{uuid.uuid4().hex}"
                                      f"{'.wav' if isinstance(audio_source, bytes) else audio_source.suffix}")
            sf.write(output_path, processed, sr)
            return str(output_path)

        except Exception as e:
            print(f"Error processing audio: {e}. Using original file.")
            return audio_source

    def copy(self):

        """
        Creates a shallow copy of the current request.

        Returns:
            A new instance of TTSRequest with the same fields as the current instance.
        """
        copy_fields = {
            'text': self.text,
            'speaker_files': self.speaker_files,
            'enhance_speech': self.enhance_speech,
            'audio_config': self.audio_config,
            'language': self.language,
            'request_id': self.request_id,
            'load_sample_rate': self.load_sample_rate,
            'sound_norm_refs': self.sound_norm_refs,
            'max_ref_length': self.max_ref_length,
            'gpt_cond_len': self.gpt_cond_len,
            'gpt_cond_chunk_len': self.gpt_cond_chunk_len,
            'stream': self.stream,
            'temperature': self.temperature,
            'top_p': self.top_p,
            'top_k': self.top_k,
            'repetition_penalty': self.repetition_penalty,
            'length_penalty': self.length_penalty,
            'do_sample': self.do_sample
        }

        return TTSRequest(**copy_fields)


================================================================================
# File: auralis/common/definitions/protocols/__init__.py
# Path: /home/marco/PycharmProjects/betterVoiceCraft/Auralis/src/auralis/common/definitions/protocols/__init__.py
================================================================================

#  Copyright (c) 2024 Astramind. Licensed under Apache License, Version 2.0.



================================================================================
# File: auralis/common/definitions/protocols/openai.py
# Path: /home/marco/PycharmProjects/betterVoiceCraft/Auralis/src/auralis/common/definitions/protocols/openai.py
================================================================================

#  Copyright (c) 2024 Astramind. Licensed under Apache License, Version 2.0.

import base64
from dataclasses import fields
from typing import List, Optional, Dict, Any, Literal

from pydantic import BaseModel, Field, field_validator

from auralis.common.definitions.dto.requests import TTSRequest


class ChatCompletionMessage(BaseModel):
    role: Literal["system", "user", "assistant"]
    content: str

tts_defaults = {field.name: field.default for field in fields(TTSRequest)}

class VoiceChatCompletionRequest(BaseModel):
    # Chat completion fields
    model: str
    messages: List[ChatCompletionMessage]
    speaker_files: List[str] = Field(..., description="List of base64-encoded audio or paths to audio files on the server machine")
    modalities: List[Literal["text", "audio"]] = Field(
        default=["text", "audio"],
        description="Output modalities to return"
    )
    openai_api_url: Optional[str] = Field(
        default=None,
        description="Custom OpenAI API endpoint to make the LLM reqeust to"
    )
    vocalize_at_every_n_words: int = Field(
        default=100,
        ge=1,
        description="Number of words after which to generate audio"
    )
    stream: bool = Field(default=True)

    # TTSRequest parameters usando i defaults dalla dataclass
    enhance_speech: bool = Field(default=tts_defaults['enhance_speech'])
    language: str = Field(default=tts_defaults['language'])
    max_ref_length: int = Field(default=tts_defaults['max_ref_length'])
    gpt_cond_len: int = Field(default=tts_defaults['gpt_cond_len'])
    gpt_cond_chunk_len: int = Field(default=tts_defaults['gpt_cond_chunk_len'])
    temperature: float = Field(default=tts_defaults['temperature'])
    top_p: float = Field(default=tts_defaults['top_p'])
    top_k: int = Field(default=tts_defaults['top_k'])
    repetition_penalty: float = Field(default=tts_defaults['repetition_penalty'])
    length_penalty: float = Field(default=tts_defaults['length_penalty'])
    do_sample: bool = Field(default=tts_defaults['do_sample'])

    @field_validator('openai_api_url')
    def validate_oai_url(cls, v):
        if v is None:
            raise ValueError("You should always give a url for the text generation")
        return v

    @field_validator('stream')
    def validate_stream(cls, v):
        if not v:
            raise ValueError('Streaming should be enabled! For non-streaming conversion use the audio endpoint')
        return v

    @field_validator('speaker_files')
    def validate_speaker_files(cls, v):
        if not v:
            raise ValueError("At least one speaker file is required")
        for file in v:
            try:
                base64.b64decode(file)
            except Exception:
                raise ValueError(f"Invalid base64 encoding in speaker file")
        return v

    @field_validator('modalities')
    def validate_modalities(cls, v):
        valid_modalities = ["text", "audio"]
        if not all(m in valid_modalities for m in v):
            raise ValueError(f"Invalid modalities. Must be one or more of {valid_modalities}")
        return v

    def to_tts_request(self, text: str = "") -> TTSRequest:
        """Convert to TTSRequest with decoded speaker files"""
        speaker_data_list = [base64.b64decode(f) for f in self.speaker_files]

        return TTSRequest(
            text=text,
            stream=False,
            speaker_files=speaker_data_list,
            enhance_speech=self.enhance_speech,
            language=self.language,
            max_ref_length=self.max_ref_length,
            gpt_cond_len=self.gpt_cond_len,
            gpt_cond_chunk_len=self.gpt_cond_chunk_len,
            temperature=self.temperature,
            top_p=self.top_p,
            top_k=self.top_k,
            repetition_penalty=self.repetition_penalty,
            length_penalty=self.length_penalty,
            do_sample=self.do_sample
        )

    def to_openai_request(self) -> Dict[str, Any]:
        """Convert to OpenAI API compatible request format"""
        oai_dict = {
            k: v for k, v in self.model_dump().items()
            if k not in ["speaker_files", "openai_api_url", "vocalize_at_every_n_words", 'modalities'] and
               not k in tts_defaults.keys()
        }
        oai_dict.update({"stream": True})
        return oai_dict


class AudioSpeechGenerationRequest(BaseModel):
        # Chat completion fields
        input: str = Field(..., description="The textual input to convert")
        model: str = Field(..., description="The model to use for conversion")
        voice: List[str] = Field(..., description="List of base64-encoded audios or paths to audio files on the server machine")
        response_format: Literal["mp3", "opus", "aac", "flac", "wav", "pcm"] = Field(
            default='mp3', description="List of base64-encoded audio files"
        )
        speed: float = Field(default=1.0, description="List of base64-encoded audio files"),

        # TTSRequest parameters
        enhance_speech: bool = Field(default=tts_defaults['enhance_speech'])
        language: str = Field(default=tts_defaults['language'])
        max_ref_length: int = Field(default=tts_defaults['max_ref_length'])
        gpt_cond_len: int = Field(default=tts_defaults['gpt_cond_len'])
        gpt_cond_chunk_len: int = Field(default=tts_defaults['gpt_cond_chunk_len'])
        temperature: float = Field(default=tts_defaults['temperature'])
        top_p: float = Field(default=tts_defaults['top_p'])
        top_k: int = Field(default=tts_defaults['top_k'])
        repetition_penalty: float = Field(default=tts_defaults['repetition_penalty'])
        length_penalty: float = Field(default=tts_defaults['length_penalty'])
        do_sample: bool = Field(default=tts_defaults['do_sample'])

        @field_validator('voice')
        def validate_speaker_files(cls, v):
            if not v:
                raise ValueError("At least one voice file is required")
            for file in v:
                try:
                    base64.b64decode(file)
                except Exception:
                    raise ValueError(f"Invalid base64 encoding in voice file")
            return v

        def to_tts_request(self) -> TTSRequest:
            """Convert to TTSRequest with decoded speaker files"""
            speaker_data_list = [base64.b64decode(f) for f in self.voice]

            return TTSRequest(
                text=self.input,
                stream=False,
                speaker_files=speaker_data_list,
                enhance_speech=self.enhance_speech,
                language=self.language,
                max_ref_length=self.max_ref_length,
                gpt_cond_len=self.gpt_cond_len,
                gpt_cond_chunk_len=self.gpt_cond_chunk_len,
                temperature=self.temperature,
                top_p=self.top_p,
                top_k=self.top_k,
                repetition_penalty=self.repetition_penalty,
                length_penalty=self.length_penalty,
                do_sample=self.do_sample
            )


================================================================================
# File: auralis/common/definitions/types/scheduler.py
# Path: /home/marco/PycharmProjects/betterVoiceCraft/Auralis/src/auralis/common/definitions/types/scheduler.py
================================================================================

#  Copyright (c) 2024 Astramind. Licensed under Apache License, Version 2.0.
from typing import Callable, Optional

Function = Callable
Lambda = Callable

FakeFactoriesForSchedulerProfiling = tuple[Lambda, Optional[Lambda], Lambda]

================================================================================
# File: auralis/common/definitions/types/__init__.py
# Path: /home/marco/PycharmProjects/betterVoiceCraft/Auralis/src/auralis/common/definitions/types/__init__.py
================================================================================

#  Copyright (c) 2024 Astramind. Licensed under Apache License, Version 2.0.



================================================================================
# File: auralis/common/definitions/types/orchestrator.py
# Path: /home/marco/PycharmProjects/betterVoiceCraft/Auralis/src/auralis/common/definitions/types/orchestrator.py
================================================================================

#  Copyright (c) 2024 Astramind. Licensed under Apache License, Version 2.0.
from typing import Union, Callable, Coroutine, Any, AsyncGenerator

from auralis.common.definitions.dto.output import TTSOutput
from auralis.common.definitions.dto.requests import TTSRequest
from auralis.common.definitions.scheduler.context import GenerationContext

BatcherFunction = Callable[
                     [Union[TTSRequest, GenerationContext]],
                      Coroutine[Any, Any, Union[
                          list[GenerationContext], GenerationContext, AsyncGenerator[TTSOutput, None]]]
                 ]

================================================================================
# File: auralis/common/definitions/types/generator.py
# Path: /home/marco/PycharmProjects/betterVoiceCraft/Auralis/src/auralis/common/definitions/types/generator.py
================================================================================

#  Copyright (c) 2024 Astramind. Licensed under Apache License, Version 2.0.

from typing import List, Union

import torch

Token = Union[int, torch.Tensor]
Tokens = List[Token]

SpeakerEmbeddings = torch.Tensor
DecodingEmbeddingsModifier = torch.Tensor
Spectrogram = Union[torch.Tensor, List[torch.Tensor]]



================================================================================
# File: auralis/common/definitions/batch/__init__.py
# Path: /home/marco/PycharmProjects/betterVoiceCraft/Auralis/src/auralis/common/definitions/batch/__init__.py
================================================================================



================================================================================
# File: auralis/common/definitions/batch/batches.py
# Path: /home/marco/PycharmProjects/betterVoiceCraft/Auralis/src/auralis/common/definitions/batch/batches.py
================================================================================

#  Copyright (c) 2024 Astramind. Licensed under Apache License, Version 2.0.
from abc import ABC, abstractmethod
from dataclasses import dataclass
from typing import List, Dict

from dataclasses import dataclass, field
from typing import Dict, List, Optional, Union
import torch

from auralis.common.definitions.batch.batchable_item import BatchableItem
from auralis.common.definitions.dto.requests import TTSRequest
from auralis.common.definitions.scheduler.context import GenerationContext


@dataclass
class BatchedItems:
    """Container for batched items with support for TTSRequest and GenerationContext"""
    stage: str
    items: List[BatchableItem] = field(default_factory=list)
    #_indexes: List[str] = field(default_factory=list)
    # all of this is commented since it will be useful when request are actually batched and not only generated in parallel

    # Batched data containers
    #texts: List[str] = field(default_factory=list)
    #speaker_files: Optional[torch.Tensor] = None
    #speaker_embeddings: Optional[torch.Tensor] = None
    #tokens: Optional[torch.Tensor] = None
    #decoding_modifiers: Optional[torch.Tensor] = None
    #spectrograms: Optional[torch.Tensor] = None

    # Metadata
    lengths: torch.Tensor = None
    #attention_mask: torch.Tensor = None

    def batch(self, item: BatchableItem):
        """Add an item to the batch and update tensors accordingly"""
        self.items.append(item)
        #self._indexes.append(item.request_id)

    @property
    def length(self) -> int:
        """Get the total length of all items in the batch"""
        return sum(item.length(self.stage) for item in self.items)

    # def _batch_tts_request(self, item: TTSRequest):
    #     """Handle batching for TTSRequest items"""
    #     if self.stage == 'conditioning_phase':
    #         self.texts.append(item.text)
    #         if isinstance(item.speaker_files, list):
    #             # Stack speaker files as tensors with padding
    #             files_tensor = self._pad_and_stack_audio(item.speaker_files)
    #             self.speaker_files = files_tensor if self.speaker_files is None else \
    #                 torch.cat([self.speaker_files, files_tensor])
    #         else:
    #             # Single speaker file
    #             audio_tensor = self._load_audio_as_tensor(item.speaker_files)
    #             self.speaker_files = audio_tensor if self.speaker_files is None else \
    #                 torch.cat([self.speaker_files, audio_tensor.unsqueeze(0)])
    #
    # def _batch_generation_context(self, item: GenerationContext):
    #     """Handle batching for GenerationContext items"""
    #     if item.tokens is not None:
    #         tokens_tensor = torch.tensor(item.tokens)
    #         self.tokens = tokens_tensor if self.tokens is None else \
    #             torch.cat([self.tokens, tokens_tensor.unsqueeze(0)])
    #
    #     if item.speaker_embeddings is not None:
    #         emb_tensor = torch.tensor(item.speaker_embeddings)
    #         self.speaker_embeddings = emb_tensor if self.speaker_embeddings is None else \
    #             torch.cat([self.speaker_embeddings, emb_tensor.unsqueeze(0)])
    #
    #     if item.decoding_embeddings_modifier is not None:
    #         mod_tensor = torch.tensor(item.decoding_embeddings_modifier)
    #         self.decoding_modifiers = mod_tensor if self.decoding_modifiers is None else \
    #             torch.cat([self.decoding_modifiers, mod_tensor.unsqueeze(0)])
    #
    #     if item.spectrogram is not None:
    #         spec_tensor = torch.tensor(item.spectrogram)
    #         self.spectrograms = spec_tensor if self.spectrograms is None else \
    #             torch.cat([self.spectrograms, spec_tensor.unsqueeze(0)])
    #
    # def _pad_and_stack_audio(self, audio_files: List[str]) -> torch.Tensor:
    #     """Load audio files and stack them with padding"""
    #     audio_tensors = [self._load_audio_as_tensor(f) for f in audio_files]
    #     max_length = max(t.size(-1) for t in audio_tensors)
    #
    #     padded_tensors = []
    #     for tensor in audio_tensors:
    #         padding = max_length - tensor.size(-1)
    #         padded = torch.nn.functional.pad(tensor, (0, padding))
    #         padded_tensors.append(padded)
    #
    #     return torch.stack(padded_tensors)
    #
    # def _load_audio_as_tensor(self, audio_file: str) -> torch.Tensor:
    #     """Load audio file and convert to tensor"""
    #     # Implementation depends on your audio loading library
    #     # This is a placeholder - implement actual audio loading
    #     return torch.zeros(1)  # placeholder
    #
    #
    # def _unbatch_tts_request(self, item: TTSRequest, idx: int) -> TTSRequest:
    #     """Unbatch a TTSRequest item"""
    #     new_item = item.copy()
    #     if self.speaker_files is not None:
    #         new_item.speaker_files = self.speaker_files[idx].numpy()
    #     return new_item
    #
    # def _unbatch_generation_context(self, item: GenerationContext, idx: int) -> GenerationContext:
    #     """Unbatch a GenerationContext item"""
    #     new_item = GenerationContext(
    #         request_id=item.request_id,
    #         tokens=self.tokens[idx].numpy() if self.tokens is not None else None,
    #         speaker_embeddings=self.speaker_embeddings[idx].numpy() if self.speaker_embeddings is not None else None,
    #         decoding_embeddings_modifier=self.decoding_modifiers[
    #             idx].numpy() if self.decoding_modifiers is not None else None,
    #         spectrogram=self.spectrograms[idx].numpy() if self.spectrograms is not None else None
    #     )
    #     return new_item



================================================================================
# File: auralis/common/definitions/batch/batchable_item.py
# Path: /home/marco/PycharmProjects/betterVoiceCraft/Auralis/src/auralis/common/definitions/batch/batchable_item.py
================================================================================

from abc import ABC, abstractmethod


class BatchableItem(ABC):

    @abstractmethod
    def length(self, key: str):
        raise NotImplementedError

    # @property
    # @abstractmethod
    # def lenght(self):
    #     raise NotImplementedError
    #


================================================================================
# File: auralis/common/definitions/metrics/__init__.py
# Path: /home/marco/PycharmProjects/betterVoiceCraft/Auralis/src/auralis/common/definitions/metrics/__init__.py
================================================================================

#  Copyright (c) 2024 Astramind. Licensed under Apache License, Version 2.0.



================================================================================
# File: auralis/common/definitions/metrics/tracker.py
# Path: /home/marco/PycharmProjects/betterVoiceCraft/Auralis/src/auralis/common/definitions/metrics/tracker.py
================================================================================

#  Copyright (c) 2024 Astramind. Licensed under Apache License, Version 2.0.

import time
from dataclasses import dataclass, field

from auralis.common.logging.logger import setup_logger


@dataclass
class TTSMetricsTracker:
    logger = setup_logger(__file__)

    window_start: float = field(default_factory=time.time)
    last_log_time: float = field(default_factory=time.time)
    log_interval: float = 5.0  # sec between logs

    window_tokens: int = 0
    window_audio_seconds: float = 0
    window_requests: int = 0

    @property
    def requests_per_second(self) -> float:
        elapsed = time.time() - self.window_start
        return self.window_requests / elapsed if elapsed > 0 else 0

    @property
    def tokens_per_second(self) -> float:
        elapsed = time.time() - self.window_start
        return self.window_tokens / elapsed if elapsed > 0 else 0

    @property
    def ms_per_second_of_audio(self) -> float:
        elapsed = (time.time() - self.window_start) * 1000  # in ms
        return elapsed / self.window_audio_seconds if self.window_audio_seconds > 0 else 0

    def reset_window(self) -> None:
        current_time = time.time()
        self.last_log_time = current_time
        # reset window
        self.window_start = current_time
        self.window_tokens = 0
        self.window_audio_seconds = 0
        self.window_requests = 0

    def update_metrics(self, tokens: int, audio_seconds: float) -> bool:
        self.window_tokens += tokens
        self.window_audio_seconds += audio_seconds
        self.window_requests += 1

        current_time = time.time()
        should_log = current_time - self.last_log_time >= self.log_interval

        return should_log

================================================================================
# File: auralis/common/logging/__init__.py
# Path: /home/marco/PycharmProjects/betterVoiceCraft/Auralis/src/auralis/common/logging/__init__.py
================================================================================

#  Copyright (c) 2024 Astramind. Licensed under Apache License, Version 2.0.



================================================================================
# File: auralis/common/logging/logger.py
# Path: /home/marco/PycharmProjects/betterVoiceCraft/Auralis/src/auralis/common/logging/logger.py
================================================================================

#  Copyright (c) 2024 Astramind. Licensed under Apache License, Version 2.0.

import copy
import logging
import os
import re
import sys
import traceback
from datetime import datetime
from pathlib import Path
from typing import Optional, Union

import colorama
from colorama import Fore, Back, Style

# Initialize colorama
colorama.init()

VLLM_LOGGER_LEVEL = logging.INFO

class VLLMLogOverrider:
    """Override VLLM loggers to use custom formatting"""

    def __init__(self, target_logger: logging.Logger):
        self.target_logger = target_logger
        self.perf_pattern = re.compile(
            r"Avg prompt throughput:.+tokens/s,.+GPU KV cache usage:.+CPU KV cache usage:.+"
        )
        self.pipeline_warning_pattern = re.compile(r"Your model uses the legacy input pipeline instead of the new")
        self._override_vllm_loggers()

    def _override_vllm_loggers(self):
        """Override VLLM loggers to use our custom handler"""
        global VLLM_LOGGER_LEVEL
        for name in logging.root.manager.loggerDict:
            if name.startswith('vllm'):
                vllm_logger = logging.getLogger(name)
                current_level = VLLM_LOGGER_LEVEL
                vllm_logger.handlers.clear()
                vllm_logger.propagate = False
                handler = self._create_redirecting_handler()
                vllm_logger.addHandler(handler)
                vllm_logger.setLevel(current_level)

    def _create_redirecting_handler(self):
        """Create a handler that uses our custom formatting"""

        class RedirectHandler(logging.Handler):
            def __init__(self, target_logger, perf_pattern, pipe_warn):
                super().__init__()
                self.target_logger = target_logger
                self.pipe_warn = pipe_warn
                self.perf_pattern = perf_pattern

            def emit(self, record):
                msg = str(record.msg)
                if record.args:
                    msg = msg % record.args

                # Modify performance metrics format
                if self.perf_pattern.search(msg):
                    self.target_logger.log(record.levelno, f"Decoder performance: {msg}")
                elif self.pipe_warn.search(msg):
                    # Skip pipeline warning logs
                    pass
                else:
                    # Pass through all other logs normally
                    self.target_logger.log(record.levelno, msg)

        return RedirectHandler(self.target_logger, self.perf_pattern, self.pipeline_warning_pattern)


class ColoredFormatter(logging.Formatter):
    """Colored formatter with structured output and file location"""

    COLORS = {
        'DEBUG': {
            'color': Fore.CYAN,
            'style': Style.DIM,
            'icon': 'ð'
        },
        'INFO': {
            'color': Fore.GREEN,
            'style': Style.NORMAL,
            'icon': 'â¹ï¸'
        },
        'WARNING': {
            'color': Fore.YELLOW,
            'style': Style.BRIGHT,
            'icon': 'â ï¸'
        },
        'ERROR': {
            'color': Fore.RED,
            'style': Style.BRIGHT,
            'icon': 'â'
        },
        'CRITICAL': {
            'color': Fore.WHITE,
            'style': Style.BRIGHT,
            'bg': Back.RED,
            'icon': 'ð'
        }
    }

    def format(self, record: logging.LogRecord) -> str:
        colored_record = copy.copy(record)

        # Get color scheme
        scheme = self.COLORS.get(record.levelname, {
            'color': Fore.WHITE,
            'style': Style.NORMAL,
            'icon': 'â¢'
        })

        # Format timestamp
        timestamp = datetime.fromtimestamp(record.created).strftime('%H:%M:%S.%f')[:-3]

        # Get file location
        file_location = f"{os.path.basename(record.pathname)}:{record.lineno}"

        # Build components
        components = []

        # log formatting
        components.extend([
            f"{Fore.BLUE}{timestamp}{Style.RESET_ALL}",
            f"{Fore.WHITE}{Style.DIM}{file_location}{Style.RESET_ALL}",
            f"{scheme['color']}{scheme['style']}{scheme['icon']} {record.levelname:8}{Style.RESET_ALL}",
            f"{scheme['color']}{record.msg}{Style.RESET_ALL}"
        ])

        # Add exception info
        if record.exc_info:
            components.append(
                f"\n{Fore.RED}{Style.BRIGHT}"
                f"{''.join(traceback.format_exception(*record.exc_info))}"
                f"{Style.RESET_ALL}"
            )

        return " | ".join(components)


def setup_logger(
        name: Optional[Union[str, Path]] = None,
        level: int = logging.DEBUG
) -> logging.Logger:
    """
    Setup a colored logger with VLLM override and file location

    Args:
        name: Logger name or __file__ for module name
        level: Logging level
    """
    # Get logger name from file path
    if isinstance(name, (str, Path)) and Path(name).suffix == '.py':
        name = Path(name).stem

    # Get or create logger
    logger = logging.getLogger(name)
    logger.setLevel(level)

    # Only add handler if none exists
    if not logger.handlers:
        # Create console handler
        console_handler = logging.StreamHandler(sys.stdout)
        console_handler.setFormatter(ColoredFormatter())
        logger.addHandler(console_handler)

        # Override VLLM loggers to use our logger
        VLLMLogOverrider(logger)

    return logger


def set_vllm_logging_level(level: logging):
    """
    Set the logging level for VLLM loggers

    Args:
        level: Logging level to set (e.g., logging.INFO, logging.ERROR)
    """
    for name in logging.root.manager.loggerDict:
        if name.startswith('vllm'):
            vllm_logger = logging.getLogger(name)
            vllm_logger.setLevel(level)


================================================================================
# File: auralis/common/metrics/__init__.py
# Path: /home/marco/PycharmProjects/betterVoiceCraft/Auralis/src/auralis/common/metrics/__init__.py
================================================================================

#  Copyright (c) 2024 Astramind. Licensed under Apache License, Version 2.0.



================================================================================
# File: auralis/common/metrics/profiling.py
# Path: /home/marco/PycharmProjects/betterVoiceCraft/Auralis/src/auralis/common/metrics/profiling.py
================================================================================

import sys
import torch
import functools
import inspect
from typing import Optional, Callable


class CUDALineProfiler:
    def __init__(self, func_name: str):
        self.func_name = func_name
        self.last_mem = torch.cuda.memory_allocated() / (1024 * 1024)
        self.original_trace = None
        self.first_line = None
        self.function_source = []
        self.changes = {}  # Memorizza i cambiamenti per linea

    def line_profiler(self, frame, event, arg):
        if event != 'line':
            return self.original_trace if self.original_trace else None

        # Inizializza al primo frame
        if not self.function_source:
            source_lines = inspect.getsource(frame.f_code).splitlines()
            self.function_source = [line for line in source_lines if line.strip()]
            self.first_line = frame.f_code.co_firstlineno

        try:
            curr_mem = torch.cuda.memory_allocated() / (1024 * 1024)
            delta = curr_mem - self.last_mem

            if abs(delta) > 1.0:  # Salva solo cambiamenti > 1MB
                self.changes[frame.f_lineno] = (curr_mem, delta)

            self.last_mem = curr_mem

        except Exception as e:
            print(f"Error in profiler: {e}")

        return self.original_trace if self.original_trace else None

    def print_profile(self):
        print(f"\nCUDA Memory profiling for {self.func_name}:")
        print(f"{'Line #':<10} {'CUDA (MB)':<12} {'Increment':<12} {'Line Contents'}")
        print("=" * 70)

        current_line = self.first_line
        for line in self.function_source:
            if current_line in self.changes:
                curr_mem, delta = self.changes[current_line]
                print(f"{current_line:<10} {curr_mem:<12.2f} {delta:>+12.2f}  {line}")
            else:
                print(f"{current_line:<10} {'':24} {line}")
            current_line += 1


def profile_cuda(func: Callable):
    @functools.wraps(func)
    async def wrapper(*args, **kwargs):
        profiler = CUDALineProfiler(func.__name__)
        profiler.original_trace = sys.gettrace()
        sys.settrace(profiler.line_profiler)

        try:
            result = await func(*args, **kwargs)
            return result
        finally:
            sys.settrace(profiler.original_trace)
            profiler.print_profile()

            final_mem = torch.cuda.memory_allocated() / (1024 * 1024)
            print("\nSummary:")
            print(f"Initial CUDA memory: {profiler.last_mem:.2f} MB")
            print(f"Final CUDA memory: {final_mem:.2f} MB")
            print(f"Peak CUDA memory: {torch.cuda.max_memory_allocated() / (1024 * 1024):.2f} MB")
            print(f"Net change: {final_mem - profiler.last_mem:.2f} MB")

    return wrapper

================================================================================
# File: auralis/common/metrics/performance.py
# Path: /home/marco/PycharmProjects/betterVoiceCraft/Auralis/src/auralis/common/metrics/performance.py
================================================================================

#  Copyright (c) 2024 Astramind. Licensed under Apache License, Version 2.0.

from functools import wraps
from typing import TypeVar, AsyncGenerator, Callable

from auralis.common.definitions.metrics.tracker import TTSMetricsTracker

T = TypeVar('T')


metrics = TTSMetricsTracker()


def track_generation(func: Callable[..., AsyncGenerator[T, None]]) -> Callable[..., AsyncGenerator[T, None]]:
    @wraps(func)
    async def wrapper(*args, **kwargs) -> AsyncGenerator[T, None]:
        async for output in func(*args, **kwargs):
            if output.start_time:
                audio_seconds = output.array.shape[0] / output.sample_rate

                if metrics.update_metrics(output.token_length, audio_seconds):
                    metrics.logger.info(
                        f"Generation metrics | "
                        f"Throughput: {metrics.requests_per_second:.2f} req/s | "
                        f"{metrics.tokens_per_second:.1f} tokens/s | "
                        f"Latency: {metrics.ms_per_second_of_audio:.0f}ms per second of audio generated"
                    )
                    metrics.reset_window()
            yield output

    return wrapper

================================================================================
# File: auralis/entrypoints/__init__.py
# Path: /home/marco/PycharmProjects/betterVoiceCraft/Auralis/src/auralis/entrypoints/__init__.py
================================================================================

#  Copyright (c) 2024 Astramind. Licensed under Apache License, Version 2.0.



================================================================================
# File: auralis/entrypoints/oai_server.py
# Path: /home/marco/PycharmProjects/betterVoiceCraft/Auralis/src/auralis/entrypoints/oai_server.py
================================================================================

#  Copyright (c) 2024 Astramind. Licensed under Apache License, Version 2.0.

import argparse
import base64
import json
import logging
import uuid
from typing import Optional

import aiohttp
import uvicorn
from auralis.common.definitions.protocols.openai import VoiceChatCompletionRequest, AudioSpeechGenerationRequest
from fastapi import FastAPI, Header, HTTPException, Response
from fastapi.responses import JSONResponse
from starlette.responses import StreamingResponse

from auralis.core.tts import TTS

app = FastAPI()

tts_engine: TTS

logger_str_to_logging={
    "info": logging.INFO,
    "warn": logging.WARNING,
    "err": logging.ERROR
}

def start_tts_engine(args, logging_level):
    global tts_engine
    tts_engine = (TTS(
        scheduler_max_concurrency=args.max_concurrency,
        vllm_logging_level=logging_level)
    .from_pretrained(
        args.model, gpt_model=args.gpt_model
    ))

@app.post("/v1/audio/speech")
async def generate_audio(request: AudioSpeechGenerationRequest):

    try:
        # Create TTSRequest with default params and auralis overrides
        tts_request = request.to_tts_request()

        output = await tts_engine.generate_speech_async(tts_request)
        output = output.change_speed(request.speed)
        audio_bytes = output.to_bytes(request.response_format)

        return Response(content=audio_bytes, media_type=f"audio/{request.response_format}")

    except Exception as e:
        return JSONResponse(status_code=500, content={"error": f"Error generating audio: {str(e)}"})


@app.post("/v1/chat/completions")
async def chat_completions(request: VoiceChatCompletionRequest, authorization: Optional[str] = Header(None)):
    if not authorization or not authorization.startswith("Bearer "):
        return JSONResponse(
            status_code=400,
            content={"error": "Authorization header with Bearer token is required"}
        )
    try:
        # Rest of the parameters
        openai_api_key = authorization[len("Bearer "):]
        modalities = request.modalities
        num_of_token_to_vocalize = request.vocalize_at_every_n_words

        # Initialize TTS request with auralis parameters
        tts_request = request.to_tts_request(text='')

        # Prepare OpenAI request
        openai_request_data = request.to_openai_request()

        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {openai_api_key}"
        }

        tts_request.context_partial_function = await tts_engine.prepare_for_streaming_generation(tts_request)
        request_id = uuid.uuid4().hex

        # Validate modalities
        valid_modalities = ['text', 'audio']
        if not all(m in valid_modalities for m in modalities):
            return JSONResponse(
                status_code=400,
                content={"error": f"Invalid modalities. Must be one or more of {valid_modalities}"}
            )

        async def stream_generator():
            accumulated_content = ""

            try:
                async with aiohttp.ClientSession() as session:
                    async with session.post(request.openai_api_url, json=openai_request_data, headers=headers) as resp:
                        if resp.status != 200:
                            error_response = await resp.text()
                            raise HTTPException(status_code=resp.status, detail=error_response)

                        async for line in resp.content:
                            if not line:
                                continue

                            line = line.decode("utf-8").strip()
                            if not line.startswith("data:"):
                                continue

                            data_str = line[5:].strip()
                            if data_str == "[DONE]":
                                break

                            try:
                                data = json.loads(data_str)
                                content = data.get("choices", [{}])[0].get("delta", {}).get("content", "")

                                if content:
                                    accumulated_content += content
                                    # Only yield text if text modality is requested
                                    if 'text' in modalities:
                                        yield f"data: {json.dumps(data)}\n\n"

                                    if len(accumulated_content.split()) >= num_of_token_to_vocalize:
                                        # Only generate and yield audio if audio modality is requested
                                        if 'audio' in modalities:
                                            tts_request.text = accumulated_content
                                            tts_request.infer_language()
                                            audio_output = await tts_engine.generate_speech_async(tts_request)
                                            audio_base64 = base64.b64encode(audio_output.to_bytes()).decode("utf-8")
                                            yield f"data: {json.dumps({'id': request_id, 'object': 'audio.chunk', 'data': audio_base64})}\n\n"

                                        accumulated_content = ""
                                elif 'text' in modalities:
                                    # Other non-content text events only if text modality is requested
                                    yield f"data: {json.dumps(data)}\n\n"

                            except json.JSONDecodeError:
                                continue

                # Process any remaining content for audio if needed
                if accumulated_content and 'audio' in modalities:
                    tts_request.text = accumulated_content
                    tts_request.infer_language()
                    audio_output = await tts_engine.generate_speech_async(tts_request)
                    audio_base64 = base64.b64encode(audio_output.to_bytes()).decode("utf-8")
                    yield f"data: {json.dumps({'id': request_id, 'object': 'audio.chunk', 'data': audio_base64})}\n\n"

                # Send completion messages only if text modality is requested
                if 'text' in modalities:
                    yield f"data: {json.dumps({'id': request_id, 'object': 'chat.completion.chunk', 'choices': [{'delta': {}, 'index': 0, 'finish_reason': 'stop'}]})}\n\n"
                yield "data: [DONE]\n\n"

            except Exception as e:
                yield f"data: {json.dumps({'error': str(e)})}\n\n"
            finally:
                if hasattr(tts_request, 'cleanup'):
                    tts_request.cleanup()

        return StreamingResponse(stream_generator(), media_type="text/event-stream")

    except Exception as e:
        return JSONResponse(status_code=500, content={"error": f"Error in chat completions: {str(e)}"})

def main():
    parser = argparse.ArgumentParser(description="Auralis TTS FastAPI Server")
    parser.add_argument("--host", type=str, default="127.0.0.1", help="Host to run the server on")
    parser.add_argument("--port", type=int, default=8000, help="Port to run the server on")
    parser.add_argument("--model",  type=str, default='AstraMindAI/xttsv2', help="The base model to run")
    parser.add_argument("--gpt_model", type=str, default='AstraMindAI/xtts2-gpt', help="The gpt model to load alongside the base model, if present")
    parser.add_argument("--max_concurrency", type=int, default=8, help="The concurrency value that is used in the TTS Engine, it is directly connected to the memory consumption")
    parser.add_argument("--vllm_logging_level", type=str, default='warn', help="The vllm logging level, could be one of [info | warn | err]")

    args = parser.parse_args()

    # Initialize the TTS engine
    logging_level = logger_str_to_logging.get(args.vllm_logging_level, None)
    if not logging_level:
        raise ValueError("The logging level for vllm was not correct, please choose between ['info' | 'warn' | 'err']")

    start_tts_engine(args, logging_level)

    uvicorn.run(
        "auralis.entrypoints.oai_server:app",
        host=args.host,
        port=args.port,
    )

if __name__ == "__main__":
    main()

================================================================================
# File: auralis/models/__init__.py
# Path: /home/marco/PycharmProjects/betterVoiceCraft/Auralis/src/auralis/models/__init__.py
================================================================================

#  Copyright (c) 2024 Astramind. Licensed under Apache License, Version 2.0.

from .xttsv2 import XTTSv2Engine



================================================================================
# File: auralis/models/base.py
# Path: /home/marco/PycharmProjects/betterVoiceCraft/Auralis/src/auralis/models/base.py
================================================================================

#  Copyright (c) 2024 Astramind. Licensed under Apache License, Version 2.0.
import asyncio
from abc import ABC, abstractmethod
from contextlib import asynccontextmanager
from functools import wraps
from pathlib import Path
from typing import Union, Optional, AsyncGenerator, List, Callable

import torch
import torchaudio

from auralis.common.definitions.dto.output import TTSOutput
from auralis.common.definitions.dto.requests import TTSRequest
from auralis.common.definitions.scheduler.context import GenerationContext


class BaseAsyncTTSEngine(ABC, torch.nn.Module):
    """
    Base interface for TTS engines.
    It assumes a three-phase generation process:
    1. Conditioning phase: where the audio conditioning is generated.
    2. Phonetic phase: where the audio tokens are generated.
    3. Speech phase: where the audio is generated.
    """

    ### Phases ###
    @abstractmethod
    async def preprocess_inputs(self, request: TTSRequest) -> List[GenerationContext]:
        """
        Preprocesses a TTS request, returning a GenerationContext object.
        This method should be implemented by subclasses.

        Args:
            request: TTS request object.

        Returns:
            A GenerationContext object.
        """
        raise NotImplementedError

    @abstractmethod
    async def conditioning_phase(
            self,
            request: TTSRequest,
    ) -> List[GenerationContext]:
        """
        This phase should be where the audio conditioning is generated.
        in XTTSv2 this is composed by a text embedding and a speaker embedding, as well as the voice cloning embedding

        Args:
            request: TTS request object.

        Returns:
            A list of generation context objects.
        """
        raise NotImplementedError

    @abstractmethod
    async def phonetic_phase(
            self,
            context: GenerationContext
    ) -> GenerationContext:
        """
        This phase should be where the audio tokens are generated.
        In XTTSv2 this is the part where the GPT model generates the phonetic tokens

        Args:
            context: A generation context object.

        Returns:
            A generation context object.
        """
        raise NotImplementedError

    @abstractmethod
    async def speech_phase(
            self,
            context: GenerationContext,
    ) -> TTSOutput:
        """
        This phase should be where the audio is generated.
        In XTTSv2 this is the part where the vocoder generates the audio

        Args:
            context: A generation context object.

        Returns:
            A TTSOutput object.
        """
        raise NotImplementedError

    ### Utilities ###

    @abstractmethod
    def get_memory_usage_curve(self):
        """Get memory usage curve by manually testing for vllm memory usage at different concurrency."""
        raise NotImplementedError

    @classmethod
    def from_pretrained(
            cls,
            *args,
            **kwargs
    ) -> 'BaseAsyncTTSEngine':
        """Load a pretrained model."""
        raise NotImplementedError


    @property
    def device(self):
        """Get the current device of the model."""
        return next(self.parameters()).device

    @property
    def dtype(self):
        """Get the current dtype of the model."""
        return next(self.parameters()).dtype

    @staticmethod
    def get_memory_percentage(memory: int) -> Optional[float]:
        """
        Estimate the memory occupation of the model on the GPU.

        The function tries to estimate the memory occupation of the model by
        checking the free memory on each GPU and summing up the memory
        required by the model. If the estimated memory occupation is between
        0 and 1, it is returned, otherwise None is returned.

        Args:
            memory (int): The memory required by the model in bytes.

        Returns:
            Optional[float]: The estimated memory occupation of the model
                between 0 and 1, or None if the estimation fails.
        """
        for i in range(torch.cuda.device_count()):
            free_memory, total_memory = torch.cuda.mem_get_info(i)
            used_memory = total_memory - free_memory
            estimated_mem_occupation = (memory + used_memory) / total_memory
            if estimated_mem_occupation > 0 and estimated_mem_occupation < 1:
                return estimated_mem_occupation
        return None

    @staticmethod
    def load_audio(audio_path: Union[str, Path], sampling_rate: int = 22050) -> torch.Tensor:
        """
        Loads an audio file into a tensor.

        Args:
            audio_path: path to the audio file.
            sampling_rate: target sampling rate.

        Returns:
            A tensor containing the audio data.
        """
        audio, lsr = torchaudio.load(audio_path)

        # Stereo to mono if needed
        if audio.size(0) != 1:
            audio = torch.mean(audio, dim=0, keepdim=True)

        if lsr != sampling_rate:
            audio = torchaudio.functional.resample(audio, lsr, sampling_rate)

        # Clip audio invalid values
        audio.clip_(-1, 1)
        return audio




================================================================================
# File: auralis/models/registry.py
# Path: /home/marco/PycharmProjects/betterVoiceCraft/Auralis/src/auralis/models/registry.py
================================================================================

#  Copyright (c) 2024 Astramind. Licensed under Apache License, Version 2.0.

from enum import Enum
from typing import Dict, Optional


class SupportedModelTypes(Enum):
    XTTSv2 = "xtts"


class ModelRegistry:
    _instance = None
    _models: Dict[str, dict] = {}

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance

    @classmethod
    def register_model(cls, model_type: SupportedModelTypes, config_converter=None, **model_info):
        def decorator(model_class):
            cls._models[model_type.value] = {
                'class': model_class,
                'config_converter': config_converter,
                **model_info
            }
            return model_class

        return decorator

    @classmethod
    def get_model_info(cls, model_type: SupportedModelTypes) -> Optional[dict]:
        return cls._models.get(model_type)

    @classmethod
    def get_model_class(self, model_type: SupportedModelTypes):
        return self._models[model_type]['class']

# Decorator to register a model
def register_tts_model(model_type: SupportedModelTypes, **kwargs):
    return ModelRegistry.register_model(model_type, **kwargs)

================================================================================
# File: auralis/models/xttsv2/__init__.py
# Path: /home/marco/PycharmProjects/betterVoiceCraft/Auralis/src/auralis/models/xttsv2/__init__.py
================================================================================

#  Copyright (c) 2024 Astramind. Licensed under Apache License, Version 2.0.

from vllm import ModelRegistry

from .XTTSv2 import XTTSv2Engine
from .components.vllm_mm_gpt import XttsGPT

ModelRegistry.register_model("XttsGPT", XttsGPT)


================================================================================
# File: auralis/models/xttsv2/XTTSv2.py
# Path: /home/marco/PycharmProjects/betterVoiceCraft/Auralis/src/auralis/models/xttsv2/XTTSv2.py
================================================================================

#  Copyright (c) 2024 Astramind. Licensed under Apache License, Version 2.0.

import asyncio
import functools
import time
import uuid
import weakref
from concurrent.futures import ThreadPoolExecutor
from pathlib import Path
from typing import Optional, List, Tuple, Union, AsyncGenerator, Dict

import librosa
import numpy as np
import torch
import torchaudio
from torch import nn
from vllm import AsyncLLMEngine, AsyncEngineArgs, TokensPrompt
from vllm.multimodal import MultiModalDataDict
from vllm.sampling_params import RequestOutputKind
from vllm.utils import Counter

from .components.tts.layers.xtts.hifigan_decoder import HifiDecoder
from .components.tts.layers.xtts.latent_encoder import ConditioningEncoder
from .components.tts.layers.xtts.perceiver_encoder import PerceiverResampler
from .components.vllm_mm_gpt import LearnedPositionEmbeddings
from .config.tokenizer import XTTSTokenizerFast
from .config.xttsv2_config import XTTSConfig
from .config.xttsv2_gpt_config import XTTSGPTConfig
from ..base import BaseAsyncTTSEngine
from ..registry import register_tts_model, SupportedModelTypes
from ...common.definitions.dto.output import TTSOutput
from ...common.definitions.dto.requests import TTSRequest
from ...common.definitions.scheduler.context import GenerationContext
from ...common.logging.logger import setup_logger
from ...common.metrics.profiling import profile_cuda
from ...common.utilities import wav_to_mel_cloning, load_audio
from ...common.vllm.hidden_state_collector import HiddenStatesCollector
from ...common.vllm.hijack import ExtendedSamplingParams, LogitsRepetitionPenalizer

def mock_context_data(ctx):
    """Return the worst case scenario for profiling of the context generation part."""

    # one since we enforce mono, 60s since we enforce 60s max reference lenght
    placeholder_audio_tensor = [torch.zeros((1, ctx['input_sample_rate'] * 60),
                                            device=ctx['device'],
                                            # 5 is the hard limit for reference files
                                            dtype=ctx['dtype'])] * 5 * ctx['concurrences'][0]
    return GenerationContext(
        tokens=[torch.zeros(
            (1, ctx['max_sizes'][0]),
            dtype=torch.long,
            device=ctx['device'])
               ] * ctx['concurrences'][0],
        speaker_files=placeholder_audio_tensor)

def get_tensor_size_mb(tensor):
    return tensor.element_size() * tensor.nelement() / (1024 * 1024)

def mock_synth_data(ctx):
    """Return the worst case scenario for profiling."""
    max_seq_len = ctx['max_sizes'][2] # start and eos tokens and the conditioning sql
    placeholder_tensor = torch.zeros(
        (ctx['concurrences'][2], max_seq_len, ctx['hidden_size']),
        device=ctx['device'],
        dtype=ctx['dtype']
    )
    decoding_conditioning = torch.zeros(
        (ctx['concurrences'][2], 512, 1),
        device=ctx['device'],
        dtype=ctx['dtype']
    )
    return GenerationContext(
        spectrogram=placeholder_tensor,
        speaker_embeddings=decoding_conditioning)


@register_tts_model(
    model_type=SupportedModelTypes.XTTSv2,
    uses_vllm=True,
    supported_languages = ['en', 'es', 'fr', 'de', 'it', 'pt', 'pl', 'tr', 'ru',
                           'nl', 'cs', 'ar', 'zh-cn', 'ja', 'hu', 'ko', 'hi'],
    fake_data_factories = (mock_context_data, None, mock_synth_data),
)
class XTTSv2Engine(BaseAsyncTTSEngine):
    """Async XTTS model implementation using VLLM's AsyncEngine."""

    model_type: str = "xtts"

    def __init__(self,
                 hifi_config: XTTSConfig,
                 gpt_config: XTTSGPTConfig,
                 pipeline_parallel_size: int = 1,
                 tensor_parallel_size: int = 1,
                 **kwargs):
        super().__init__()

        self.max_gb_for_vllm_model = None
        self.logger = setup_logger(__file__)
        self.logger.info("Initializing XTTSv2Engine...")

        self.gpt_model = kwargs.pop('gpt_model')
        self.hifi_config = hifi_config
        self.gpt_config = gpt_config
        self.mel_bos_token_id = gpt_config.start_audio_token
        self.mel_eos_token_id = gpt_config.stop_audio_token
        self.tp = tensor_parallel_size
        self.pp = pipeline_parallel_size
        self.tokenizer = XTTSTokenizerFast.from_pretrained(self.gpt_model)
        self.request_counter = Counter()

        self.max_concurrency = kwargs.pop('scheduler_max_concurrency', 10)
        self.first_and_third_concurrency = max(1, self.max_concurrency // 6) * self.tp

        # Register buffer before creating modules
        self.register_buffer("mel_stats", torch.ones(80))

        # Initialize all nn.Module components
        self.conditioning_encoder = ConditioningEncoder(
            gpt_config.audio_config.mel_channels,
            gpt_config.hidden_size,
            num_attn_heads=gpt_config.num_attention_heads
        )

        self.text_embedding = nn.Embedding(
            gpt_config.number_text_tokens,
            gpt_config.hidden_size
        )

        self.text_pos_embedding = (
            LearnedPositionEmbeddings(
                gpt_config.max_text_tokens + 2,
                gpt_config.hidden_size,
                supports_pp=False
            )
            if gpt_config.max_audio_tokens != -1
            else functools.partial(gpt_config.null_position_embeddings, dim=gpt_config.hidden_size)
        )

        self.conditioning_perceiver = PerceiverResampler(
            dim=gpt_config.hidden_size,
            depth=2,
            dim_context=gpt_config.hidden_size,
            num_latents=32,
            dim_head=64,
            heads=8,
            ff_mult=4,
            use_flash_attn=False,
        )

        # Initialize HiFi-GAN decoder
        self.hifigan_decoder = HifiDecoder(
            input_sample_rate=self.hifi_config.input_sample_rate,
            output_sample_rate=self.hifi_config.output_sample_rate,
            output_hop_length=self.hifi_config.output_hop_length,
            ar_mel_length_compression=self.hifi_config.gpt_code_stride_len,
            decoder_input_dim=self.hifi_config.decoder_input_dim,
            d_vector_dim=self.hifi_config.d_vector_dim,
            cond_d_vector_in_each_upsampling_layer=self.hifi_config.cond_d_vector_in_each_upsampling_layer,
        )

        self.final_norm = nn.LayerNorm(gpt_config.hidden_size, eps=1e-5, bias=True)

        # Kept for model loading purposes
        self.text_head = nn.Linear(gpt_config.hidden_size, gpt_config.number_text_tokens, bias=True)

        self.get_memory_usage_curve()

        # Initialize VLLM engine at the end, settings its concurrency
        self.init_vllm_engine(self.max_concurrency)

        # Semaphore for concurrency control of the encoding process
        self.eval()

    @property
    def config(self):
        return vars(self.gpt_config) | vars(self.hifi_config) | {
            "concurrences": (self.first_and_third_concurrency,self.max_concurrency, self.first_and_third_concurrency),
            "max_sizes":(
            (vars(self.gpt_config)['max_text_tokens']),
            None,
            vars(self.gpt_config)['max_text_tokens'] + (vars(self.gpt_config)['max_audio_tokens']) + 32 + 5  # start and eos tokens and the conditioning sql
            ),
            "dtype": self.dtype,
            "device": self.device
        }

    def get_memory_usage_curve(self):
        # empirically found values
        x = np.array([2, 5, 10, 16])
        y = np.array([1.25, 1.35, 1.45, 1.625])

        # polynomial fit
        coefficients = np.polyfit(x, y, 2)

        # create a polynomial object
        self.max_gb_for_vllm_model = (coefficients[0] * self.max_concurrency ** 2 +
                    coefficients[1] * self.max_concurrency +
                    coefficients[2])

    def half(self):
        self.logger.warning("Cannot call .half() on XTTSv2Engine. it will be ignored.")
        # We cannot permit downcasting since it will throw an error while padding
        return

    def to(self, *args, **kwargs):
        # Block downcasting
        dtype = kwargs.get('dtype', None)
        if dtype == torch.float16 or dtype == torch.bfloat16:
            self.logger.warning("Cannot cast to half precision. Ignoring the request.")
            kwargs['dtype'] = torch.float32
        elif len(args) > 0 and (args[0] == torch.float16 or args[0] == torch.bfloat16):
            self.logger.warning("Cannot cast to half precision. Ignoring the request.")
            args = list(args)
            args[0] = torch.float32
            args = tuple(args)
        return super().to(*args, **kwargs)

    def init_vllm_engine(self, concurrency):
        """Initialize models with AsyncVLLMEngine."""
        max_seq_num = concurrency
        mem_utils = self.get_memory_percentage(self.max_gb_for_vllm_model * 1024 ** 3) #
        if not mem_utils:
            raise RuntimeError("Could not find the memory usage for the VLLM model initialization.")
        engine_args = AsyncEngineArgs(
            model=self.gpt_model,
            tensor_parallel_size=self.tp,
            pipeline_parallel_size=self.pp,
            dtype="auto",
            max_model_len=self.gpt_config.max_text_tokens +
                          self.gpt_config.max_audio_tokens +
                          32 + 5 + 3, # this is from the xttsv2 code, 32 is the conditioning sql
            gpu_memory_utilization=mem_utils,
            trust_remote_code=True,
            enforce_eager=True,
            limit_mm_per_prompt={"audio": 1}, # even if more audio are present, they'll be condendesed into one
            max_num_seqs=max_seq_num,
            disable_log_stats=True, # temporary fix for the log stats, there is a known bug in vllm that will be fixed in the next relaese
            max_num_batched_tokens=(self.gpt_config.max_text_tokens +
                                    self.gpt_config.max_audio_tokens +
                                    32 + 5 + 3) * max_seq_num,
            #We round to the nearest multiple of 32 and multiply by max_seq_num to get the max batched number (arbitrary) of tokens
        )
        self.logger.info(f"Initializing VLLM engine with args: {engine_args}")
        self.llm_engine = AsyncLLMEngine.from_engine_args(engine_args)

    @classmethod
    def from_pretrained(
            cls,
            pretrained_model_name_or_path: str,
            torch_dtype: torch.dtype = torch.float32,
            device_map: Optional[str] = "auto",
            tensor_parallel_size: int = 1,
            pipeline_parallel_size: int = 1,
            **kwargs,
    ) -> nn.Module:
        """Load pretrained XTTS model from HuggingFace Hub."""
        from huggingface_hub import hf_hub_download
        import json
        import os

        # Download and load configs
        if not os.path.exists(pretrained_model_name_or_path):
            config_file = hf_hub_download(
                repo_id=pretrained_model_name_or_path,
                filename="config.json"
            )
            with open(config_file, 'r') as f:
                config = json.load(f)

        else:
            # Load from local path
            with open(os.path.join(pretrained_model_name_or_path, "config.json"), 'r') as f:
                config = json.load(f)

        # Initialize configs
        gpt_config = XTTSGPTConfig(**config['gpt_config'])
        hifi_config = XTTSConfig(**config)

        # Initialize model
        model = cls(
            hifi_config=hifi_config,
            gpt_config=gpt_config,
            tensor_parallel_size=tensor_parallel_size,
            pipeline_parallel_size=pipeline_parallel_size,
            **kwargs
        )

        # Load model weights
        if not os.path.exists(pretrained_model_name_or_path):
            hifigan_weights = hf_hub_download(
                repo_id=pretrained_model_name_or_path,
                filename="xtts-v2.safetensors"
            )
        else:
            hifigan_weights = os.path.join(pretrained_model_name_or_path, "xtts-v2.safetensors")

        import safetensors.torch

        # Load HiFi-GAN weights
        hifigan_state = safetensors.torch.load_file(hifigan_weights)
        model.load_state_dict(hifigan_state)

        # Cast model to specified dtype
        model = model.to(torch_dtype)
        model = model.to('cuda')

        return model

    async def _merge_conditioning(self,
                                  text_conditioning: List[torch.Tensor],
                                  audio_conditioning: torch.Tensor) -> List[torch.Tensor]:
        cond_latents = []
        for text_embedding in text_conditioning:
            # Concatenate along sequence dimension
            cond_latents.append((torch.cat([audio_conditioning, text_embedding], dim=1).squeeze(0)
                                 .to(self.llm_engine.engine.model_config.dtype)))
        return cond_latents

    async def _get_speaker_embedding(self,
                                     audio_list: List[torch.Tensor],
                                     sr: int) -> (
            Tuple)[torch.Tensor, torch.Tensor]: # ([bs, embeddings], [, audio])
        # here we could not batch the inputs, because we cannot guarantee that the audio is the same length
        # we would have to modify the model to accept a padding mask

        audios=[]
        for audio in audio_list:
            audio_16k = torchaudio.functional.resample(audio, sr, 16000)
            audios.append (
                (
                    self.hifigan_decoder.speaker_encoder.forward(
                    audio_16k.to(self.device), l2_norm=True)
                ).unsqueeze(-1)
                .to(self.device)
            )
        stacked_audio = torch.stack(audios).mean(dim=0)
        del audios # it'll help the gc
        return stacked_audio, torch.cat(audio_list, dim=-1)


    def get_gpt_cond_latents(self, audio, sr, length: int = 30, chunk_length: int = 6):
        """Compute the conditioning latents for the GPT model from the given audio."""
        if sr != 22050:
            audio = torchaudio.functional.resample(audio, sr, 22050)
        if length > 0:
            audio = audio[:, : 22050 * length]
        if self.gpt_config.use_perceiver_resampler:
            style_embs = []
            for i in range(0, audio.shape[1], 22050 * chunk_length):
                audio_chunk = audio[:, i: i + 22050 * chunk_length]

                # if the chunk is too short ignore it
                if audio_chunk.size(-1) < 22050 * 0.33:
                    continue

                mel_chunk = wav_to_mel_cloning(
                    audio_chunk,
                    mel_norms=self.mel_stats.cpu(),
                    n_fft=2048,
                    hop_length=256,
                    win_length=1024,
                    power=2,
                    normalized=False,
                    sample_rate=22050,
                    f_min=0,
                    f_max=8000,
                    n_mels=80,
                )
                style_emb = self.get_style_emb(mel_chunk.to(self.device), None)
                style_embs.append(style_emb)

            # mean style embedding
            cond_latent = torch.stack(style_embs).mean(dim=0)
        else:
            mel = wav_to_mel_cloning(
                audio,
                mel_norms=self.mel_stats.cpu(),
                n_fft=4096,
                hop_length=1024,
                win_length=4096,
                power=2,
                normalized=False,
                sample_rate=22050,
                f_min=0,
                f_max=8000,
                n_mels=80,
            )
            cond_latent = self.get_style_emb(mel.to(self.device))
        return cond_latent.transpose(1, 2)

    async def get_conditioning_latents(
            self,
            context: GenerationContext,
            librosa_trim_db=None,
            sound_norm_refs=False,
            load_sr=22050,
    ):
        """Get the conditioning latents for the GPT model from the given audio."""
        # Deal with multiple references
        assert (isinstance(context.speaker_files, bytes) or
                isinstance(context.speaker_files, str) or
                isinstance(context.speaker_files, list) or
                isinstance(context.speaker_files, torch.Tensor)# for profiling
                ), \
            f"speaker_files must be a string, byte or a list but it is {type(context.speaker_files)}"

        if not isinstance(context.speaker_files, list):
            audio_paths = [context.speaker_files]
        else:
            audio_paths = context.speaker_files

        audios = []
        for file_path in audio_paths:
            audio = load_audio(file_path, load_sr) if not isinstance(file_path, torch.Tensor) else file_path
            audio = audio[:, : load_sr * context.max_ref_length].to(self.device).to(self.dtype)
            if sound_norm_refs:
                audio = (audio / torch.abs(audio).max()) * 0.75
            if librosa_trim_db is not None:
                audio = librosa.effects.trim(audio, top_db=librosa_trim_db)[0]
            audios.append(audio)

        # Compute latents for the decoder
        speaker_embedding, full_audio = await self._get_speaker_embedding(audios, load_sr)

        # Merge all the audios and compute the latents for the GPT
        gpt_cond_latents = self.get_gpt_cond_latents(
            full_audio, load_sr, length=context.gpt_cond_len, chunk_length=context.gpt_cond_chunk_len
        )  # [1, 1024, T]

        return gpt_cond_latents, speaker_embedding

    def get_style_emb(self, cond_input: torch.Tensor, return_latent: Optional[bool] = False) -> torch.Tensor:
        """Get conditioning embeddings from mel spectrograms."""
        if not return_latent:
            if cond_input.ndim == 4:
                cond_input = cond_input.squeeze(1)
            conds = self.conditioning_encoder(cond_input)

            if hasattr(self, 'conditioning_perceiver'):
                conds = self.conditioning_perceiver(
                    conds.permute(0, 2, 1)
                ).transpose(1, 2) # (b,d,32)
        else:
            conds = cond_input.unsqueeze(1)
        return conds

    async def preprocess_inputs(self, request: TTSRequest) -> List[GenerationContext]:
        """
        Preprocess a TTSRequest to prepare it for text-to-speech generation.

        This method handles the token elaboration, adding special beginning-of-sequence
        and end-of-sequence tokens to the provided text tokens, and converts them
        into a suitable tensor format for subsequent processing.

        Args:
            request (TTSRequest): The text-to-speech request containing the necessary
                                  information for processing, including the input text.

        Returns:
            GenerationContext: The context required for the generation process,
                               including the processed tokens.
        """
        async def elaborate_tokens(text_tokens: List[int]) -> torch.Tensor:

            text_tokens.insert(0, self.tokenizer.bos_token_id)
            text_tokens.append(self.tokenizer.eos_token_id)
            return torch.tensor(text_tokens).unsqueeze(0).to(self.text_embedding.weight.device)

        if isinstance(request.text, str):
            self.logger.debug(f"Preparing text tokens for text: {request.text}")

            if request.split_text:
                text_tokens = self.tokenizer.batch_encode_with_split(request.text, lang=[request.language])
                context = GenerationContext.from_request(request)
                contexts_for_generations=[]
                for idx, text_token in enumerate(text_tokens):

                    new_context = context.copy()
                    new_context.update(
                        request_id=None, # To generate a new request id
                        tokens=[await elaborate_tokens(text_token)],
                    )
                    contexts_for_generations.append(new_context)
                return contexts_for_generations
            else:
                text_tokens = self.tokenizer(request.text, lang=[request.language])['input_ids'][0]
                text_tokens = await elaborate_tokens(text_tokens)
                return [GenerationContext.from_request(request, tokens=text_tokens)]


    async def prepare_text_tokens_and_embeddings_async(self, context: GenerationContext) \
            -> Tuple[List[Union[int, List[int]]], List[torch.Tensor]]:
        """
        Prepare the text tokens and their embeddings asynchronously.

        This method takes a GenerationContext, and asynchronously prepares the text
        tokens and their embeddings. It first embeds the text tokens using the
        text_embedding and text_pos_embedding layers. It then prepares the tokens
        and their embeddings by calling prepare_token_and_textual_embeddings on each
        of the tokens.

        Args:
            context (GenerationContext): The GenerationContext containing the text
                                         tokens and other information.

        Returns:
            Tuple[List[Union[int, List[int]]], List[torch.Tensor]]: A tuple containing
                the prepared tokens and their embeddings.
        """

        async def embed_tokens(text_tokens: Union[torch.Tensor, List[torch.Tensor]]) -> List[torch.Tensor]:
            return self.text_embedding(text_tokens) + self.text_pos_embedding(text_tokens)

        async def prepare_token_and_textual_embeddings(tokens: Union[torch.Tensor, List[torch.Tensor]]) -> \
            Tuple[List[Union[int, List[int]]], List[torch.Tensor]]:
            return [1] * tokens.shape[-1], await embed_tokens(tokens)

        if isinstance(context.tokens, torch.Tensor):
            context.tokens = [context.tokens]

        prepared_tokens = []
        prepared_embeddings = []
        for token in context.tokens:
            results = await prepare_token_and_textual_embeddings(token)
            prepared_tokens.append(results[0])
            prepared_embeddings.append(results[1])

        return prepared_tokens, prepared_embeddings


    async def prepare_inputs_async(self,
                                   context: GenerationContext) \
            -> (Tuple)[List[List[int]], List[torch.Tensor], torch.Tensor]:
        """Prepare input text with conditioning tokens. Return combined conditioning latents"""
        # Tokenize text based on the language
        text_tokens, text_embeddings = await self.prepare_text_tokens_and_embeddings_async(context)

        # Load the speaker file and convert it to a tensor
        gpt_cond_latent, speaker_embeddings = await self.get_audio_conditioning(
            context
        )

        cond_latents = await self._merge_conditioning(text_embeddings, gpt_cond_latent)
        return text_tokens, cond_latents, speaker_embeddings

    async def get_audio_conditioning(
            self,
            context: GenerationContext,
            librosa_trim_db=None,
            sound_norm_refs=False,
            load_sr=22050,
    ):
        """Async version of get_conditioning_latents with concurrency control."""

        # Run the original get_conditioning_latents in executor
        result = await self.get_conditioning_latents(
            context,
            librosa_trim_db,
            sound_norm_refs,
            load_sr
        )
        return result

    async def get_model_logits(
            self,
            token_ids: List[int],
            conditioning: MultiModalDataDict,
            request_id: str,
    ) -> torch.Tensor:
        """
        Get model logits for a request with retry logic for empty hidden states.

        Args:
            token_ids: Input token IDs
            conditioning: Conditioning data
            request_id: Unique request ID
        """
        request_id = f"{request_id}_logits"


        # Reset token_ids on each attempt
        token_ids = ([self.mel_bos_token_id] + list(token_ids) + [self.mel_eos_token_id] * 4)
        # we need 5 eos tokens

        engine_inputs = TokensPrompt(prompt_token_ids=token_ids)
        conditioning['audio']['sequence_length'] = len(token_ids)

        engine_inputs["multi_modal_data"] = conditioning

        hidden_states_collector = HiddenStatesCollector()
        # Bind the collector to this request
        bound_collector = hidden_states_collector.bind_to_request(request_id)

        # Set up sampling parameters with the bound collector
        sampling_params = ExtendedSamplingParams(
            detokenize=False,
            request_id=request_id,
            max_tokens=1,
            hidden_state_collector=weakref.proxy(bound_collector),
            output_kind=RequestOutputKind.FINAL_ONLY
        )

        # Generate with unique request ID
        generator = self.llm_engine.generate(
            prompt=engine_inputs,
            sampling_params=sampling_params,
            request_id=request_id
        )

        async for output in generator:  # consume the generator
            if output.finished:
                pass


        # Get the collected hidden states
        hidden_states = await hidden_states_collector.get_hidden_states(request_id)

        del hidden_states_collector # explicitly delete it since vllm might hang to it

        if hidden_states is None:
            raise RuntimeError(
                f"No hidden states collected for request {request_id}. "
                f"This should never happen! Please report this issue on GitHub."
            )
        start_of_audio_hs = conditioning["audio"]["embeds"].shape[0] # type: ignore
        # Successfully got hidden states
        return self.final_norm(hidden_states[start_of_audio_hs:-5, ...].unsqueeze(0).to(self.device).to(self.dtype))

    async def conditioning_phase(
            self,
            contexts: GenerationContext,
    ) -> Optional[List[GenerationContext]]:
        """
        Performs the conditioning phase for text-to-speech generation.

        This method prepares input tokens, GPT embeddings, and speaker embeddings
        for one or multiple generation contexts. It processes them concurrently
        and creates new contexts for each generation combination.

        Args:
            contexts: Single GenerationContext or list of GenerationContext objects
                     containing the input parameters for generation.

        Returns:
            Optional[List[GenerationContext]]: List of new generation contexts with
            prepared tokens, embeddings, and modifiers. Each context is ready for
            the generation phase.
        """
        def is_nested(lst):
            """
            Check if a list contains any nested lists.

            Args:
                lst: A list of items.

            Returns:
                bool: True if the list contains any nested lists, False otherwise.
            """
            return any(isinstance(item, list) for item in lst)

        # Unpack results into separate lists
        tokens, gpt_embed_input, speaker_embeddings = await self.prepare_inputs_async(contexts)

        # Create new contexts for each generation combination
        contexts_for_generations = []
        for idx, (token_seq, single_gpt_embed_input) in enumerate(zip(tokens, gpt_embed_input)):
                # Create a new context with updated values
                new_context = contexts.copy()
                new_context.update(
                    request_id=new_context.request_id+f'_{idx}',
                    tokens=token_seq[0] if is_nested(token_seq) else token_seq,
                    decoding_embeddings_modifier=single_gpt_embed_input,
                    speaker_embeddings=speaker_embeddings
                )
                contexts_for_generations.append(new_context)

        del (tokens,
            gpt_embed_input,
            single_gpt_embed_input,
            contexts,
            speaker_embeddings,
            new_context,
            token_seq,

        ) # delete the references to gc can collect

        return contexts_for_generations

    async def phonetic_phase(self, context: GenerationContext) -> GenerationContext:
        logit_processor = LogitsRepetitionPenalizer(context.repetition_penalty)
        sampling_params = ExtendedSamplingParams(
            temperature=context.temperature,
            top_p=context.top_p,
            detokenize=False,
            request_id=uuid.uuid4(),
            top_k=context.top_k,
            logits_processors=[logit_processor],
            repetition_penalty=1.0,  # Since we're handling repetition penalty manually
            max_tokens=self.gpt_config.gpt_max_audio_tokens,
            ignore_eos=True,  # Ignore the tokenizer eos token since it is for textual generation
            stop_token_ids=[self.mel_eos_token_id],
            output_kind=RequestOutputKind.FINAL_ONLY
        )

        del logit_processor # explicitly delete it since vllm might hang to it

        engine_inputs = TokensPrompt(prompt_token_ids=context.tokens)
        if context.decoding_embeddings_modifier is not None:
            engine_inputs["multi_modal_data"] = {
                "audio": {
                    "embeds": context.decoding_embeddings_modifier,
                    "is_logits_only_mode": False,
                    "sequence_length": len(context.tokens)
                }
            }

        request_id =f"{context.request_id}"
        # Get audio token generator from VLLM
        token_generator = self.llm_engine.generate(
            prompt=engine_inputs,
            sampling_params=sampling_params,
            request_id=request_id,
        )

        async for output in token_generator:

            if output.finished:
                # get the hidden states
                context.spectrogram = await self.get_model_logits(
                    list(output.outputs[0].token_ids),
                    {
                        "audio": {
                            'embeds': context.decoding_embeddings_modifier,  # Use multimodal data for conditioning
                            "is_logits_only_mode": True,
                            "sequence_length": False # will be inserted later in the decoding process
                        },
                    },
                    output.request_id
                )
                context.tokens = list(output.outputs[0].token_ids)

        return context


    async def speech_phase(
            self,
            context: GenerationContext,
    ) -> TTSOutput:
        """
        Process tokens to speech using a vocoder
        """
        context.to(self.hifigan_decoder)
        wav = (self.hifigan_decoder(
                        context.spectrogram,
                        g=context.speaker_embeddings
                    )).cpu().detach().numpy().squeeze()

        # return the audio output
        return TTSOutput(
                        request_id = context.request_id,
                        parent_request_id = context.parent_request_id,
                        array= wav,
                        start_time = context.start_time,
                        end_time = time.time(),
                        token_length = len(context.tokens or []),
                        )

    async def shutdown(self):
        self.llm_engine.shutdown_background_loop()



================================================================================
# File: auralis/models/xttsv2/config/__init__.py
# Path: /home/marco/PycharmProjects/betterVoiceCraft/Auralis/src/auralis/models/xttsv2/config/__init__.py
================================================================================

#  Copyright (c) 2024 Astramind. Licensed under Apache License, Version 2.0.



================================================================================
# File: auralis/models/xttsv2/config/xttsv2_config.py
# Path: /home/marco/PycharmProjects/betterVoiceCraft/Auralis/src/auralis/models/xttsv2/config/xttsv2_config.py
================================================================================

#  Copyright (c) 2024 Astramind. Licensed under Apache License, Version 2.0.

from dataclasses import asdict, dataclass
from typing import Dict, Optional, List

from transformers.configuration_utils import PretrainedConfig
from transformers.utils import logging

logger = logging.get_logger(__name__)


@dataclass
class GPTAudioConfig:
    """Configuration for GPT audio processing parameters"""
    mel_channels: int = 80
    sample_rate: int = 22050
    output_sample_rate: int = 24000

@dataclass
class XTTSAudioConfig:
    """Configuration for audio processing parameters"""
    sample_rate: int = 22050
    output_sample_rate: int = 24000
    mel_channels: int = 80
    hop_length: int = 256
    win_length: int = 1024
    n_fft: int = 1024
    fmin: int = 0
    fmax: int = 8000
    power: float = 1.0
    mel_norms_file: Optional[str] = None


class XTTSGPTConfig(PretrainedConfig):
    """Configuration class for the GPT component of XTTS."""
    model_type = "xtts_gpt"

    def __init__(
            self,
            # Model architecture
            hidden_size: int = 1024,  # gpt_n_model_channels in original
            n_inner: int = 4096,
            num_hidden_layers: int = 30,  # gpt_layers in original
            num_attention_heads: int = 16,  # gpt_n_heads in original

            # Tokenizer settings
            vocab_size: int = 6681,  # gpt_number_text_tokens in original
            number_text_tokens: int = 6681,  # Explicit text token vocabulary size
            start_text_token: Optional[int] = None,
            stop_text_token: Optional[int] = None,

            # Audio token settings
            num_audio_tokens: int = 1026,  # gpt_num_audio_tokens in original
            start_audio_token: int = 1024,  # gpt_start_audio_token in original
            stop_audio_token: int = 1025,  # gpt_stop_audio_token in original

            # Sequence length settings
            max_audio_tokens: int = 605,  # gpt_max_audio_tokens in original
            max_text_tokens: int = 402,  # gpt_max_text_tokens in original
            max_prompt_tokens: int = 70,  # gpt_max_prompt_tokens in original
            gpt_max_audio_tokens: int = 605,  # Used for generation

            # Model behavior settings
            use_masking_gt_prompt_approach: bool = True,  # gpt_use_masking_gt_prompt_approach in original
            use_perceiver_resampler: bool = True,  # gpt_use_perceiver_resampler in original
            kv_cache: bool = True,
            enable_redaction: bool = False,

            # GPT batch settings
            gpt_batch_size: int = 1,

            # Audio processing
            audio_config: Optional[Dict] = None,

            # Architecture specifics
            layer_norm_epsilon: float = 1e-5,
            initializer_range: float = 0.02,
            add_cross_attention: bool = False,
            scale_attn_by_inverse_layer_idx: bool = False,
            reorder_and_upcast_attn: bool = False,

            # Size settings for the decoder
            decoder_input_dim: int = 1024,
            architectures=["XttsGPT"],
            auto_map={
                "AutoConfig": "AstraMindAI/xtts2-gpt--gpt_config.XTTSGPTConfig",
                "AutoModelForCausalLM": "AstraMindAI/xtts2-gpt--xtts2_gpt_modeling.XttsGPT",
            },
            activation_function: str = "gelu",
            attn_pdrop: float = 0.1,
            **kwargs
    ):
        super().__init__(**kwargs)
        self.architectures = architectures
        self.auto_map = auto_map
        self.audio_config = GPTAudioConfig(
            **audio_config if audio_config is not None else {}
        )
        self.activation_function = activation_function
        self.attn_pdrop = attn_pdrop
        self.hidden_size = hidden_size
        self.n_inner = n_inner
        self.num_hidden_layers = num_hidden_layers
        self.num_attention_heads = num_attention_heads

        self.vocab_size = vocab_size
        self.number_text_tokens = number_text_tokens
        self.start_text_token = start_text_token
        self.stop_text_token = stop_text_token

        self.num_audio_tokens = num_audio_tokens
        self.start_audio_token = start_audio_token
        self.stop_audio_token = stop_audio_token

        self.max_audio_tokens = max_audio_tokens
        self.max_text_tokens = max_text_tokens
        self.max_prompt_tokens = max_prompt_tokens
        self.gpt_max_audio_tokens = gpt_max_audio_tokens

        self.use_masking_gt_prompt_approach = use_masking_gt_prompt_approach
        self.use_perceiver_resampler = use_perceiver_resampler
        self.kv_cache = kv_cache
        self.enable_redaction = enable_redaction

        self.gpt_batch_size = gpt_batch_size

        self.layer_norm_epsilon = layer_norm_epsilon
        self.initializer_range = initializer_range
        self.add_cross_attention = add_cross_attention
        self.scale_attn_by_inverse_layer_idx = scale_attn_by_inverse_layer_idx
        self.reorder_and_upcast_attn = reorder_and_upcast_attn

        self.decoder_input_dim = decoder_input_dim

    def to_dict(self) -> Dict:
        """Convert the config to a dictionary."""
        output = super().to_dict()
        output["audio_config"] = asdict(self.audio_config)
        return output

    @classmethod
    def from_dict(cls, config_dict: Dict, *args, **kwargs) -> "XTTSGPTConfig":
        """Create a config from a dictionary."""
        return cls(**config_dict)


class XTTSConfig(PretrainedConfig):
    """Configuration class for XTTS model components except GPT."""
    model_type = "xtts"

    def __init__(
            self,
            # Audio settings
            audio_config: Optional[Dict] = None,
            input_sample_rate: int = 22050,
            output_sample_rate: int = 24000,
            output_hop_length: int = 256,

            # Model architecture
            decoder_input_dim: int = 1024,
            d_vector_dim: int = 512,
            cond_d_vector_in_each_upsampling_layer: bool = True,

            # Training settings
            gpt_code_stride_len: int = 1024,
            duration_const: int = 102400,

            # Tokenizer settings
            tokenizer_file: str = "",
            num_chars: int = 255,

            # Language support
            languages: Optional[List[str]] = None,

            # GPT configuration
            gpt_config: Optional[Dict] = None,
            architectures=["Xtts"],
            auto_map = {
                       "AutoConfig": "AstraMindAI/xtts2--xtts2_config.XTTSConfig",
                       "AutoModelForCausalLM": "AstraMindAI/xtts2--xtts2_modeling.Xtts",
                   },
            **kwargs
    ):
        super().__init__(**kwargs)
        self.architectures = architectures
        self.auto_map = auto_map
        # Initialize audio config
        self.audio_config = XTTSAudioConfig(
            **audio_config if audio_config is not None else {}
        )

        self.input_sample_rate = input_sample_rate
        self.output_sample_rate = output_sample_rate
        self.output_hop_length = output_hop_length

        self.decoder_input_dim = decoder_input_dim
        self.d_vector_dim = d_vector_dim
        self.cond_d_vector_in_each_upsampling_layer = cond_d_vector_in_each_upsampling_layer

        self.gpt_code_stride_len = gpt_code_stride_len
        self.duration_const = duration_const

        self.tokenizer_file = tokenizer_file
        self.num_chars = num_chars

        # Initialize GPT config
        self.gpt = XTTSGPTConfig(**gpt_config if gpt_config is not None else {})

        if languages is None:
            self.languages = [
                "en", "es", "fr", "de", "it", "pt", "pl", "tr", "ru",
                "nl", "cs", "ar", "zh-cn", "hu", "ko", "ja", "hi"
            ]
        else:
            self.languages = languages

    def to_dict(self) -> Dict:
        """Convert the config to a dictionary."""
        output = super().to_dict()
        output["audio_config"] = asdict(self.audio_config)
        output["gpt_config"] = self.gpt.to_dict()
        return output

    @classmethod
    def from_dict(cls, config_dict: Dict, *args, **kwargs) -> "XTTSConfig":
        """Create a config from a dictionary."""
        if "gpt_config" in config_dict:
            gpt_config = config_dict["gpt_config"]
            config_dict = {k: v for k, v in config_dict.items() if k != "gpt_config"}
            return cls(gpt_config=gpt_config, **config_dict)
        return cls(**config_dict)

================================================================================
# File: auralis/models/xttsv2/config/xttsv2_gpt_config.py
# Path: /home/marco/PycharmProjects/betterVoiceCraft/Auralis/src/auralis/models/xttsv2/config/xttsv2_gpt_config.py
================================================================================

#  Copyright (c) 2024 Astramind. Licensed under Apache License, Version 2.0.

from dataclasses import asdict, dataclass
from typing import Dict, Optional

from transformers.configuration_utils import PretrainedConfig
from transformers.utils import logging

logger = logging.get_logger(__name__)


@dataclass
class GPTAudioConfig:
    """Configuration for GPT audio processing parameters"""
    mel_channels: int = 80
    sample_rate: int = 22050
    output_sample_rate: int = 24000

@dataclass
class XTTSAudioConfig:
    """Configuration for audio processing parameters"""
    sample_rate: int = 22050
    output_sample_rate: int = 24000
    mel_channels: int = 80
    hop_length: int = 256
    win_length: int = 1024
    n_fft: int = 1024
    fmin: int = 0
    fmax: int = 8000
    power: float = 1.0
    mel_norms_file: Optional[str] = None


class XTTSGPTConfig(PretrainedConfig):
    """Configuration class for the GPT component of XTTS."""
    model_type = "xtts_gpt"

    def __init__(
            self,
            # Model architecture
            hidden_size: int = 1024,  # gpt_n_model_channels in original
            n_inner: int = 4096,
            num_hidden_layers: int = 30,  # gpt_layers in original
            num_attention_heads: int = 16,  # gpt_n_heads in original

            # Tokenizer settings
            vocab_size: int = 6681,  # gpt_number_text_tokens in original
            number_text_tokens: int = 6681,  # Explicit text token vocabulary size
            start_text_token: Optional[int] = None,
            stop_text_token: Optional[int] = None,

            # Audio token settings
            num_audio_tokens: int = 1026,  # gpt_num_audio_tokens in original
            start_audio_token: int = 1024,  # gpt_start_audio_token in original
            stop_audio_token: int = 1025,  # gpt_stop_audio_token in original

            # Sequence length settings
            max_audio_tokens: int = 605,  # gpt_max_audio_tokens in original
            max_text_tokens: int = 402,  # gpt_max_text_tokens in original
            max_prompt_tokens: int = 70,  # gpt_max_prompt_tokens in original
            gpt_max_audio_tokens: int = 605,  # Used for generation

            # Model behavior settings
            use_masking_gt_prompt_approach: bool = True,  # gpt_use_masking_gt_prompt_approach in original
            use_perceiver_resampler: bool = True,  # gpt_use_perceiver_resampler in original
            kv_cache: bool = True,
            enable_redaction: bool = False,

            # GPT batch settings
            gpt_batch_size: int = 1,

            # Audio processing
            audio_config: Optional[Dict] = None,

            # Architecture specifics
            layer_norm_epsilon: float = 1e-5,
            initializer_range: float = 0.02,
            add_cross_attention: bool = False,
            scale_attn_by_inverse_layer_idx: bool = False,
            reorder_and_upcast_attn: bool = False,

            # Size settings for the decoder
            decoder_input_dim: int = 1024,
            architectures=["XttsGPT"],
            auto_map={
                "AutoConfig": "AstraMindAI/xtts2-gpt--gpt_config.XTTSGPTConfig",
                "AutoModelForCausalLM": "AstraMindAI/xtts2-gpt--xtts2_gpt_modeling.XttsGPT",
            },
            activation_function: str = "gelu",
            attn_pdrop: float = 0.1,
            **kwargs
    ):
        super().__init__(**kwargs)
        self.architectures = architectures
        self.auto_map = auto_map
        self.audio_config = GPTAudioConfig(
            **audio_config if audio_config is not None else {}
        )
        self.activation_function = activation_function
        self.attn_pdrop = attn_pdrop
        self.hidden_size = hidden_size
        self.n_inner = n_inner
        self.num_hidden_layers = num_hidden_layers
        self.num_attention_heads = num_attention_heads

        self.vocab_size = vocab_size
        self.number_text_tokens = number_text_tokens
        self.start_text_token = start_text_token
        self.stop_text_token = stop_text_token

        self.num_audio_tokens = num_audio_tokens
        self.start_audio_token = start_audio_token
        self.stop_audio_token = stop_audio_token

        self.max_audio_tokens = max_audio_tokens
        self.max_text_tokens = max_text_tokens
        self.max_prompt_tokens = max_prompt_tokens
        self.gpt_max_audio_tokens = gpt_max_audio_tokens

        self.use_masking_gt_prompt_approach = use_masking_gt_prompt_approach
        self.use_perceiver_resampler = use_perceiver_resampler
        self.kv_cache = kv_cache
        self.enable_redaction = enable_redaction

        self.gpt_batch_size = gpt_batch_size

        self.layer_norm_epsilon = layer_norm_epsilon
        self.initializer_range = initializer_range
        self.add_cross_attention = add_cross_attention
        self.scale_attn_by_inverse_layer_idx = scale_attn_by_inverse_layer_idx
        self.reorder_and_upcast_attn = reorder_and_upcast_attn

        self.decoder_input_dim = decoder_input_dim

    def to_dict(self) -> Dict:
        """Convert the config to a dictionary."""
        output = super().to_dict()
        output["audio_config"] = asdict(self.audio_config)
        return output

    @classmethod
    def from_dict(cls, config_dict: Dict, *args, **kwargs) -> "XTTSGPTConfig":
        """Create a config from a dictionary."""
        return cls(**config_dict)




================================================================================
# File: auralis/models/xttsv2/config/tokenizer.py
# Path: /home/marco/PycharmProjects/betterVoiceCraft/Auralis/src/auralis/models/xttsv2/config/tokenizer.py
================================================================================

#  Copyright (c) 2024 Astramind. Licensed under Apache License, Version 2.0.

import re
from functools import cached_property
from typing import List, Optional, Union, Dict, Any

import cutlet
import pypinyin
import torch
from hangul_romanize import Transliter
from hangul_romanize.rule import academic
from num2words import num2words
from spacy.lang.ar import Arabic
from spacy.lang.en import English
from spacy.lang.es import Spanish
from spacy.lang.ja import Japanese
from spacy.lang.zh import Chinese
from tokenizers import Tokenizer
from tokenizers.pre_tokenizers import WhitespaceSplit
from tokenizers.processors import TemplateProcessing
from transformers import PreTrainedTokenizerFast, BatchEncoding
from transformers.tokenization_utils_base import TruncationStrategy, PaddingStrategy

from auralis.models.xttsv2.components.tts.layers.xtts.zh_num2words import TextNorm as zh_num2words


def get_spacy_lang(lang):
    if lang == "zh":
        return Chinese()
    elif lang == "ja":
        return Japanese()
    elif lang == "ar":
        return Arabic()
    elif lang == "es":
        return Spanish()
    else:
        # For most languages, English does the job
        return English()


def find_best_split_point(text: str, target_pos: int, window_size: int = 30) -> int:
    """
    Find best split point near target position considering punctuation and language markers.
    added for better sentence splitting in TTS.
    """
    # Define split markers by priority
    markers = [
        # Strong breaks (longest pause)
        (r'[.!?Øááá]+[\s]*', 1.0),  # Periods, exclamation, question (multi-script)
        (r'[\n\r]+\s*[\n\r]+', 1.0),  # Multiple newlines
        (r'[:|;ï¼ï¼ï¼][\s]*', 0.9),  # Colons, semicolons (multi-script)

        # Medium breaks
        (r'[,ï¼Øã][\s]*', 0.8),  # Commas (multi-script)
        (r'[)}\]ï¼ããÂ»âºã\s]+', 0.7),  # Closing brackets/parentheses
        (r'[-ââ]+[\s]*', 0.7),  # Dashes

        # Weak breaks
        (r'\s+[&+=/\s]+\s+', 0.6),  # Special characters with spaces
        (r'[\s]+', 0.5),  # Any whitespace as last resort
    ]

    # Calculate window boundaries
    start = max(0, target_pos - window_size)
    end = min(len(text), target_pos + window_size)
    window = text[start:end]

    best_pos = target_pos
    best_score = 0

    for pattern, priority in markers:
        matches = list(re.finditer(pattern, window))
        for match in matches:
            # Calculate position score based on distance from target
            pos = start + match.end()
            distance = abs(pos - target_pos)
            distance_score = 1 - (distance / (window_size * 2))

            # Combine priority and position scores
            score = priority * distance_score

            if score > best_score:
                best_score = score
                best_pos = pos

    return best_pos


def split_sentence(text: str, lang: str, text_split_length: int = 250) -> List[str]:
    """
    Enhanced sentence splitting with language awareness and optimal breakpoints.

    Args:
        text: Input text to split
        lang: Language code
        text_split_length: Target length for splits

    Returns:
        List of text splits optimized for TTS
    """
    text = text.strip()
    if len(text) <= text_split_length:
        return [text]

    nlp = get_spacy_lang(lang)
    if "sentencizer" not in nlp.pipe_names:
        nlp.add_pipe("sentencizer")

    # Get base sentences using spaCy
    doc = nlp(text)
    sentences = list(doc.sents)

    splits = []
    current_split = []
    current_length = 0

    for sent in sentences:
        sentence_text = str(sent).strip()
        sentence_length = len(sentence_text)

        # If sentence fits in current split
        if current_length + sentence_length <= text_split_length:
            current_split.append(sentence_text)
            current_length += sentence_length + 1

        # Handle long sentences
        elif sentence_length > text_split_length:
            # Add current split if exists
            if current_split:
                splits.append(" ".join(current_split))
                current_split = []
                current_length = 0

            # Split long sentence at optimal points
            remaining = sentence_text
            while len(remaining) > text_split_length:
                split_pos = find_best_split_point(
                    remaining,
                    text_split_length,
                    window_size=30
                )

                # Add split and continue with remainder
                splits.append(remaining[:split_pos].strip())
                remaining = remaining[split_pos:].strip()

            # Handle remaining text
            if remaining:
                current_split = [remaining]
                current_length = len(remaining)

        # Start new split
        else:
            splits.append(" ".join(current_split))
            current_split = [sentence_text]
            current_length = sentence_length

    # Add final split if needed
    if current_split:
        splits.append(" ".join(current_split))

    cleaned_sentences = [s[:-1]+' ' if s.endswith('.') else s for s in splits if s] # prevents annoying sounds in italian
    # Clean up splits
    return cleaned_sentences

_whitespace_re = re.compile(r"\s+")

# List of (regular expression, replacement) pairs for abbreviations:
_abbreviations = {
    "en": [
        (re.compile("\\b%s\\." % x[0], re.IGNORECASE), x[1])
        for x in [
            ("mrs", "misess"),
            ("mr", "mister"),
            ("dr", "doctor"),
            ("st", "saint"),
            ("co", "company"),
            ("jr", "junior"),
            ("maj", "major"),
            ("gen", "general"),
            ("drs", "doctors"),
            ("rev", "reverend"),
            ("lt", "lieutenant"),
            ("hon", "honorable"),
            ("sgt", "sergeant"),
            ("capt", "captain"),
            ("esq", "esquire"),
            ("ltd", "limited"),
            ("col", "colonel"),
            ("ft", "fort"),
        ]
    ],
    "es": [
        (re.compile("\\b%s\\." % x[0], re.IGNORECASE), x[1])
        for x in [
            ("sra", "seÃ±ora"),
            ("sr", "seÃ±or"),
            ("dr", "doctor"),
            ("dra", "doctora"),
            ("st", "santo"),
            ("co", "compaÃ±Ã­a"),
            ("jr", "junior"),
            ("ltd", "limitada"),
        ]
    ],
    "fr": [
        (re.compile("\\b%s\\." % x[0], re.IGNORECASE), x[1])
        for x in [
            ("mme", "madame"),
            ("mr", "monsieur"),
            ("dr", "docteur"),
            ("st", "saint"),
            ("co", "compagnie"),
            ("jr", "junior"),
            ("ltd", "limitÃ©e"),
        ]
    ],
    "de": [
        (re.compile("\\b%s\\." % x[0], re.IGNORECASE), x[1])
        for x in [
            ("fr", "frau"),
            ("dr", "doktor"),
            ("st", "sankt"),
            ("co", "firma"),
            ("jr", "junior"),
        ]
    ],
    "pt": [
        (re.compile("\\b%s\\." % x[0], re.IGNORECASE), x[1])
        for x in [
            ("sra", "senhora"),
            ("sr", "senhor"),
            ("dr", "doutor"),
            ("dra", "doutora"),
            ("st", "santo"),
            ("co", "companhia"),
            ("jr", "jÃºnior"),
            ("ltd", "limitada"),
        ]
    ],
    "it": [
        (re.compile("\\b%s\\." % x[0], re.IGNORECASE), x[1])
        for x in [
            # ("sig.ra", "signora"),
            ("sig", "signore"),
            ("dr", "dottore"),
            ("st", "santo"),
            ("co", "compagnia"),
            ("jr", "junior"),
            ("ltd", "limitata"),
        ]
    ],
    "pl": [
        (re.compile("\\b%s\\." % x[0], re.IGNORECASE), x[1])
        for x in [
            ("p", "pani"),
            ("m", "pan"),
            ("dr", "doktor"),
            ("sw", "ÅwiÄty"),
            ("jr", "junior"),
        ]
    ],
    "ar": [
        (re.compile("\\b%s\\." % x[0], re.IGNORECASE), x[1])
        for x in [
            # There are not many common abbreviations in Arabic as in English.
        ]
    ],
    "zh": [
        (re.compile("\\b%s\\." % x[0], re.IGNORECASE), x[1])
        for x in [
            # Chinese doesn't typically use abbreviations in the same way as Latin-based scripts.
        ]
    ],
    "cs": [
        (re.compile("\\b%s\\." % x[0], re.IGNORECASE), x[1])
        for x in [
            ("dr", "doktor"),  # doctor
            ("ing", "inÅ¾enÃ½r"),  # engineer
            ("p", "pan"),  # Could also map to pani for woman but no easy way to do it
            # Other abbreviations would be specialized and not as common.
        ]
    ],
    "ru": [
        (re.compile("\\b%s\\b" % x[0], re.IGNORECASE), x[1])
        for x in [
            ("Ð³-Ð¶Ð°", "Ð³Ð¾ÑÐ¿Ð¾Ð¶Ð°"),  # Mrs.
            ("Ð³-Ð½", "Ð³Ð¾ÑÐ¿Ð¾Ð´Ð¸Ð½"),  # Mr.
            ("Ð´-Ñ", "Ð´Ð¾ÐºÑÐ¾Ñ"),  # doctor
            # Other abbreviations are less common or specialized.
        ]
    ],
    "nl": [
        (re.compile("\\b%s\\." % x[0], re.IGNORECASE), x[1])
        for x in [
            ("dhr", "de heer"),  # Mr.
            ("mevr", "mevrouw"),  # Mrs.
            ("dr", "dokter"),  # doctor
            ("jhr", "jonkheer"),  # young lord or nobleman
            # Dutch uses more abbreviations, but these are the most common ones.
        ]
    ],
    "tr": [
        (re.compile("\\b%s\\." % x[0], re.IGNORECASE), x[1])
        for x in [
            ("b", "bay"),  # Mr.
            ("byk", "bÃ¼yÃ¼k"),  # bÃ¼yÃ¼k
            ("dr", "doktor"),  # doctor
            # Add other Turkish abbreviations here if needed.
        ]
    ],
    "hu": [
        (re.compile("\\b%s\\." % x[0], re.IGNORECASE), x[1])
        for x in [
            ("dr", "doktor"),  # doctor
            ("b", "bÃ¡csi"),  # Mr.
            ("nÅv", "nÅvÃ©r"),  # nurse
            # Add other Hungarian abbreviations here if needed.
        ]
    ],
    "ko": [
        (re.compile("\\b%s\\." % x[0], re.IGNORECASE), x[1])
        for x in [
            # Korean doesn't typically use abbreviations in the same way as Latin-based scripts.
        ]
    ],
}

def expand_abbreviations_multilingual(text, lang="en"):
    if lang in _abbreviations:
        for regex, replacement in _abbreviations[lang]:
            text = re.sub(regex, replacement, text)
    return text

_symbols_multilingual = {
    "en": [
        (re.compile(r"%s" % re.escape(x[0]), re.IGNORECASE), x[1])
        for x in [
            ("&", " and "),
            ("@", " at "),
            ("%", " percent "),
            ("#", " hash "),
            ("$", " dollar "),
            ("Â£", " pound "),
            ("Â°", " degree "),
        ]
    ],
    "es": [
        (re.compile(r"%s" % re.escape(x[0]), re.IGNORECASE), x[1])
        for x in [
            ("&", " y "),
            ("@", " arroba "),
            ("%", " por ciento "),
            ("#", " numeral "),
            ("$", " dolar "),
            ("Â£", " libra "),
            ("Â°", " grados "),
        ]
    ],
    "fr": [
        (re.compile(r"%s" % re.escape(x[0]), re.IGNORECASE), x[1])
        for x in [
            ("&", " et "),
            ("@", " arobase "),
            ("%", " pour cent "),
            ("#", " diÃ¨se "),
            ("$", " dollar "),
            ("Â£", " livre "),
            ("Â°", " degrÃ©s "),
        ]
    ],
    "de": [
        (re.compile(r"%s" % re.escape(x[0]), re.IGNORECASE), x[1])
        for x in [
            ("&", " und "),
            ("@", " at "),
            ("%", " prozent "),
            ("#", " raute "),
            ("$", " dollar "),
            ("Â£", " pfund "),
            ("Â°", " grad "),
        ]
    ],
    "pt": [
        (re.compile(r"%s" % re.escape(x[0]), re.IGNORECASE), x[1])
        for x in [
            ("&", " e "),
            ("@", " arroba "),
            ("%", " por cento "),
            ("#", " cardinal "),
            ("$", " dÃ³lar "),
            ("Â£", " libra "),
            ("Â°", " graus "),
        ]
    ],
    "it": [
        (re.compile(r"%s" % re.escape(x[0]), re.IGNORECASE), x[1])
        for x in [
            ("&", " e "),
            ("@", " chiocciola "),
            ("%", " per cento "),
            ("#", " cancelletto "),
            ("$", " dollaro "),
            ("Â£", " sterlina "),
            ("Â°", " gradi "),
        ]
    ],
    "pl": [
        (re.compile(r"%s" % re.escape(x[0]), re.IGNORECASE), x[1])
        for x in [
            ("&", " i "),
            ("@", " maÅpa "),
            ("%", " procent "),
            ("#", " krzyÅ¼yk "),
            ("$", " dolar "),
            ("Â£", " funt "),
            ("Â°", " stopnie "),
        ]
    ],
    "ar": [
        # Arabic
        (re.compile(r"%s" % re.escape(x[0]), re.IGNORECASE), x[1])
        for x in [
            ("&", " Ù "),
            ("@", " Ø¹ÙÙ "),
            ("%", " ÙÙ Ø§ÙÙØ¦Ø© "),
            ("#", " Ø±ÙÙ "),
            ("$", " Ø¯ÙÙØ§Ø± "),
            ("Â£", " Ø¬ÙÙÙ "),
            ("Â°", " Ø¯Ø±Ø¬Ø© "),
        ]
    ],
    "zh": [
        # Chinese
        (re.compile(r"%s" % re.escape(x[0]), re.IGNORECASE), x[1])
        for x in [
            ("&", " å "),
            ("@", " å¨ "),
            ("%", " ç¾åä¹ "),
            ("#", " å· "),
            ("$", " ç¾å "),
            ("Â£", " è±é "),
            ("Â°", " åº¦ "),
        ]
    ],
    "cs": [
        # Czech
        (re.compile(r"%s" % re.escape(x[0]), re.IGNORECASE), x[1])
        for x in [
            ("&", " a "),
            ("@", " na "),
            ("%", " procento "),
            ("#", " kÅÃ­Å¾ek "),
            ("$", " dolar "),
            ("Â£", " libra "),
            ("Â°", " stupnÄ "),
        ]
    ],
    "ru": [
        # Russian
        (re.compile(r"%s" % re.escape(x[0]), re.IGNORECASE), x[1])
        for x in [
            ("&", " Ð¸ "),
            ("@", " ÑÐ¾Ð±Ð°ÐºÐ° "),
            ("%", " Ð¿ÑÐ¾ÑÐµÐ½ÑÐ¾Ð² "),
            ("#", " Ð½Ð¾Ð¼ÐµÑ "),
            ("$", " Ð´Ð¾Ð»Ð»Ð°Ñ "),
            ("Â£", " ÑÑÐ½Ñ "),
            ("Â°", " Ð³ÑÐ°Ð´ÑÑ "),
        ]
    ],
    "nl": [
        # Dutch
        (re.compile(r"%s" % re.escape(x[0]), re.IGNORECASE), x[1])
        for x in [
            ("&", " en "),
            ("@", " bij "),
            ("%", " procent "),
            ("#", " hekje "),
            ("$", " dollar "),
            ("Â£", " pond "),
            ("Â°", " graden "),
        ]
    ],
    "tr": [
        (re.compile(r"%s" % re.escape(x[0]), re.IGNORECASE), x[1])
        for x in [
            ("&", " ve "),
            ("@", " at "),
            ("%", " yÃ¼zde "),
            ("#", " diyez "),
            ("$", " dolar "),
            ("Â£", " sterlin "),
            ("Â°", " derece "),
        ]
    ],
    "hu": [
        (re.compile(r"%s" % re.escape(x[0]), re.IGNORECASE), x[1])
        for x in [
            ("&", " Ã©s "),
            ("@", " kukac "),
            ("%", " szÃ¡zalÃ©k "),
            ("#", " kettÅskereszt "),
            ("$", " dollÃ¡r "),
            ("Â£", " font "),
            ("Â°", " fok "),
        ]
    ],
    "ko": [
        # Korean
        (re.compile(r"%s" % re.escape(x[0]), re.IGNORECASE), x[1])
        for x in [
            ("&", " ê·¸ë¦¬ê³  "),
            ("@", " ì "),
            ("%", " í¼ì¼í¸ "),
            ("#", " ë²í¸ "),
            ("$", " ë¬ë¬ "),
            ("Â£", " íì´ë "),
            ("Â°", " ë "),
        ]
    ],
}

def expand_symbols_multilingual(text, lang="en"):
    if lang in _symbols_multilingual:
        for regex, replacement in _symbols_multilingual[lang]:
            text = re.sub(regex, replacement, text)
            text = text.replace("  ", " ")  # Ensure there are no double spaces
    return text.strip()

_ordinal_re = {
    "en": re.compile(r"([0-9]+)(st|nd|rd|th)"),
    "es": re.compile(r"([0-9]+)(Âº|Âª|er|o|a|os|as)"),
    "fr": re.compile(r"([0-9]+)(Âº|Âª|er|re|e|Ã¨me)"),
    "de": re.compile(r"([0-9]+)(st|nd|rd|th|Âº|Âª|\.(?=\s|$))"),
    "pt": re.compile(r"([0-9]+)(Âº|Âª|o|a|os|as)"),
    "it": re.compile(r"([0-9]+)(Âº|Â°|Âª|o|a|i|e)"),
    "pl": re.compile(r"([0-9]+)(Âº|Âª|st|nd|rd|th)"),
    "ar": re.compile(r"([0-9]+)(ÙÙ|ÙÙ|Ø«|Ø±|Ù)"),
    "cs": re.compile(r"([0-9]+)\.(?=\s|$)"),  # In Czech, a dot is often used after the number to indicate ordinals.
    "ru": re.compile(r"([0-9]+)(-Ð¹|-Ñ|-Ðµ|-Ð¾Ðµ|-ÑÐµ|-Ð³Ð¾)"),
    "nl": re.compile(r"([0-9]+)(de|ste|e)"),
    "tr": re.compile(r"([0-9]+)(\.|inci|nci|uncu|Ã¼ncÃ¼|\.)"),
    "hu": re.compile(r"([0-9]+)(\.|adik|edik|odik|edik|Ã¶dik|Ã¶dike|ik)"),
    "ko": re.compile(r"([0-9]+)(ë²ì§¸|ë²|ì°¨|ì§¸)"),
}
_number_re = re.compile(r"[0-9]+")
# noinspection Annotator
_currency_re = {
    "USD": re.compile(r"((\$[0-9\.\,]*[0-9]+)|([0-9\.\,]*[0-9]+\$))"),
    "GBP": re.compile(r"((Â£[0-9\.\,]*[0-9]+)|([0-9\.\,]*[0-9]+Â£))"),
    "EUR": re.compile(r"(([0-9\.\,]*[0-9]+â¬)|((â¬[0-9\.\,]*[0-9]+)))"),
}

_comma_number_re = re.compile(r"\b\d{1,3}(,\d{3})*(\.\d+)?\b")
_dot_number_re = re.compile(r"\b\d{1,3}(\.\d{3})*(\,\d+)?\b")
_decimal_number_re = re.compile(r"([0-9]+[.,][0-9]+)")

def _remove_commas(m):
    text = m.group(0)
    if "," in text:
        text = text.replace(",", "")
    return text

def _remove_dots(m):
    text = m.group(0)
    if "." in text:
        text = text.replace(".", "")
    return text

def _expand_decimal_point(m, lang="en"):
    amount = m.group(1).replace(",", ".")
    return num2words(float(amount), lang=lang if lang != "cs" else "cz")

def _expand_currency(m, lang="en", currency="USD"):
    amount = float((re.sub(r"[^\d.]", "", m.group(0).replace(",", "."))))
    full_amount = num2words(amount, to="currency", currency=currency, lang=lang if lang != "cs" else "cz")

    and_equivalents = {
        "en": ", ",
        "es": " con ",
        "fr": " et ",
        "de": " und ",
        "pt": " e ",
        "it": " e ",
        "pl": ", ",
        "cs": ", ",
        "ru": ", ",
        "nl": ", ",
        "ar": ", ",
        "tr": ", ",
        "hu": ", ",
        "ko": ", ",
    }

    if amount.is_integer():
        last_and = full_amount.rfind(and_equivalents.get(lang, ", "))
        if last_and != -1:
            full_amount = full_amount[:last_and]

    return full_amount

def _expand_ordinal(m, lang="en"):
    return num2words(int(m.group(1)), ordinal=True, lang=lang if lang != "cs" else "cz")

def _expand_number(m, lang="en"):
    return num2words(int(m.group(0)), lang=lang if lang != "cs" else "cz")

def expand_numbers_multilingual(text, lang="en"):
    if lang == "zh":
        text = zh_num2words()(text)
    else:
        if lang in ["en", "ru"]:
            text = re.sub(_comma_number_re, _remove_commas, text)
        else:
            text = re.sub(_dot_number_re, _remove_dots, text)
        try:
            text = re.sub(_currency_re["GBP"], lambda m: _expand_currency(m, lang, "GBP"), text)
            text = re.sub(_currency_re["USD"], lambda m: _expand_currency(m, lang, "USD"), text)
            text = re.sub(_currency_re["EUR"], lambda m: _expand_currency(m, lang, "EUR"), text)
        except Exception as e:
            pass
        if lang != "tr":
            text = re.sub(_decimal_number_re, lambda m: _expand_decimal_point(m, lang), text)
        if lang in _ordinal_re:
            text = re.sub(_ordinal_re[lang], lambda m: _expand_ordinal(m, lang), text)
        text = re.sub(_number_re, lambda m: _expand_number(m, lang), text)
    return text

def lowercase(text):
    return text.lower()

def collapse_whitespace(text):
    return re.sub(_whitespace_re, " ", text)

def multilingual_cleaners(text, lang):
    text = text.replace('"', "")
    if lang == "tr":
        text = text.replace("Ä°", "i")
        text = text.replace("Ã", "Ã¶")
        text = text.replace("Ã", "Ã¼")
    text = lowercase(text)
    text = expand_numbers_multilingual(text, lang)
    text = expand_abbreviations_multilingual(text, lang)
    text = expand_symbols_multilingual(text, lang=lang)
    text = collapse_whitespace(text)
    return text

def basic_cleaners(text):
    """Basic pipeline that lowercases and collapses whitespace without transliteration."""
    text = lowercase(text)
    text = collapse_whitespace(text)
    return text

def chinese_transliterate(text):
    return "".join(
        [p[0] for p in pypinyin.pinyin(text, style=pypinyin.Style.TONE3, heteronym=False, neutral_tone_with_five=True)]
    )

def japanese_cleaners(text, katsu):
    text = katsu.romaji(text)
    text = lowercase(text)
    return text

def korean_transliterate(text, transliter):
    return transliter.translit(text)

# Fast Tokenizer Class

class XTTSTokenizerFast(PreTrainedTokenizerFast):
    """
    Fast Tokenizer implementation for XTTS model using HuggingFace's PreTrainedTokenizerFast
    """

    def __init__(
            self,
            vocab_file: str = None,
            tokenizer_object: Optional[Tokenizer] = None,
            unk_token: str = "[UNK]",
            pad_token: str = "[PAD]",
            bos_token: str = "[START]",
            eos_token: str = "[STOP]",
            auto_map: dict = {"AutoTokenizer": ["AstraMindAI/xtts2-gpt--tokenizer.XTTSTokenizerFast", None]},
            clean_up_tokenization_spaces: bool = True,
            **kwargs
    ):
        if tokenizer_object is None and vocab_file is not None:
            tokenizer_object = Tokenizer.from_file(vocab_file)

        if tokenizer_object is not None:
            # Configure the tokenizer
            tokenizer_object.pre_tokenizer = WhitespaceSplit()
            tokenizer_object.post_processor = TemplateProcessing(
                single=f"{bos_token} $A {eos_token}",
                special_tokens=[
                    (bos_token, tokenizer_object.token_to_id(bos_token)),
                    (eos_token, tokenizer_object.token_to_id(eos_token)),
                ],
            )

        super().__init__(
            tokenizer_object=tokenizer_object,
            unk_token=unk_token,
            pad_token=pad_token,
            bos_token=bos_token,
            eos_token=eos_token,
            clean_up_tokenization_spaces=clean_up_tokenization_spaces,
            **kwargs
        )

        # Character limits per language
        self.char_limits = {
            "en": 250, "de": 253, "fr": 273, "es": 239,
            "it": 213, "pt": 203, "pl": 224, "zh": 82,
            "ar": 166, "cs": 186, "ru": 182, "nl": 251,
            "tr": 226, "ja": 71, "hu": 224, "ko": 95,
        }

        # Initialize language tools
        self._katsu = None
        self._korean_transliter = Transliter(academic)

        # Ensure pad_token_id is set
        if self.pad_token_id is None:
            self.pad_token_id = self.tokenizer.token_to_id(self.pad_token)

    @cached_property
    def katsu(self):
        if self._katsu is None:
            self._katsu = cutlet.Cutlet()
        return self._katsu

    def preprocess_text(self, text: str, lang: str) -> str:
        """Apply text preprocessing for language"""
        base_lang = lang.split("-")[0]  # remove region
        if base_lang in {"ar", "cs", "de", "en", "es", "fr", "hu", "it",
                         "nl", "pl", "pt", "ru", "tr", "zh", "ko"}:
            text = multilingual_cleaners(text, base_lang)
            if base_lang == "zh":
                text = chinese_transliterate(text)
            if base_lang == "ko":
                text = korean_transliterate(text, self._korean_transliter)
        elif base_lang == "ja":
            text = japanese_cleaners(text, self.katsu)
        else:
            text = basic_cleaners(text)
        return text

    def batch_encode_with_split(self, texts: Union[str, List[str]], lang: Union[str, List[str]],
                                **kwargs) -> torch.Tensor:
        """
        Split texts into smaller chunks based on language character limits and encode them using HuggingFace fast tokenizer.
        strictly mimic the xttsv2 tokenizer
        """
        # Convert single inputs to lists
        if isinstance(texts, str):
            texts = [texts]
        if isinstance(lang, str):
            lang = [lang]
        # Ensure lang list matches texts list
        if len(lang) == 1 and len(texts) > 1:
            lang = lang * len(texts)

        # Check if texts and lang have the same length
        if len(texts) != len(lang):
            raise ValueError(f"Number of texts ({len(texts)}) does not match number of languages ({len(lang)}).")

        chunk_list = []
        max_splits = 0

        # For each text, split into chunks based on character limit
        for text, text_lang in zip(texts, lang):
            # Get language character limit
            base_lang = text_lang.split("-")[0]
            char_limit = self.char_limits.get(base_lang, 250)

            # Clean and preprocess
            #text = self.preprocess_text(text, text_lang) we do this in the hidden function

            # Split text into sentences/chunks based on language
            chunk_list = split_sentence(text, base_lang, text_split_length=char_limit)

        # Ensure the tokenizer is a fast tokenizer
        if not self.is_fast:
            raise ValueError("The tokenizer must be a fast tokenizer.")

        # Encode all chunks using the fast tokenizer
        encoding: BatchEncoding = self(
            chunk_list,
            lang = lang,
            add_special_tokens=False,
            padding=False,
            **kwargs
        )

        # The 'input_ids' tensor will have shape [total_chunks, max_sequence_length]
        return encoding['input_ids']  # Tensor of shape [total_chunks, sequence_length]

    def _batch_encode_plus(
            self,
            batch_text_or_text_pairs,
            add_special_tokens: bool = True,
            padding_strategy=PaddingStrategy.DO_NOT_PAD,
            truncation_strategy=TruncationStrategy.DO_NOT_TRUNCATE,
            max_length: Optional[int] = None,
            stride: int = 0,
            is_split_into_words: bool = False,
            pad_to_multiple_of: Optional[int] = None,
            return_tensors: Optional[str] = None,
            return_token_type_ids: Optional[bool] = None,
            return_attention_mask: Optional[bool] = None,
            return_overflowing_tokens: bool = False,
            return_special_tokens_mask: bool = False,
            return_offsets_mapping: bool = False,
            return_length: bool = False,
            verbose: bool = True,
            **kwargs
    ) -> Dict[str, Any]:
        """
        Override batch encoding to handle language-specific preprocessing
        """
        lang = kwargs.pop("lang", ["en"] * len(batch_text_or_text_pairs))
        if isinstance(lang, str):
            lang = [lang]
        # Ensure lang list matches texts list
        if len(lang) == 1 and len(batch_text_or_text_pairs) > 1:
            lang = lang * len(batch_text_or_text_pairs)

        # Check if batch_text_or_text_pairs and lang have the same length
        if len(batch_text_or_text_pairs) != len(lang):
            raise ValueError(f"Number of texts ({len(batch_text_or_text_pairs)}) does not match number of languages ({len(lang)}).")

        # Preprocess each text in the batch with its corresponding language
        processed_texts = []
        for text, text_lang in zip(batch_text_or_text_pairs, lang):
            if isinstance(text, str):
                # Check length and preprocess
                #self.check_input_length(text, text_lang)
                processed_text = self.preprocess_text(text, text_lang)

                # Format text with language tag and spaces
                base_lang = text_lang.split("-")[0]
                lang_code = "zh-cn" if base_lang == "zh" else base_lang
                processed_text = f"[{lang_code}]{processed_text}"
                processed_text = processed_text.replace(" ", "[SPACE]")

                processed_texts.append(processed_text)
            else:
                processed_texts.append(text)

        # Call the parent class's encoding method with processed texts
        return super()._batch_encode_plus(
            processed_texts,
            add_special_tokens=add_special_tokens,
            padding_strategy=padding_strategy,
            truncation_strategy=truncation_strategy,
            max_length=max_length,
            stride=stride,
            is_split_into_words=is_split_into_words,
            pad_to_multiple_of=pad_to_multiple_of,
            return_tensors=return_tensors,
            return_token_type_ids=return_token_type_ids,
            return_attention_mask=return_attention_mask,
            return_overflowing_tokens=return_overflowing_tokens,
            return_special_tokens_mask=return_special_tokens_mask,
            return_offsets_mapping=return_offsets_mapping,
            return_length=return_length,
            verbose=verbose,
            **kwargs
        )


    def __call__(
            self,
            text: Union[str, List[str]],
            lang: Union[str, List[str]] = "en",
            add_special_tokens: bool = True,
            padding: Union[bool, str, PaddingStrategy] = False,
            truncation: Union[bool, str, TruncationStrategy] = False,
            max_length: Optional[int] = None,
            stride: int = 0,
            return_tensors: Optional[str] = None,
            return_token_type_ids: Optional[bool] = None,
            return_attention_mask: Optional[bool] = True,
            **kwargs
    ):
        """
        Main tokenization method
        """
        # Convert single string to list for batch processing
        if isinstance(text, str):
            text = [text]
        if isinstance(lang, str):
            lang = [lang]
        # Ensure lang list matches texts list
        if len(lang) == 1 and len(text) > 1:
            lang = lang * len(text)

        # Ensure text and lang lists have same length
        if len(text) != len(lang):
            raise ValueError(f"Number of texts ({len(text)}) does not match number of languages ({len(lang)}).")

        # Convert padding strategy
        if isinstance(padding, bool):
            padding_strategy = PaddingStrategy.LONGEST if padding else PaddingStrategy.DO_NOT_PAD
        else:
            padding_strategy = PaddingStrategy(padding)

        # Convert truncation strategy
        if isinstance(truncation, bool):
            truncation_strategy = TruncationStrategy.LONGEST_FIRST if truncation else TruncationStrategy.DO_NOT_TRUNCATE
        else:
            truncation_strategy = TruncationStrategy(truncation)

        # Use the batch encoding method
        encoded = self._batch_encode_plus(
            text,
            add_special_tokens=add_special_tokens,
            padding_strategy=padding_strategy,
            truncation_strategy=truncation_strategy,
            max_length=max_length,
            stride=stride,
            return_tensors=return_tensors,
            return_token_type_ids=return_token_type_ids,
            return_attention_mask=return_attention_mask,
            lang=lang,
            **kwargs
        )

        return encoded


================================================================================
# File: auralis/models/xttsv2/utils/__init__.py
# Path: /home/marco/PycharmProjects/betterVoiceCraft/Auralis/src/auralis/models/xttsv2/utils/__init__.py
================================================================================

#  Copyright (c) 2024 Astramind. Licensed under Apache License, Version 2.0.



================================================================================
# File: auralis/models/xttsv2/utils/checkpoint_converter.py
# Path: /home/marco/PycharmProjects/betterVoiceCraft/Auralis/src/auralis/models/xttsv2/utils/checkpoint_converter.py
================================================================================

#  Copyright (c) 2024 Astramind. Licensed under Apache License, Version 2.0.

import argparse
import os

import torch
from huggingface_hub import snapshot_download
from safetensors.torch import save_file


def download_repo_files(repo_id, output_path, exclude_extensions=['.safetensors']):
    """
    Downloads all files from a GitHub repository except specified extensions.

    Args:
        owner (str): GitHub repository owner
        repo (str): Repository name
        exclude_extensions (list): List of file extensions to exclude
    """
    # Create base directory if it doesn't exist
    if not os.path.exists(output_path):
        os.makedirs(output_path)

    snapshot_download(repo_id=repo_id, ignore_patterns=exclude_extensions, local_dir=output_path)



def convert_checkpoint(pytorch_checkpoint_path, output_dir, args):
    """
    Convert PyTorch checkpoint to SafeTensors format, mapping weights to GPT2 or XTTSv2 models
    based on specific substrings.

    Args:
        pytorch_checkpoint_path: Path to input PyTorch checkpoint
        output_dir: Directory to save the output SafeTensors files
    """
    # Create output directory if it doesn't exist
    os.makedirs(output_dir, exist_ok=True)
    os.makedirs(os.path.join(output_dir, "gpt"), exist_ok=True)
    os.makedirs(os.path.join(output_dir, "core_xttsv2"), exist_ok=True)

    # Load PyTorch checkpoint
    checkpoint = torch.load(pytorch_checkpoint_path, map_location='cpu', weights_only=False) # to avoid warning

    # Initialize dictionaries for different models
    gpt2_weights = {}
    xtts_weights = {}

    # List of substrings to identify GPT2 weights
    gpt2_substrings = [
       'ln_1.weight', 'ln_1.bias', 'attn.c_attn.weight', 'attn.c_attn.bias', 'attn.c_proj.weight',
        'attn.c_proj.bias', 'ln_2.weight', 'ln_2.bias', 'mlp.c_fc.weight',
        'mlp.c_fc.bias', 'mlp.c_proj.weight', 'mlp.c_proj.bias', 'ln_f.weight',
        'ln_f.bias', 'mel_head.weight', 'mel_head.bias'

    ]
    ignore_in_check_components = ['mel_embedding.weight', 'mel_pos_embedding.emb.weight']
    # mel_emb -> wte.emb.weight, mel_pos_emb -> wpe.emb.weight

    all_sub_str = gpt2_substrings + ignore_in_check_components
    # Separate weights based on substrings
    for key, tensor in checkpoint['model'].items():
        # Check if any GPT2 substring is in the key
        is_gpt2_weight = any(substring in key for substring in all_sub_str)

        if is_gpt2_weight:
            if 'mel_embedding.weight' in key:
                key = 'gpt.wte.weight'
            elif 'mel_pos_embedding.emb.weight' in key:
                key = 'gpt.wpe.emb.weight'
            elif 'mel_head' in key:
                key = key.replace('gpt.', '')
            else:
                key = key.replace('gpt.gpt.', 'gpt.')
            # Use a modded name for GPT-2 weights
            gpt2_weights[key] = tensor
        elif 'final_norm' in key:
            gpt2_weights[key.replace('gpt.', '')] = tensor
            xtts_weights[key.replace('gpt.', '')] = tensor
        else:
            # All other weights go to XTTS
            xtts_weights[key.replace('gpt.', '')] = tensor

    # Check if all the weights keys are matched
    assert all(any(substr in key for key in gpt2_weights.keys()) for substr in gpt2_substrings), \
        f"Missing substrings: {[substr for substr in gpt2_substrings if not any(substr in key for key in gpt2_weights.keys())]}"


    gpt2_path = os.path.join(output_dir, "gpt", 'gpt2_model.safetensors')
    save_file(gpt2_weights, gpt2_path)
    download_repo_files("AstraMindAI/xtts2-gpt", os.path.join(output_dir, "gpt"))
    print(f"Saved XTTSv2 GPT-2 weights to {gpt2_path}")
    print(f"XTTSv2 GPT-2 weights: {list(gpt2_weights.keys())}")

    # Save XTTS weights if any exist
    if xtts_weights:
        xtts_path = os.path.join(output_dir, 'core_xttsv2', 'xtts-v2.safetensors')
        save_file(xtts_weights, xtts_path)
        download_repo_files("AstraMindAI/xttsv2", os.path.join(output_dir, "core_xttsv2"))
        print(f"Saved XTTSv2 weights to {xtts_path}")
        print(f"XTTSv2 weights: {list(xtts_weights.keys())}")

def main():
    parser = argparse.ArgumentParser(description='Convert PyTorch checkpoint to SafeTensors format')
    parser.add_argument('checkpoint_path', type=str, help='Path to PyTorch checkpoint file')
    parser.add_argument('--output_dir', type=str, default=os.getcwd(),
                        help='Output directory (defaults to current working directory)')

    args = parser.parse_args()

    # Verify checkpoint file exists
    if not os.path.exists(args.checkpoint_path):
        print(f"Error: Checkpoint file '{args.checkpoint_path}' does not exist")
        return

    # Convert the checkpoint
    convert_checkpoint(args.checkpoint_path, args.output_dir, args)

if __name__ == '__main__':
    main()

================================================================================
# File: auralis/models/xttsv2/components/__init__.py
# Path: /home/marco/PycharmProjects/betterVoiceCraft/Auralis/src/auralis/models/xttsv2/components/__init__.py
================================================================================

#  Copyright (c) 2024 Astramind. Licensed under Apache License, Version 2.0.



================================================================================
# File: auralis/models/xttsv2/components/vllm_mm_gpt.py
# Path: /home/marco/PycharmProjects/betterVoiceCraft/Auralis/src/auralis/models/xttsv2/components/vllm_mm_gpt.py
================================================================================

#  Copyright (c) 2024 Astramind. Licensed under Apache License, Version 2.0.

import functools
import random
import time
from collections import defaultdict
from dataclasses import dataclass
from typing import Dict, List
from typing import Optional, Union, Iterable, Tuple, Mapping

import torch
import torch.nn as nn
from torch import Tensor
from transformers import GPT2Config
from vllm.attention import AttentionMetadata
from vllm.config import CacheConfig, VllmConfig
from vllm.distributed import get_pp_group
from vllm.inputs import InputContext, INPUT_REGISTRY, DecoderOnlyInputs, token_inputs, DummyData
from vllm.model_executor.layers.logits_processor import LogitsProcessor
from vllm.model_executor.layers.quantization import QuantizationConfig
from vllm.model_executor.layers.sampler import Sampler, SamplerOutput
from vllm.model_executor.layers.vocab_parallel_embedding import VocabParallelEmbedding, ParallelLMHead
from vllm.model_executor.model_loader.weight_utils import default_weight_loader
from vllm.model_executor.models.gpt2 import GPT2Block
from vllm.model_executor.models.interfaces import SupportsMultiModal, SupportsPP
from vllm.model_executor.models.utils import make_layers, make_empty_intermediate_tensors_factory
from vllm.model_executor.sampling_metadata import SamplingMetadata
from vllm.multimodal import MULTIMODAL_REGISTRY, MultiModalKwargs
from vllm.multimodal.inputs import PlaceholderRange
from vllm.multimodal.utils import consecutive_placeholder_ranges
from vllm.sequence import IntermediateTensors, SequenceData
from vllm.utils import is_list_of

VLLM_TIMEOUT_POS_EMB_CORRECTER = 30  # seconds


PrefillLength= Union[int, List[int]]
TokenPosition= Union[int, List[int]]
TokenId = Union[Union[torch.Tensor,int], List[Union[torch.Tensor,int]]]

@dataclass
class TokenPositionAndPrefillTuple:
    prefill_len: Optional[PrefillLength] = None
    pos_id: Optional[TokenPosition] = None
    token_id: Optional[TokenId] = None

    def update_(self,
                prefill_len: Optional[PrefillLength] = None,
                pos_id: Optional[TokenPosition] = None,
                token_id: Optional[TokenId] = None):
        if prefill_len is not None:
            self.prefill_len=prefill_len
        if pos_id is not None:
            self.pos_id=pos_id
        if token_id is not None:
            self.token_id= token_id
        return self


class PositionalEmbeddingsCorrecter:
    """Corrects positional embeddings for XTTS model,
    since they have a different length than the text embeddings.
    This class tracks tokens both by request_id and position for vLLM compatibility.
    """

    def __init__(self):
        # Maps request_id to its prefill length
        self.request_tracker_dict: Dict[str, TokenPositionAndPrefillTuple] = defaultdict(lambda: TokenPositionAndPrefillTuple())
        # Maps token_position pairs to their request_id
        self.token_to_request: Dict[str, str] = {}
        self._last_access: Dict[str, float] = {}

    def _cleanup_old(self):
        current_time = time.time()
        expired = [
            req_id for req_id, last_time in self._last_access.items()
            if current_time - last_time > VLLM_TIMEOUT_POS_EMB_CORRECTER
        ]
        for req_id in expired:
            self.clear_request(req_id)
            self._last_access.pop(req_id)

    def _update_access(self, request_id: str):
        self._last_access[request_id] = time.time()
        self._cleanup_old()

    def init_request_id_prefill(self, request_id: str, prefill_len: PrefillLength, nex_token: torch.Tensor):
        """Initialize a request_id with its prefill length."""
        self._update_access(request_id)
        self.request_tracker_dict[request_id] = TokenPositionAndPrefillTuple(prefill_len, prefill_len)
        self.token_to_request[f"{nex_token}_{prefill_len}"] = request_id

    def get_by_request_id(self, request_id: str) -> TokenPositionAndPrefillTuple:
        """Retrieve the prefill length for a given request_id."""
        if request_id in self.request_tracker_dict:
            self._update_access(request_id)
        return self.request_tracker_dict.get(request_id, None)

    def get_by_next_token(self,
                          next_token_ids: List[int],
                          next_position_ids: List[int]
                          ) -> List[Optional[TokenPositionAndPrefillTuple]]:
        """Retrieve prefill lengths for given token and position pairs.

        Args:
            next_token_ids: List of token IDs
            next_position_ids: List of position IDs, corresponding to token IDs

        Returns:
            List of prefill lengths for each token-position pair

        Raises:
            ValueError: If no valid token mappings are found
        """
        prefill_lengths = []
        assert len(next_token_ids) == len(next_position_ids), "Token and position lists must have the same length"
        if len(next_token_ids) == 0:
            return prefill_lengths

        for next_token_id, next_position_id in zip(next_token_ids, next_position_ids):
            token_key = f"{next_token_id}_{next_position_id}"
            if token_key in self.token_to_request:
                request_id = self.token_to_request[token_key]
                self._update_access(request_id)
                prefill_lengths.append(self.request_tracker_dict[request_id].update_(token_id=next_token_id))

        if not prefill_lengths:
            raise ValueError(f"No valid mappings found for token pairs")
        return prefill_lengths

    def _invalidate_previous_mapping(self, request_id: str):
        """Remove all token mappings associated with a given request_id.

        This prevents memory leaks from old token mappings and ensures
        we don't have stale token-to-request associations.
        """
        # Find all token keys that map to this request_id
        keys_to_remove = [
            token_key for token_key, req_id in self.token_to_request.items()
            if req_id == request_id
        ]

        # Remove all found mappings
        for token_key in keys_to_remove:
            del self.token_to_request[token_key]

    def _get_pos_id_and_update (self, request_id: str):
        """Get the position ID for a given request_id and update it."""
        tuple_prefill_token = self.get_by_request_id(request_id)
        # Update the position ID
        self.request_tracker_dict[request_id] = TokenPositionAndPrefillTuple(tuple_prefill_token.prefill_len, tuple_prefill_token.pos_id + 1)
        return tuple_prefill_token.pos_id + 1


    def associate_new_tokens(self, request_id: str, next_token_id: int):
        """Associate a new token-position pair with a request_id.

        Before creating the new association, it removes all previous
        token mappings for this request_id to maintain consistency.

        Args:
            request_id: The request identifier
            next_token_id: The token ID to associate
        """
        pos_id = self._get_pos_id_and_update(request_id)

        # Clean up old mappings first
        self._invalidate_previous_mapping(request_id)

        # Create new mapping
        self.token_to_request[f"{next_token_id}_{pos_id}"] = request_id
        self._update_access(request_id)

    def clear_request(self, request_id: str):
        """Remove all data associated with a request_id.

        This includes both the prefill length tracking and any token mappings.
        """
        if request_id in self.request_tracker_dict:
            # First remove all token mappings
            self._invalidate_previous_mapping(request_id)
            # Then remove the request tracking
            del self.request_tracker_dict[request_id]

class LearnedPositionEmbeddings(nn.Module):
    def __init__(self, seq_len, model_dim, init=0.02, relative=False, supports_pp=False):
        super().__init__()
        # nn.Embedding
        self.emb = VocabParallelEmbedding(seq_len, model_dim) if supports_pp else nn.Embedding(seq_len, model_dim)
        # Initializing this way is standard for GPT-2
        self.emb.weight.data.normal_(mean=0.0, std=init)
        self.relative = relative
        self.seq_len = seq_len

    def forward(self, x):
        sl = x.shape[1]
        if self.relative:
            start = random.randint(sl, self.seq_len) - sl
            indices = torch.arange(start, start + sl, device=x.device)
            # Validate indices
            assert (indices < self.seq_len).all() and (indices >= 0).all(), \
                f"Invalid position indices in forward: min={indices.min().item()}, max={indices.max().item()}, valid_range=[0,{self.seq_len-1}]"
            return self.emb(indices)
        else:
            indices = torch.arange(0, sl, device=x.device)
            # Validate indices
            assert (indices < self.seq_len).all(), \
                f"Sequence length {sl} exceeds maximum position embedding length {self.seq_len}"
            return self.emb(indices)

    def get_fixed_embedding(self, ind: torch.Tensor, dev: torch.device) -> torch.Tensor:
        """Get position embeddings with batch support.

        Args:
            ind: Position indices tensor. Can be single or batched
                 Shape: [..., seq_len] or [seq_len]
            dev: Target device for the embeddings

        Returns:
            Position embeddings tensor matching input shape plus embedding dimension
            Shape: [batch_size, seq_len, model_dim] or [1, 1, model_dim]
        """
        # Validation of indices to prevent unknown errors
        assert (ind < self.seq_len).all(), \
            f"Position indices out of range. Found max={ind.max().item()}, but maximum allowed is {self.seq_len-1}"
        assert (ind >= 0).all(), \
            f"Negative position indices found. Min value={ind.min().item()}"

        if ind.shape[0] > 1:

            return self.emb(ind)
        else:
            #assert ind.dim() <= 2, f"Single input should have 1 or 2 dimensions, got {ind.dim()}"
            return self.emb(torch.tensor([ind], device=dev)).unsqueeze(0)



def get_xtts_max_audio_tokens(ctx: InputContext) -> int:
    """Calculate maximum audio tokens based on text context and audio duration."""
    return 32 # the conditoning perciever output


def dummy_seq_data_for_xtts(
        ctx: InputContext,
        seq_len: int,
        audio_count: int,
):
    """Create dummy sequence data for XTTS profiling."""
    # Calculate audio token space needed
    conditioning_lenght = (32 # the conditioning perceiver output length in the sql (which is fixed)
                           +
                           1) # the start audio token

    return SequenceData.from_prompt_token_counts(
        (1, conditioning_lenght * audio_count),
        (0, seq_len - conditioning_lenght * audio_count)),{
        "audio":
            consecutive_placeholder_ranges(num_items=audio_count,
                                           item_size=conditioning_lenght)
    }


def dummy_conditioning_for_xtts(
        ctx: InputContext,
        seq_len: int,
        audio_count: int,
) -> dict:
    """Create dummy conditioning data for XTTS."""
    return {
        "audio": {
            "embeds":[
                torch.zeros(
                    (seq_len, ctx.model_config.hf_config.hidden_size),
                    dtype=ctx.model_config.dtype) for _ in range(audio_count)
            ],
            "is_logits_only_mode": False,
            "sequence_length": -1,
        }
    }


def dummy_data_for_xtts(
        ctx: InputContext,
        seq_len: int,
        mm_counts: Mapping[str, int],
):
    """Create complete dummy data for XTTS profiling."""
    audio_count = mm_counts["audio"]
    seq_data, ranges = dummy_seq_data_for_xtts(ctx, seq_len, audio_count)
    cond_data = dummy_conditioning_for_xtts(ctx, seq_len, audio_count)
    return DummyData(seq_data, cond_data, ranges)


def input_mapper_for_xtts(ctx: InputContext, data: Union[Dict, List[Tensor]]) -> MultiModalKwargs:
    """Map input data to XTTS format."""

    if not isinstance(data, list):
        data = [data]

    if len(data) == 0:
        return MultiModalKwargs()

    assert is_list_of(data, dict, check="all"), (f"Expected a list of dictionaries, "
                                                 f"but got a list of {[type(dat) for dat in data if type(dat) != dict][0]}")

    embeds = [dat["embeds"] for dat in data]
    is_logits_only_mode = [dat.get("is_logits_only_mode", False) for dat in data]
    sequence_length = [dat.get("sequence_length", -1) for dat in data]
    return MultiModalKwargs(
        {
            "cond_latents": embeds,
            "is_logits_only_mode": is_logits_only_mode,
            "sequence_length": sequence_length
        }
    )




def input_processor_for_xtts2_gpt(ctx: InputContext, inputs: DecoderOnlyInputs):
    """
    We'll accomodate for the extra contditioning token and for the start audio token,
    we actually insert a -1 repeated for the differecne in length between the conditioning and the tokenized text
    and then we add 1 for the start audio token
    Args:
        ctx:
        inputs:

    Returns:

    """
    multi_modal_data = inputs.get("multi_modal_data")
    if multi_modal_data is None or "audio" not in multi_modal_data:
        raise ValueError("Missing audio data in multi-modal inputs")

    audio_dict = multi_modal_data['audio']
    audio = audio_dict.get('embeds')

    is_last_decoding_pass = audio_dict.get("is_logits_only_mode", False)

    prompt_token_ids = inputs.get("prompt_token_ids")

    if not is_last_decoding_pass:
        # we fill everything with 1 since we don't actually needs text token ids, it would mess up in the sampling step
        new_token_ids = ([1] * (audio.shape[0])) + [ctx.model_config.hf_config.start_audio_token] # add the start audio generation token
    else:
        new_token_ids = ([1] * audio.shape[0]) + prompt_token_ids
    # the encoding had already been done externally to reuse the embeddings for later use but we
    # account for the new token that will be added before generation
    new_prompt = None
    return token_inputs(prompt_token_ids=new_token_ids,
                        prompt=new_prompt,
                        multi_modal_data=multi_modal_data,
                        multi_modal_placeholders={'audio':[PlaceholderRange(offset=0, length=len(new_token_ids))]})


@MULTIMODAL_REGISTRY.register_input_mapper("audio", input_mapper_for_xtts)
@MULTIMODAL_REGISTRY.register_max_multimodal_tokens("audio", get_xtts_max_audio_tokens)
@INPUT_REGISTRY.register_dummy_data(dummy_data_for_xtts)
@INPUT_REGISTRY.register_input_processor(input_processor_for_xtts2_gpt)
class XttsGPT(nn.Module, SupportsMultiModal, SupportsPP):
    def __init__( # type: ignore
            self,
            vllm_config: VllmConfig,
            prefix: str,
            cache_config: Optional[CacheConfig] = None,
            quant_config: Optional[QuantizationConfig] = None,
    ):
        super().__init__()
        self.config = vllm_config
        self.gpt_config = self.config.model_config.hf_config
        self.quant_config = quant_config

        self.prefix_sequence_dict: Dict[str, torch.Tensor] = {}
        # Core GPT components
        self.gpt = GPT2Model(
            self.gpt_config,
            cache_config,
            quant_config,
            prefix="gpt"
        )

        self.final_norm =  nn.LayerNorm(self.gpt_config.hidden_size, bias=True, eps=self.gpt_config.layer_norm_epsilon)
        # Output head for mel tokens
        self.mel_head = ParallelLMHead(
            self.gpt_config.num_audio_tokens,
            self.gpt_config.hidden_size,
            bias=True,
            quant_config=quant_config,
            prefix="mel_head"
        )
        self.audio_start_generation_token = self.gpt_config.start_audio_token

        self.gpt.audio_start_generation_token = self.audio_start_generation_token


        # Initialize logits processor and sampler
        logit_scale = getattr(self.gpt_config, "logit_scale", 1.0)
        self.logits_processor = LogitsProcessor(self.gpt_config.num_audio_tokens,
                                                self.gpt_config.num_audio_tokens,
                                                logit_scale)
        self.sampler = Sampler()

        self.positional_embeddings_correcter = PositionalEmbeddingsCorrecter()

    @staticmethod
    def _check_is_logits_only_mode(is_logits_only_mode) -> torch.Tensor:

        # First check if it's a boolean
        if isinstance(is_logits_only_mode, bool):
            return torch.tensor([is_logits_only_mode])

        # Then check if it's a tensor
        if torch.is_tensor(is_logits_only_mode):
            # if it's a scalar tensor, return the value
            if is_logits_only_mode.numel() == 1:
                return is_logits_only_mode
            # for non-scalar tensors, check if all elements are the same
            return is_logits_only_mode

        # Fallback
        return torch.tensor([bool(is_logits_only_mode)])

    @staticmethod
    def find_len_of_sequence(
            positions_ids: torch.Tensor,
            index: torch.Tensor
    ) -> torch.Tensor:
        """
        Starting from the index, it goes backward in the positions until it finds a jump higher than 1.
        This function is tensorized for efficiency.

        Args:
        positions_ids: Tensor of position IDs
        index: Tensor of indices to start searching from

        Returns:
        Tensor of sequence lengths
        """
        # Ensure index is a tensor
        if not isinstance(index, torch.Tensor):
            index = torch.tensor(index, device=positions_ids.device)

        # Create a mask for valid positions (from 0 to index for each element)
        mask = torch.arange(positions_ids.size(0), device=positions_ids.device).unsqueeze(0) <= index

        # Calculate differences between adjacent positions
        diffs = positions_ids[1:] - positions_ids[:-1]

        # Pad the diffs tensor to match the original size
        diffs = torch.cat([torch.ones(1, device=positions_ids.device), diffs])

        # Find where the difference is different from 1 and is within the valid range
        jumps = (diffs != 1) & mask

        # Get the indices of the jumps
        jump_indices = jumps.nonzero()

        # If no jumps are found, return the index itself (full length)
        if jump_indices.numel() == 0:
            return torch.tensor([0], device=positions_ids.device)

        # Get the last jump for each index
        last_jumps = jump_indices[:, 1].reshape(-1, jump_indices.size(0))[:, -1]

        # Calculate the sequence lengths
        return last_jumps

    def _maybe_correct_positions(self,
                                 input_ids: torch.Tensor,
                                 positions: torch.Tensor,
                                 conditioning_inputs_list: List[torch.Tensor]):
        correct_positions_ids = self.positional_embeddings_correcter.get_by_next_token(input_ids.tolist(),
                                                                                       positions.tolist())
        if len(correct_positions_ids) > 0:
            position_and_id_tensor = torch.cat(
                [positions.unsqueeze(0), input_ids.unsqueeze(0)],
                dim=0
            )

            index_2d = torch.tensor(
                [(correct_positions_id.pos_id, correct_positions_id.token_id) for
                 correct_positions_id in correct_positions_ids],
                device=positions.device
            )

            prefill_len_token = torch.tensor(
                [correct_positions_id.prefill_len for correct_positions_id in correct_positions_ids],
                device=positions.device)

            position_and_id_expanded = position_and_id_tensor.unsqueeze(-1)
            index_2d_expanded = index_2d.T.unsqueeze(1)

            matches = (position_and_id_expanded == index_2d_expanded).all(dim=0)
            matching_indices = matches.any(dim=1).nonzero().squeeze(1)

            if not isinstance(conditioning_inputs_list, list) or len(conditioning_inputs_list) < 1:
                # this is the case where all the tokens are a "second iter" token,
                # so we don't have mixed stages in the batch
                return 1 + positions - prefill_len_token
            # Iterate through all matching indices
            for idx, seq_idx in enumerate(matching_indices):

                # Ensure we have corresponding conditioning input
                if (isinstance(conditioning_inputs_list, list) and
                        len(conditioning_inputs_list) > 0 and
                        idx < len(conditioning_inputs_list)):
                    end_pos = seq_idx + 1
                    start_pos = self.find_len_of_sequence(positions, seq_idx)  # type: ignore

                    # Apply correction only to the relevant part of the sequence
                    positions[start_pos:end_pos] = 1 + positions[start_pos:end_pos] - \
                                                   correct_positions_ids[
                                                       idx].prefill_len

            return positions

    def _apply_op_to_seq_in_batch(self,
                                  input_ids: torch.Tensor,
                                  positions: torch.Tensor,
                                  conditioning_inputs_list: List[torch.Tensor],
                                  is_logit_only_mode: torch.Tensor,
                                  seq_len: Union[torch.Tensor],
                                  is_profiling_run: bool = False
                                  ) -> Tuple[List[int], torch.Tensor, torch.Tensor]:
        """
        Apply different ops to the tensors sequence in the batch
        Returns:
            - List of starting indexes
            - A tensor for the logit only mode
            - A mask to reinsert the tokens in the correct position for the logit only mode
            - Modified input IDs
            - Modified positions
        """
        if is_profiling_run:
            return [], input_ids, positions

        # Pre-allocate lists for better memory efficiency
        starting_indexes = []

        # Find all end markers at once
        end_markers = (input_ids == self.audio_start_generation_token).nonzero(as_tuple=True)[0]

        if len(end_markers) == 0:
            positions = self._maybe_correct_positions(input_ids, positions, conditioning_inputs_list)
            return [], input_ids, positions

        # Create mask for valid conditioning inputs
        cond_latent_mask = torch.tensor([
            isinstance(cond_latent, torch.Tensor) and cond_latent.dim() > 1
            for cond_latent in conditioning_inputs_list
        ], device=input_ids.device)

        effective_indexes = cond_latent_mask.nonzero(as_tuple=True)[0]

        # Pre-calculate all sequence lengths
        sequence_lengths = torch.tensor([
            cond.shape[(0 if cond.dim == 1 else 1)] if isinstance(cond, torch.Tensor) and cond.dim() > 1
            else 0 for cond in conditioning_inputs_list
        ], device=input_ids.device)

        # Create masks for efficient tensor operations
        keep_mask = torch.ones(len(input_ids), dtype=torch.bool, device=input_ids.device)
        non_logit_mask = torch.ones_like(keep_mask)

        cumulative_offset = 0

        for idx, end_marker in zip(effective_indexes, end_markers):
            # Calculate effective positions
            end_pos = end_marker.item() - cumulative_offset
            start_pos = end_pos - sequence_lengths[idx].item()
            start_pos_for_masking = start_pos + cumulative_offset

            # Store original starting index
            starting_indexes.append(start_pos_for_masking)

            if is_logit_only_mode[idx]:
                # here the logic is a bit messy:
                # in the og implementation, the treats the embedding for the star tof generation token differently.
                # during the autoregressive token generation phase they use the token embeddings of the start
                # of generation token as input for the position embeddings, but in the logit only mode they use the
                # position id of the start of generation token as input for the position embeddings

                non_logit_mask[start_pos_for_masking : end_pos + cumulative_offset + seq_len[idx]] = False
                keep_mask[start_pos_for_masking:end_pos + cumulative_offset] = False
                # Generate positions for this sequence
                new_positions = torch.arange(
                    0, seq_len[idx].item(), # starting from zero since we have the start audio token
                    device=input_ids.device,
                    dtype=positions.dtype
                )
                # Update positions
                if end_pos + len(new_positions) <= len(positions):
                    positions[end_pos + cumulative_offset:end_pos + cumulative_offset + seq_len[idx]] = new_positions

            else:

                # Update masks
                keep_mask[start_pos_for_masking:end_pos + cumulative_offset + 1] = False

            cumulative_offset += (end_pos - start_pos + 1)

        # Apply masks to get final tensors
        # First we select tokens that are not used in the logit only mode
        # we have tre scenarios here:
        # 1. We are in a first pass where we have a sequence of 1s tokens terminated by a start audio token,
        # we completely remove this and we keep the index on where to insert since we have already precomputed the values
        # 2. We are in a "second pass" (autoregressive pass), using the default process of vllm with corrected positions ids
        # 3. We are in a logit only mode, since in xttsv2 we need to capture the hs,
        # and to do this we pass the conditioning alongside the generated tokens,
        # we need to remove the placeholder sequence at the beginning while adjusting
        # the positioning inside that condition
        non_logit_input_ids = input_ids[non_logit_mask & keep_mask]
        non_logit_positions = positions[non_logit_mask & keep_mask]

        correct_positions = self._maybe_correct_positions(
            # if we arrive here it means that we had mixed "second passes" and "logit only mode" in the batch,
            non_logit_input_ids,
            non_logit_positions,
            conditioning_inputs_list
        )
        if correct_positions is not None:
            # only happens if chunk prefill is enabled
            positions[non_logit_mask & keep_mask] = correct_positions

        modified_input_ids = input_ids[keep_mask]
        modified_positions = positions[keep_mask]
        assert (modified_positions < 608).all()
        assert (modified_positions >= 0).all()
        return starting_indexes, modified_input_ids, modified_positions


    # noinspection PyMethodOverriding
    def forward( # type: ignore
            self,
            input_ids: torch.Tensor,
            positions: torch.Tensor,
            kv_caches: List[torch.Tensor],
            attn_metadata: AttentionMetadata,
            intermediate_tensors: Optional["IntermediateTensors"] = None,
            cond_latents: Optional[Union[torch.Tensor, List[torch.Tensor]]] = False, # so we can always have a list
            is_logits_only_mode: Union[torch.Tensor, bool] = False,
            sequence_length: Union[torch.Tensor,int] = -1,
            **kwargs,
    ) -> Union[torch.Tensor, "IntermediateTensors"]:
        """Forward pass following VLLM pattern."""

        is_profiling_run = False

        # we work with list conditioning so we convert them to list regardless of vllm batching
        if isinstance(cond_latents, torch.Tensor):
            if len(cond_latents.shape) > 4:
                is_profiling_run = True
            else:
                # if two equal tensors are passed, vllm aggregate them in a new (batched) tensor
                cond_latents = list(cond_latents)  # so we unbacth them :) (unless we are in the profiling run)

        is_logits_only_mode = self._check_is_logits_only_mode(is_logits_only_mode)

        starting_sequence_start_ids, input_ids, positions = self._apply_op_to_seq_in_batch(input_ids,
                                                                                           positions,
                                                                                           cond_latents,
                                                                                           is_logits_only_mode,
                                                                                           sequence_length,
                                                                                           is_profiling_run)


        hidden_states = self.gpt(
            input_ids=input_ids,
            position_ids=positions,
            kv_caches=kv_caches,
            attn_metadata=attn_metadata,
            intermediate_tensors=intermediate_tensors,
            # this is the conditioning input ( voice conditioning + text_embeds )
            input_embeds=cond_latents,
            starting_sequence_start_ids=starting_sequence_start_ids,
            is_profiling_run= is_profiling_run,
            is_logit_only=is_logits_only_mode
        )

        return hidden_states

    # noinspection PyUnresolvedReferences
    def compute_logits(
            self,
            hidden_states: torch.Tensor,
            sampling_metadata: SamplingMetadata,
    ) -> Optional[torch.Tensor]:
        # normalize the hidden states
        # we keep this, because in the xttsv2 code they have a nn.sequential with norm and then lm head
        hidden_states = self.final_norm(hidden_states)

        # we keep track of the last collected index to properly associate the hidden states with the correct request_id
        last_collected_idx = 0
        for seq in sampling_metadata.seq_groups:
            # Check if we need to collect hidden states
            sampling_params = seq.sampling_params
            if (hasattr(sampling_params, 'hidden_state_collector')
                    and sampling_params.hidden_state_collector is not None):
                self.positional_embeddings_correcter.clear_request(sampling_params.request_id)
                # Call the collector directly with the hidden states
                sampling_params.hidden_state_collector(hidden_states[last_collected_idx:last_collected_idx+seq.seq_len], sampling_params.request_id)  # The request_id is already bound

            last_collected_idx += seq.seq_len or 0

        # Compute logits using the mel_head
        logits = self.logits_processor(self.mel_head, hidden_states, sampling_metadata, self.mel_head.bias)
        return logits

    # noinspection PyUnresolvedReferences
    def sample(
            self,
            logits: torch.Tensor,
            sampling_metadata: SamplingMetadata,
    ) -> Optional[SamplerOutput]:
        next_tokens = self.sampler(logits, sampling_metadata)
        for seq_id, seq_groups in enumerate(sampling_metadata.seq_groups):
            if hasattr(seq_groups.sampling_params, 'request_id') and seq_groups.sampling_params.request_id is not None:
                idx = seq_groups.seq_ids[0]
                # Call the collector directly with the next tokens
                if not self.positional_embeddings_correcter.get_by_request_id(seq_groups.sampling_params.request_id):
                    self.positional_embeddings_correcter.init_request_id_prefill(
                        request_id = seq_groups.sampling_params.request_id,
                        prefill_len=len(seq_groups.seq_data[idx].prompt_token_ids),
                        nex_token=next_tokens.outputs[seq_id].samples[0].output_token # index out of error
                    )
                else:
                    self.positional_embeddings_correcter.associate_new_tokens(
                        request_id=seq_groups.sampling_params.request_id,
                        next_token_id=next_tokens.outputs[seq_id].samples[0].output_token)

        return next_tokens

    def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]]):
        """Load weights following VLLM pattern."""
        params_dict = dict(self.named_parameters(remove_duplicate=False))
        loaded_names = set()
        for name, loaded_weight in weights:
            if name not in params_dict:
                continue

            param = params_dict[name]
            if "c_attn" in name or "c_proj" in name or "c_fc" in name:
                if name.endswith(".weight"):
                    loaded_weight = loaded_weight.t()

            weight_loader = getattr(param, "weight_loader", default_weight_loader)
            weight_loader(param, loaded_weight)
            loaded_names.add(name)
        # used to check if all weights were loaded
        assert set(params_dict.keys()) - loaded_names == set(), \
            (f"Missing weights: {set(params_dict.keys()) - loaded_names}, "
             f"this probably means you are using an incompatible model ")

class GPT2Model(nn.Module):

    def __init__(
            self,
            config: GPT2Config,
            cache_config: Optional[CacheConfig] = None,
            quant_config: Optional[QuantizationConfig] = None,
            prefix: str = "",
    ):
        super().__init__()
        self.config = config
        assert not config.add_cross_attention
        assert not config.scale_attn_by_inverse_layer_idx
        assert not config.reorder_and_upcast_attn
        self.audio_start_generation_token = None
        self.embed_dim = config.hidden_size
        self.wte = VocabParallelEmbedding(config.num_audio_tokens, self.embed_dim)
        self.wpe = (
            LearnedPositionEmbeddings(config.max_audio_tokens + 3, config.decoder_input_dim)
            if config.max_audio_tokens != -1
            else functools.partial(config.null_position_embeddings, dim=config.decoder_input_dim)
        )
        self.start_layer, self.end_layer, self.h = make_layers(
            config.num_hidden_layers,
            lambda prefix: GPT2Block(
                config, cache_config, quant_config, prefix=prefix),
            prefix=f"{prefix}.h")
        self.ln_f = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_epsilon)
        self.make_empty_intermediate_tensors = (
            make_empty_intermediate_tensors_factory(["hidden_states"],
                                                    config.hidden_size))

    @staticmethod
    def _insert_conditioning_into_hidden_states(hidden_states: torch.Tensor,
                                                conditioning_inputs: Optional[List[torch.Tensor]],
                                                start_of_generation_embed: Optional[torch.Tensor],
                                                insertion_ids: List[int],
                                                is_logit_only: torch.Tensor) -> torch.Tensor:
        empty_tensor = torch.empty(
            (0,hidden_states.shape[-1]),
            device=hidden_states.device, dtype=hidden_states.dtype
        )
        for idx, (inserion_idx, conditioning_input) in enumerate(zip(insertion_ids, conditioning_inputs)):
                hidden_states = torch.cat([
                hidden_states[:inserion_idx],
                conditioning_input.squeeze(0),
                (start_of_generation_embed if ~is_logit_only[idx] else empty_tensor),
                hidden_states[inserion_idx:]], dim=0
            )

        return hidden_states

    def forward(
            self,
            input_ids: torch.Tensor,
            position_ids: torch.Tensor,
            kv_caches: List[torch.Tensor],
            attn_metadata: AttentionMetadata,
            intermediate_tensors: Optional[IntermediateTensors],
            input_embeds: Optional[torch.Tensor] = None,
            starting_sequence_start_ids: Optional[List[int]] = None,
            is_profiling_run: bool = False,
            is_logit_only: torch.Tensor = False
    ) -> Union[torch.Tensor, IntermediateTensors]:

        if get_pp_group().is_first_rank:
            starting_sequence_embed = None
            if isinstance(input_embeds, list) and len(input_embeds) > 0:
                # we could be either in start condition or in a final condition or both
                if len(starting_sequence_start_ids) > 0 and not (is_logit_only).all():
                    # we have starting sequences, so we just need to get one hs to insert later
                    starting_sequence_embed = self.wte(
                        torch.tensor(
                            self.audio_start_generation_token,
                            device=input_ids.device
                        ).unsqueeze(0)
                    )

                    starting_sequence_embed += self.wpe(starting_sequence_embed.reshape(-1, 1))

            audio_inputs_embeds = self.wte(input_ids).squeeze(0)

            if len(input_ids) == 0:
                # if we have just starting sequences audio_inputs_embeds is an empty tensor
                position_embeds = audio_inputs_embeds.clone()
            else:
                position_embeds = self.wpe.get_fixed_embedding(
                    position_ids, input_ids.device
                ) if not is_profiling_run else self.wpe(input_ids.reshape(-1, 1))

            hidden_states = (audio_inputs_embeds + position_embeds).view(-1, self.embed_dim)

            if isinstance(input_embeds, list) and len(input_embeds) > 0:
                hidden_states = self._insert_conditioning_into_hidden_states(
                    hidden_states,
                    input_embeds,
                    starting_sequence_embed,
                    starting_sequence_start_ids,
                    is_logit_only)

        else:
            assert intermediate_tensors is not None
            hidden_states = intermediate_tensors["hidden_states"]

        for i in range(self.start_layer, self.end_layer):
            layer = self.h[i]
            hidden_states = layer(hidden_states,
                                  kv_caches[i - self.start_layer],
                                  attn_metadata)

        if not get_pp_group().is_last_rank:
            return IntermediateTensors({"hidden_states": hidden_states})

        hidden_states = self.ln_f(hidden_states)
        return hidden_states



================================================================================
# File: auralis/models/xttsv2/components/tts/__init__.py
# Path: /home/marco/PycharmProjects/betterVoiceCraft/Auralis/src/auralis/models/xttsv2/components/tts/__init__.py
================================================================================

#  Copyright (c) 2024 Astramind. Licensed under Apache License, Version 2.0.



================================================================================
# File: auralis/models/xttsv2/components/tts/layers/__init__.py
# Path: /home/marco/PycharmProjects/betterVoiceCraft/Auralis/src/auralis/models/xttsv2/components/tts/layers/__init__.py
================================================================================

#  Copyright (c) 2024 Astramind. Licensed under Apache License, Version 2.0.



================================================================================
# File: auralis/models/xttsv2/components/tts/layers/xtts/__init__.py
# Path: /home/marco/PycharmProjects/betterVoiceCraft/Auralis/src/auralis/models/xttsv2/components/tts/layers/xtts/__init__.py
================================================================================

#  Copyright (c) 2024 Astramind. Licensed under Apache License, Version 2.0.



================================================================================
# File: auralis/models/xttsv2/components/tts/layers/xtts/zh_num2words.py
# Path: /home/marco/PycharmProjects/betterVoiceCraft/Auralis/src/auralis/models/xttsv2/components/tts/layers/xtts/zh_num2words.py
================================================================================

# Authors:
#   2019.5 Zhiyang Zhou (https://github.com/Joee1995/chn_text_norm.git)
#   2019.9 - 2022 Jiayu DU

import argparse
import csv
import re
import string
import sys

# fmt: off

# ================================================================================ #
#                                    basic constant
# ================================================================================ #
CHINESE_DIGIS = "é¶ä¸äºä¸åäºå­ä¸å«ä¹"
BIG_CHINESE_DIGIS_SIMPLIFIED = "é¶å£¹è´°åèä¼éææç"
BIG_CHINESE_DIGIS_TRADITIONAL = "é¶å£¹è²³åèä¼é¸ææç"
SMALLER_BIG_CHINESE_UNITS_SIMPLIFIED = "åç¾åä¸"
SMALLER_BIG_CHINESE_UNITS_TRADITIONAL = "æ¾ä½°ä»è¬"
LARGER_CHINESE_NUMERING_UNITS_SIMPLIFIED = "äº¿åäº¬åç§­ç©°æ²æ¶§æ­£è½½"
LARGER_CHINESE_NUMERING_UNITS_TRADITIONAL = "ååäº¬åç§­ç©°æºæ¾æ­£è¼"
SMALLER_CHINESE_NUMERING_UNITS_SIMPLIFIED = "åç¾åä¸"
SMALLER_CHINESE_NUMERING_UNITS_TRADITIONAL = "æ¾ä½°ä»è¬"

ZERO_ALT = "ã"
ONE_ALT = "å¹º"
TWO_ALTS = ["ä¸¤", "å©"]

POSITIVE = ["æ­£", "æ­£"]
NEGATIVE = ["è´", "è² "]
POINT = ["ç¹", "é»"]
# PLUS = [u'å ', u'å ']
# SIL = [u'æ ', u'æ§']

FILLER_CHARS = ["å", "å"]

ER_WHITELIST = (
    "(å¿å¥³|å¿å­|å¿å­|å¥³å¿|å¿åª³|å¦»å¿|"
    "èå¿|å©´å¿|æ°çå¿|å©´å¹¼å¿|å¹¼å¿|å°å¿|å°å¿|å¿æ­|å¿ç«¥|å¿ç§|æå¿æ|å­¤å¿|"
    "å¿æ|å¿å|å°å¿åº|é¹¿å¿å²|æ­£å¿å«ç»|åå¿éå½|çå¿è²å¥³|æå¿å¸¦å¥³|å»å¿é²è|ç´å¿åå¥³|"
    "ä½³å¿ä½³å¦|å¿æå½æ°|å¿æ å¸¸ç¶|å¿ä¸å«æ¯ä¸|å¿è¡åéæ¯æå¿§|å¿å¤§ä¸ç±ç·|èä¹å¿)"
)
ER_WHITELIST_PATTERN = re.compile(ER_WHITELIST)

# ä¸­ææ°å­ç³»ç»ç±»å
NUMBERING_TYPES = ["low", "mid", "high"]

CURRENCY_NAMES = "(äººæ°å¸|ç¾å|æ¥å|è±é|æ¬§å|é©¬å|æ³é|å æ¿å¤§å|æ¾³å|æ¸¯å¸|åä»¤|è¬å°é©¬å|ç±å°å°é|" "éæ|è·å°ç¾|åæ¯åºå¤|æ¯å¡å¡|å°å°¼ç¾|æåç¹|æ°è¥¿å°å|æ¯ç´¢|å¢å¸|æ°å å¡å|é©å|æ³°é¢)"
CURRENCY_UNITS = "((äº¿|åä¸|ç¾ä¸|ä¸|å|ç¾)|(äº¿|åä¸|ç¾ä¸|ä¸|å|ç¾|)å|(äº¿|åä¸|ç¾ä¸|ä¸|å|ç¾|)å|è§|æ¯|å)"
COM_QUANTIFIERS = (
    "(å¹|å¼ |åº§|å|åº|å°¾|æ¡|ä¸ª|é¦|é|éµ|ç½|ç®|é¡¶|ä¸|æ£µ|åª|æ¯|è¢­|è¾|æ|æ|é¢|å£³|çª |æ²|å¢|ç¾¤|è|"
    "ç £|åº§|å®¢|è´¯|æ|æ|å|ä»¤|æ|æ|ç½|å¡|å±±|å²­|æ±|æºª|é|é|å|å|å¯¹|åº|å£|å¤´|è|æ¿|è·³|æ|ä»¶|è´´|"
    "é|çº¿|ç®¡|å|ä½|èº«|å |è¯¾|æ¬|é¡µ|å®¶|æ·|å±|ä¸|æ¯«|å|å|é±|ä¸¤|æ¤|æ|é¢|ç³|é§|é±|å¿½|(å|æ¯«|å¾®)å|"
    "æ¯«|å|å|å¯¸|å°º|ä¸|é|å¯»|å¸¸|éº|ç¨|(å|å|å|æ¯«|å¾®)ç±³|æ®|åº|å|å|æ|ç³|ç|ç¢|ç¢|å |æ¡¶|ç¬¼|ç|"
    "ç|æ¯|é|æ|é|ç°|ç¯®|ç|æ¡¶|ç½|ç¶|å£¶|å®|ç|ç®©|ç®±|ç²|å|è¢|éµ|å¹´|æ|æ¥|å­£|å»|æ¶|å¨|å¤©|ç§|å|æ¬|"
    "çºª|å²|ä¸|æ´|å¤|æ¥|å¤|ç§|å¬|ä»£|ä¼|è¾|ä¸¸|æ³¡|ç²|é¢|å¹¢|å |æ¡|æ ¹|æ¯|é|é¢|ç|å¼ |é¢|å)"
)


# Punctuation information are based on Zhon project (https://github.com/tsroten/zhon.git)
CN_PUNCS_STOP = "ï¼ï¼ï½¡ã"
CN_PUNCS_NONSTOP = "ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ ï¼»ï¼¼ï¼½ï¼¾ï¼¿ï½ï½ï½ï½ï½ï½ï½ ï½¢ï½£ï½¤ããããããããããããããããããããããã°ã¾ã¿ââââââââââ¦â§ï¹Â·ãã-"
CN_PUNCS = CN_PUNCS_STOP + CN_PUNCS_NONSTOP

PUNCS = CN_PUNCS + string.punctuation
PUNCS_TRANSFORM = str.maketrans(PUNCS, "," * len(PUNCS), "")  # replace puncs with English comma


# https://zh.wikipedia.org/wiki/å¨è¡ååè¡
QJ2BJ = {
    "ã": " ",
    "ï¼": "!",
    "ï¼": '"',
    "ï¼": "#",
    "ï¼": "$",
    "ï¼": "%",
    "ï¼": "&",
    "ï¼": "'",
    "ï¼": "(",
    "ï¼": ")",
    "ï¼": "*",
    "ï¼": "+",
    "ï¼": ",",
    "ï¼": "-",
    "ï¼": ".",
    "ï¼": "/",
    "ï¼": "0",
    "ï¼": "1",
    "ï¼": "2",
    "ï¼": "3",
    "ï¼": "4",
    "ï¼": "5",
    "ï¼": "6",
    "ï¼": "7",
    "ï¼": "8",
    "ï¼": "9",
    "ï¼": ":",
    "ï¼": ";",
    "ï¼": "<",
    "ï¼": "=",
    "ï¼": ">",
    "ï¼": "?",
    "ï¼ ": "@",
    "ï¼¡": "A",
    "ï¼¢": "B",
    "ï¼£": "C",
    "ï¼¤": "D",
    "ï¼¥": "E",
    "ï¼¦": "F",
    "ï¼§": "G",
    "ï¼¨": "H",
    "ï¼©": "I",
    "ï¼ª": "J",
    "ï¼«": "K",
    "ï¼¬": "L",
    "ï¼­": "M",
    "ï¼®": "N",
    "ï¼¯": "O",
    "ï¼°": "P",
    "ï¼±": "Q",
    "ï¼²": "R",
    "ï¼³": "S",
    "ï¼´": "T",
    "ï¼µ": "U",
    "ï¼¶": "V",
    "ï¼·": "W",
    "ï¼¸": "X",
    "ï¼¹": "Y",
    "ï¼º": "Z",
    "ï¼»": "[",
    "ï¼¼": "\\",
    "ï¼½": "]",
    "ï¼¾": "^",
    "ï¼¿": "_",
    "ï½": "`",
    "ï½": "a",
    "ï½": "b",
    "ï½": "c",
    "ï½": "d",
    "ï½": "e",
    "ï½": "f",
    "ï½": "g",
    "ï½": "h",
    "ï½": "i",
    "ï½": "j",
    "ï½": "k",
    "ï½": "l",
    "ï½": "m",
    "ï½": "n",
    "ï½": "o",
    "ï½": "p",
    "ï½": "q",
    "ï½": "r",
    "ï½": "s",
    "ï½": "t",
    "ï½": "u",
    "ï½": "v",
    "ï½": "w",
    "ï½": "x",
    "ï½": "y",
    "ï½": "z",
    "ï½": "{",
    "ï½": "|",
    "ï½": "}",
    "ï½": "~",
}
QJ2BJ_TRANSFORM = str.maketrans("".join(QJ2BJ.keys()), "".join(QJ2BJ.values()), "")


# 2013 China National Standard: https://zh.wikipedia.org/wiki/éç¨è§èæ±å­è¡¨, raw resources:
#   https://github.com/mozillazg/pinyin-data/blob/master/kMandarin_8105.txt with 8105 chinese chars in total
CN_CHARS_COMMON = (
    "ä¸ä¸ä¸ä¸ä¸ä¸ä¸ä¸ä¸ä¸ä¸ä¸ä¸ä¸ä¸ä¸ä¸ä¸ä¸ä¸ä¸ä¸ä¸ä¸ä¸¢ä¸¤ä¸¥ä¸§ä¸ªä¸«ä¸­ä¸°ä¸²ä¸´ä¸¸ä¸¹ä¸ºä¸»ä¸½ä¸¾"
    "ä¹ä¹ä¹ä¹ä¹ä¹ä¹ä¹ä¹ä¹ä¹ä¹ä¹ä¹ä¹ä¹ä¹ä¹ä¹ä¹ä¹ä¹ ä¹¡ä¹¦ä¹©ä¹°ä¹±ä¹³ä¹¸ä¹¾äºäºäºäºäºäºäºäºäºäº"
    "äºäºäºäºäºäºäºäº¡äº¢äº¤äº¥äº¦äº§äº¨äº©äº«äº¬äº­äº®äº²äº³äºµäº¶äº¸äº¹äººäº¿ä»ä»ä»ä»ä»ä»ä»ä»ä»ä»ä»ä»ä»"
    "ä»ä»ä»ä»ä»ä»ä»ä»ä»ä»ä»ä»¡ä»£ä»¤ä»¥ä»¨ä»ªä»«ä»¬ä»°ä»²ä»³ä»µä»¶ä»·ä»»ä»½ä»¿ä¼ä¼ä¼ä¼ä¼ä¼ä¼ä¼ä¼ä¼ä¼ä¼"
    "ä¼ä¼ä¼ä¼ä¼ä¼ ä¼¢ä¼£ä¼¤ä¼¥ä¼¦ä¼§ä¼ªä¼«ä¼­ä¼¯ä¼°ä¼²ä¼´ä¼¶ä¼¸ä¼ºä¼¼ä¼½ä¼¾ä½ä½ä½ä½ä½ä½ä½ä½ä½ä½ä½ä½ä½ä½ä½"
    "ä½ä½ä½ä½ä½ä½ ä½£ä½¤ä½¥ä½©ä½¬ä½¯ä½°ä½³ä½´ä½¶ä½¸ä½ºä½»ä½¼ä½½ä½¾ä½¿ä¾ä¾ä¾ä¾ä¾ä¾ä¾ä¾ä¾ä¾ä¾ä¾ä¾ä¾ä¾ä¾ ä¾£"
    "ä¾¥ä¾¦ä¾§ä¾¨ä¾©ä¾ªä¾¬ä¾®ä¾¯ä¾´ä¾µä¾¹ä¾¿ä¿ä¿ä¿ä¿ä¿ä¿ä¿ä¿ä¿ä¿ä¿ä¿ä¿ä¿ä¿ä¿ä¿ä¿¡ä¿£ä¿¦ä¿¨ä¿©ä¿ªä¿«ä¿­ä¿®ä¿¯"
    "ä¿±ä¿³ä¿µä¿¶ä¿¸ä¿ºä¿¾åååååååååååååå¡å¥å¦å§å¨å©åªå¬å­å®å´åºå»å¼å¾ååååå"
    "ååååååå¡å¥å¬å­å°å²å¶å·å»å¾å¿ååååååååå£å¥å§å¨å©å¬å²åºå»åååååå"
    "å¦å§å¬å­å®å°å³åµå»ååååå¡å¦å³å´å¿ååååååååååååååååå¢å¥å¨å«å¬å­"
    "å®å°å±å³å´åµå¶å·å¸å¹å»å¼å½åååååååååååååååå å¢å¤å¥å¬å®å¯å°å±å²å³åµ"
    "å¶å·å»å¼å½åååååååååååååå å¡å¤å«å­å¯å°å³å¶å¸å¹åºå»å¼å½å¿åååååå"
    "ååååååååååååå å¤å¨å©å«å¬å­å®å°å³å¶å·å¸å¹åºå»å½å¿åååååååååå"
    "åååååååå¡å¥å§å©åªå¯å²å½å¿ååååååååååå å¡å¢å£å¨å©åªå«å¬å­å±å²å³å¼"
    "å¾å¿ååååååååååååå å¤å°åºå¾å¿åååååååååååååå å¡å£å¦åªå®å¹"
    "åºå»å¼å¾å¿åååååååååååååååååååå å¡å¢å£å¤å¦å§å«å¬å®å¯å°å±å³å´åµ"
    "å·å¸åºå¿ååååååååååååååå¢å£å¥å¦å¨å©å®å»å¾å¿åååååååååååå"
    "ååååååååå å£å¤å¥å¦å¨å©åªå«å¬å­å®å¯å°å±å²å³åµå¶å·å¸å¹å»å¼å½ååååååå"
    "åååååååååååååååå å¡å£å¦å§å¨å©å«å¬å­å®å¯å±å²å´åµå¸å¹å»å¼å½å¾ååå"
    "åååååååååååååååå¢å£å¤å¦å¨å±å²å³åµå¶å·å¸å»å¼å½åååååååååå"
    "ååååååååååå¡å£å¤å¥å¦å§å¨å©åªå«å¬å¯å±å³å´å¸åºå»å½å¿åååååååååå"
    "åååååååååååååååå¢å¥å¦å§å¨å©åªå­å®å±å²å³åºå¼å½å¿ååååååååå"
    "åå å¢å£å¤å§åªå¬å®å¯å°å±å³åµå·å¼å¾å¿ååååååååååå¡å¤å¥å¦å§åªå«å¬å­å®å°å´"
    "åµå¶å·å¸å»å¼å¾åååååååååååååååååååå¤å§å±å³åµå·å¹å»å½å¾åååå"
    "åååååååååååå¡å£å¤å¥å¦å¨åªå«å¬å¯å²å³åµå·å½å¾ååååååååååååå¡"
    "å£å¤å§å¬å­å±å²å´å¶å¹å»å¿åååååååååååå¢å¤å¨å©åªå«å¬å±å¶å»å¼åååååå"
    "åå£å­å¯å·å¼ååååååå å¡å¢å¤å«å­å°å±å´åµå·å¹åºå½å¾å¿åååååååååååå¢"
    "å£å¨å©åªå«å¬å­å®å¯å°å²å³å¹åºå»å¾ååååååååååååååååååååå å¡å¤å¥"
    "å¦å¨å©åªå«å¬å­å¯å°å³å·å»å¼å½ååååååååååååååååååå å¡å¢å£å¤å¦å§å©"
    "å«å­å®å¯å±å²å´åµå¸åºå¾å¿åååååååååååååååååå å¤åªå«å­å¯å´åµå¸å¹åº"
    "å¼å½å å å å å å å å å å å å å å å  å ¡å ¤å §å ¨å ªå °å ²å µå ¼å ½å ¾å¡å¡å¡å¡å¡å¡å¡å¡å¡å¡å¡¥å¡«"
    "å¡¬å¡±å¡¾å¢å¢å¢å¢å¢å¢å¢å¢å¢å¢å¢å¢å¢å¢å¢å¢¡å¢£å¢¦å¢¨å¢©å¢¼å£å£å£å£å£¤å£«å£¬å£®å£°å£³å£¶å£¸å£¹å¤å¤å¤"
    "å¤å¤å¤å¤å¤å¤å¤å¤å¤å¤¤å¤¥å¤§å¤©å¤ªå¤«å¤¬å¤­å¤®å¤¯å¤±å¤´å¤·å¤¸å¤¹å¤ºå¤¼å¥å¥å¥å¥å¥å¥å¥å¥å¥å¥å¥å¥å¥å¥"
    "å¥å¥å¥å¥ å¥¡å¥¢å¥¥å¥­å¥³å¥´å¥¶å¥¸å¥¹å¥½å¦å¦å¦å¦å¦å¦å¦å¦å¦å¦å¦å¦å¦å¦å¦å¦å¦£å¦¤å¦¥å¦§å¦¨å¦©å¦ªå¦«å¦­å¦®"
    "å¦¯å¦²å¦¹å¦»å¦¾å§å§å§å§å§å§å§å§å§å§å§å§å§å§å§å§£å§¤å§¥å§¨å§¬å§®å§±å§¶å§¹å§»å§½å§¿å¨å¨å¨å¨å¨å¨å¨å¨"
    "å¨å¨å¨å¨å¨å¨å¨å¨ å¨£å¨¥å¨©å¨±å¨²å¨´å¨µå¨¶å¨¼å©å©å©å©å©å©å©å©å©å©å© å©¢å©¤å©§å©ªå©«å©³å©´å©µå©¶å©·å©ºå©»"
    "å©¼å©¿åªåªåªåªåªåªåªåªåªåªªåª­åª±åª²åª³åªµåª¸åª¾å«å«å«å«å«å«å«å«å«å«å«å«å« å«¡å«£å«¦å«©å«ªå««å«­å«±"
    "å«½å¬å¬å¬å¬å¬¥å¬¬å¬´å¬·å¬¿å­å­å­å­å­å­å­å­å­å­å­å­å­å­å­å­å­¢å­£å­¤å­¥å­¦å­©å­ªå­¬å­°å­±å­³å­µå­ºå­½"
    "å®å®å®å®å®å®å®å®å®å®å®å®å®å®å®å®å®å®å®å®å® å®¡å®¢å®£å®¤å®¥å®¦å®§å®ªå®«å®¬å®°å®³å®´å®µå®¶å®¸å®¹å®½å®¾"
    "å®¿å¯å¯å¯å¯å¯å¯å¯å¯å¯å¯å¯å¯å¯å¯¡å¯¤å¯¥å¯¨å¯®å¯°å¯¸å¯¹å¯ºå¯»å¯¼å¯¿å°å°å°å°å°å°å°å°å°å°å°å°å°å°"
    "å°¢å°¤å°¥å°§å°¨å°ªå°¬å°±å°´å°¸å°¹å°ºå°»å°¼å°½å°¾å°¿å±å±å±å±å±å±å±å±å±å±å±å±å±å±å±å±å± å±¡å±£å±¥å±¦å±¯å±±"
    "å±¹å±ºå±¼å±¾å±¿å²å²å²å²å²å²å²å²å²å²å²å²å²å²å²å²å²å² å²¢å²£å²¨å²©å²«å²¬å²­å²±å²³å²µå²·å²¸å²½å²¿å³å³å³"
    "å³å³å³å³å³å³å³å³¡å³£å³¤å³¥å³¦å³§å³¨å³ªå³­å³°å³±å³»å³¿å´å´å´å´å´å´å´å´å´å´å´å´å´å´å´å´å´¡å´¤å´¦å´§"
    "å´©å´­å´®å´´å´¶å´½å´¾å´¿åµåµåµåµåµåµåµåµåµåµåµåµåµ©åµ«åµ¬åµ¯åµ²åµ´å¶å¶å¶å¶å¶å¶å¶å¶å¶¦å¶²å¶·å·å·å·"
    "å·å·å·å·¡å·¢å·¥å·¦å·§å·¨å·©å·«å·®å·¯å·±å·²å·³å·´å··å·½å·¾å¸å¸å¸å¸å¸å¸å¸å¸å¸å¸å¸å¸å¸å¸å¸å¸å¸å¸å¸å¸¡"
    "å¸¦å¸§å¸¨å¸­å¸®å¸±å¸·å¸¸å¸»å¸¼å¸½å¹å¹å¹å¹å¹å¹å¹å¹å¹å¹¡å¹¢å¹ªå¹²å¹³å¹´å¹¶å¹¸å¹ºå¹»å¹¼å¹½å¹¿åºåºåºåºåºåºåº"
    "åºåºåºåºåºåºåºåºåºåºåºåº åº¤åº¥åº¦åº§åº­åº±åº³åºµåº¶åº·åº¸åº¹åº¼åº¾å»å»å»å»å»å»å»å»å»å»å»¨å»ªå»¶å»·"
    "å»ºå»¿å¼å¼å¼å¼å¼å¼å¼å¼å¼å¼å¼å¼å¼å¼å¼å¼å¼å¼å¼ å¼¢å¼¥å¼¦å¼§å¼¨å¼©å¼­å¼¯å¼±å¼¶å¼¸å¼¹å¼ºå¼¼å½å½å½å½å½"
    "å½å½å½å½å½¢å½¤å½¦å½§å½©å½ªå½¬å½­å½°å½±å½³å½·å½¹å½»å½¼å¾å¾å¾å¾å¾å¾å¾å¾å¾å¾å¾å¾å¾å¾å¾å¾å¾å¾å¾¡å¾¨å¾ª"
    "å¾­å¾®å¾µå¾·å¾¼å¾½å¿å¿å¿å¿å¿å¿å¿å¿å¿å¿å¿å¿å¿å¿å¿å¿å¿ å¿¡å¿¤å¿§å¿ªå¿«å¿­å¿®å¿±å¿³å¿µå¿¸å¿ºå¿»å¿½å¿¾å¿¿æ"
    "æææææææææææææææææææ æ¡æ¥æ¦æ§æ¨æ©æªæ«æ¯æµæ»æ¼æ¿ææææææææ"
    "ææææææ¢æ£æ¤æ§æ¨æ©æªæ«æ¬æ­æ¯æ°æ³æ¶æ¸æ¹æºæ»æ¼æ½æ¿ææææææææææææææ"
    "æ æ¢æ£æ¦æ¨æ«æ¬æ­æ¯æ°æ±æ²æ´æ¸æ»æ¼æææææææææææææææææ æ¦æ§æ¨æ©æ«æ¬æ­"
    "æ®æ¯æ°æ³æ´æ¶æ¹æºæææææææææææææææ æ£æ¤æ¦æ§æ«æ­æ¿æææææææææ¢æ¥"
    "æ§æ¨æ¬æ­æ°æµæ·ææææææ§æ¨æ©æ¬æ­æ·æºæ¾ææææææææ¦æµæ¿ææææææææææ"
    "æææææææææ¡æ¢æ£æ¤æ¥æªæ¬æ­æ®æ³æ´æ·æ½æ¾æ¿æææææææææææææææææ"
    "ææææ£æ¦æ§æ©æªæ«æ¬æ­æ®æ¯æ°æ³æ¶æ¹æºæ¼æ½æ¾æ¿æææææææææææææææææ æ¡"
    "æ¢æ¤æ¥æ¨æ«æ¬æ±æµæ¹æ»æ¼æ½æ¿ææææææææææææææææææææææææ¢æ£æ¤æ¥"
    "æ¦æ§æ¨æ©æ¬æ­æ®æ¯æ±æ³æ´æ¶æ·æ¼æ½æ¾æ¿æææææææææææææææ æ¡æ£æ¤æ¥æ¦æ¨æªæ«"
    "æ¯æ²æ¹æºæ½æææææææææææææææ¡æ¢æ£æ§æ©æ­æ®æ¯æ¶æ·æºæ»æ½ææææææææ"
    "æææææææ æ¢æ£æ¥æ§æ¨æ©æªæ¬æ­æ®æ°æ³æ´æ·æ¸æºæ¼æ¾ææææææææææ æ¡æ£æ©æªæ­"
    "æ³æ´æ¶æ¸æ½æ¿æææææææææææææææ æ¡æ¦æªæ¬æ­æ´æºæ½æææææææææææ"
    "æææ§æ©æ­æ´æ¸æ¹æ½æææææææææææ¤æ©æ¬æ­æ®æ°æµæ·æ¸æºæ¼ææææææææææ¢"
    "æ¤æ¦æ¿æææææ¥æ«æ®æ¯æ¶æ¸æ¹æ»æ½æ¾æ¿ææææææææææææææ¢æ£æ¦æ©æ«æ¬æ°æ²æ´"
    "æ·æææææææææææææ æ¡æ¤æ¥æ§æ©æ«æ­æ¯æ°æ¶æ¹æ¼æ½æææææææææææææ"
    "ææ æ¢æ¥æ¦æ§æ¨æ©æ¬æ­æ®æ¯æ°æ±æ´æµæ¶æ·æ¸æºæ»æ¿ææææææææææææææææææ"
    "ææ æ¡æ£æ¤æ¥æ§æ¨æªæ«æ­æ¯æ±æ³æ´æµæ¶æºæ¼æ½æ¾æææææææææææææææææææ¡"
    "æ¢æ¤æ¦æ¨æªæ«æ®æ¯æ°æ±æ´æ¶æ·æºæ¾æææææææææææ§æ¨æ®æ²æ´æµæ¶æ¹æ¾æ¿æææææ"
    "ææ¦æ©æ°æ²æ³æ´æ·æ¹æ¼æ¾æ¿æææææææææææææææ¦æ¨æªæ«æ¬æ­æ¯æ±æ³æ´æµæ¸æºæ½"
    "ææææææææææææææææææææ æ¡æ¥æ§æ¨æ©æªæ­æ¯æ°æ²æ³æµæ·æ»æ¼æ¾æ¿æææ"
    "æææææææææææææ¢æ£æ¥æ§æ¨æªæ«æ­æ¯æ°æ²æ³æµæ¶æ·æ¸æ¹æææææææææææ"
    "ææææææææ æ¢æ¥æ©æ¬æ¯æ°æ±æ³æ´æ·æ½æ¿æ æ æ æ æ æ æ æ æ æ æ æ æ æ æ æ æ æ æ ¡æ ©"
    "æ ªæ ²æ ³æ ´æ ·æ ¸æ ¹æ »æ ¼æ ½æ ¾æ¡æ¡æ¡æ¡æ¡æ¡æ¡æ¡æ¡æ¡æ¡æ¡æ¡æ¡æ¡æ¡æ¡æ¡ æ¡¡æ¡¢æ¡£æ¡¤æ¡¥æ¡¦æ¡§æ¡¨æ¡©æ¡«æ¡¯"
    "æ¡²æ¡´æ¡¶æ¡·æ¡¹æ¢æ¢æ¢æ¢æ¢æ¢æ¢æ¢æ¢ æ¢¢æ¢£æ¢¦æ¢§æ¢¨æ¢­æ¢¯æ¢°æ¢³æ¢´æ¢µæ¢¼æ¢½æ¢¾æ¢¿æ£æ£æ£æ£æ£æ£æ£æ£æ£æ£æ£"
    "æ£æ£ æ££æ£¤æ£¨æ£ªæ£«æ£¬æ£®æ£°æ£±æ£µæ£¹æ£ºæ£»æ£¼æ£½æ¤æ¤æ¤æ¤æ¤æ¤æ¤æ¤æ¤æ¤æ¤æ¤æ¤ æ¤¤æ¤ªæ¤­æ¤°æ¤´æ¤¸æ¤¹æ¤½æ¤¿æ¥"
    "æ¥æ¥æ¥æ¥æ¥æ¥æ¥æ¥ æ¥£æ¥¦æ¥©æ¥ªæ¥«æ¥®æ¥¯æ¥·æ¥¸æ¥¹æ¥¼æ¦æ¦æ¦æ¦æ¦æ¦æ¦æ¦æ¦æ¦æ¦æ¦æ¦æ¦æ¦æ¦§æ¦¨æ¦«æ¦­æ¦°æ¦±"
    "æ¦´æ¦·æ¦»æ§æ§æ§æ§æ§æ§æ§æ§æ§æ§æ§æ§ æ§­æ§±æ§²æ§½æ§¿æ¨æ¨æ¨æ¨æ¨¡æ¨¨æ¨ªæ¨¯æ¨±æ¨µæ¨½æ¨¾æ©æ©æ©æ©æ©æ©æ©æ©"
    "æ©¡æ©¥æ©¦æ©±æ©¹æ©¼æªæªæªæªæªæªæªæª æª©æª«æª¬æ«æ¬æ¬ æ¬¡æ¬¢æ¬£æ¬¤æ¬§æ¬²æ¬¸æ¬¹æ¬ºæ¬»æ¬¾æ­æ­æ­æ­æ­æ­æ­æ­¢æ­£"
    "æ­¤æ­¥æ­¦æ­§æ­ªæ­¹æ­»æ­¼æ®æ®æ®æ®æ®æ®æ®æ®æ®æ®æ®æ®æ®æ®æ®æ®¡æ®£æ®ªæ®³æ®´æ®µæ®·æ®¿æ¯æ¯æ¯æ¯æ¯æ¯æ¯æ¯æ¯"
    "æ¯æ¯æ¯æ¯æ¯æ¯æ¯æ¯¡æ¯ªæ¯«æ¯¯æ¯³æ¯µæ¯¹æ¯½æ°æ°æ°æ°æ°æ°æ°æ°æ°æ°æ°æ°æ°æ°æ°æ°æ°¡æ°¢æ°¤æ°¦æ°§æ°¨æ°©æ°ªæ°®"
    "æ°¯æ°°æ°²æ°´æ°¸æ°¾æ°¿æ±æ±æ±æ±æ±æ±æ±æ±æ±æ±æ±æ±æ±æ±æ±æ±æ±æ±æ± æ±¡æ±¤æ±§æ±¨æ±©æ±ªæ±«æ±­æ±°æ±²æ±´æ±¶æ±¹æ±½"
    "æ±¾æ²æ²æ²æ²æ²æ²æ²æ²æ²æ²æ²æ²æ²æ²æ²æ²æ²æ²æ²æ²¡æ²£æ²¤æ²¥æ²¦æ²§æ²¨æ²©æ²ªæ²«æ²­æ²®æ²±æ²³æ²¸æ²¹æ²ºæ²»æ²¼æ²½"
    "æ²¾æ²¿æ³æ³æ³æ³æ³æ³æ³æ³æ³æ³æ³æ³æ³æ³æ³æ³æ³æ³æ³æ³ æ³¡æ³¢æ³£æ³¥æ³¨æ³ªæ³«æ³®æ³¯æ³°æ³±æ³³æ³µæ³·æ³¸æ³ºæ³»æ³¼"
    "æ³½æ³¾æ´æ´æ´æ´æ´æ´æ´æ´æ´æ´æ´æ´æ´æ´æ´æ´æ´¢æ´£æ´¥æ´§æ´¨æ´ªæ´«æ´­æ´®æ´±æ´²æ´³æ´´æ´µæ´¸æ´¹æ´ºæ´»æ´¼æ´½æ´¾æ´¿"
    "æµæµæµæµæµæµæµæµæµæµæµæµæµæµæµæµæµæµæµæµæµæµæµæµæµ æµ¡æµ£æµ¥æµ¦æµ©æµªæµ¬æµ­æµ®æµ¯æµ°æµ²æµ´æµ·æµ¸"
    "æµ¼æ¶æ¶æ¶æ¶æ¶æ¶æ¶æ¶æ¶æ¶æ¶æ¶æ¶æ¶æ¶æ¶æ¶æ¶æ¶ æ¶¡æ¶¢æ¶£æ¶¤æ¶¦æ¶§æ¶¨æ¶©æ¶ªæ¶«æ¶®æ¶¯æ¶²æ¶´æ¶µæ¶¸æ¶¿æ·æ·æ·"
    "æ·æ·æ·æ·æ·æ·æ·æ·æ·æ·æ·æ·æ·æ· æ·¡æ·¤æ·¦æ·«æ·¬æ·®æ·¯æ·±æ·³æ·´æ··æ·¹æ·»æ·¼æ¸æ¸æ¸æ¸æ¸æ¸æ¸æ¸æ¸æ¸æ¸æ¸"
    "æ¸ æ¸¡æ¸£æ¸¤æ¸¥æ¸©æ¸«æ¸­æ¸¯æ¸°æ¸²æ¸´æ¸¸æ¸ºæ¸¼æ¹æ¹æ¹æ¹æ¹æ¹æ¹æ¹æ¹æ¹æ¹æ¹æ¹æ¹æ¹£æ¹«æ¹®æ¹²æ¹´æ¹¾æ¹¿æºæºæºæº"
    "æºæºæºæºæºæºæºæºæºæºæº æº¢æº¥æº¦æº§æºªæº¯æº±æº²æº´æºµæº¶æº·æº¹æººæº»æº½æ»æ»æ»æ»æ»æ»æ»æ»æ»æ»æ»æ»æ»"
    "æ»æ»æ»æ»æ»æ» æ»¡æ»¢æ»¤æ»¥æ»¦æ»§æ»¨æ»©æ»ªæ»«æ»´æ»¹æ¼æ¼æ¼æ¼æ¼æ¼æ¼æ¼æ¼æ¼æ¼ æ¼¤æ¼¦æ¼©æ¼ªæ¼«æ¼­æ¼¯æ¼±æ¼³æ¼´æ¼¶"
    "æ¼·æ¼¹æ¼»æ¼¼æ¼¾æ½æ½æ½æ½æ½æ½æ½æ½æ½æ½æ½¢æ½¦æ½©æ½­æ½®æ½²æ½´æ½µæ½¸æ½ºæ½¼æ½½æ½¾æ¾æ¾æ¾æ¾æ¾æ¾æ¾æ¾æ¾æ¾¡æ¾¥æ¾§"
    "æ¾ªæ¾­æ¾³æ¾´æ¾¶æ¾¹æ¾¼æ¾½æ¿æ¿æ¿æ¿æ¿æ¿æ¿æ¿ æ¿¡æ¿©æ¿®æ¿¯ççççççç£ç±çµç¹ç¼ççççç«ç­ç¯ç°çµ"
    "ç¶ç¸ç¼ç¾ç¿ççççççççççççççççç£ç«ç¬ç­ç®ç¯ç±ç³ç·ç¸ç¹ç»ç¼ç½ççççç"
    "ççççççççç ç¤ç¦ç§ç¨ç©ç«ç¬ç­ç¯ç¶ç·ç¹çºç»ç½ççççççççççççççç¦ç¯"
    "ç°ç±ç¶çççççççççççç¤ç¦ç§ç¨ç®ç²ç³ç´ç¸çºç½ççççççççççç ç¥ç¨ç¬çµ"
    "ç¹ç»ççççççççç ç¥ç§ç®ç¹ççççççç¨çªç¬ç°ç±çµç¶ç·ç¸ç¹ç»ç½ç¿çççççç"
    "çççççççç¡ç¢ç¤ç¥ç¦ç§ç©ç®ç¯ç²çµç¹çºç»ç¾ç¿ççççççççççç¨ç¬ç¯ç°ç´ç¶ç·"
    "ç¸ç¹ççççççççççççççç ç¡ç¨ç©ç¬ç­ç®ç¯ç°ç±ç²ç³ç´ç·ç¸çºç»ç¼çççççç"
    "çççççççç¡ç¢ç¥ç©çªç«ç¬ç®ç¯ç°ç±ç´ç·ç¹çºç¾ç¿ççççç ç¬ç­ç¯ç´ç¾çççççç"
    "çççççççççççç ç¡ç¢ç¤ç¥ç¦ç©ç«ç­ç®ç¯ç°ç±ç²ç³ç¶ç·ç¹çºç»ç¼ç¿ççççççç"
    "çççççççççççççç ç¢ç£ç¥ç¦ç§ç©çªç«ç­ç°ç²çµç·ç¸ç¹çºç½ççççççççç"
    "çççççççç¡ç¢ç¤ç¥ç¦ç¨çªç«ç¬ç­ç®ç¯ç°ç²ç³ç´çµç¶ç¼çççççççççççççç"
    "ççççççç¢ç§ç¨ç¬ç­ç°ç±ç³ç¶ç·ç¾ççççççççççççççç ç¥ç§ç¨ç©çªç¬ç®ç±"
    "ç²çºççççççç ç¢ç£ç¤ç¦ç®ç¯ç´ç¶ç·ç»ç¿ççççççççççç¡ç¥ç¦ç¨ç©çªç«ç¬ç­ç¯"
    "ç°ç±ç²ç³çµç·ç¸çºç»ç¾çççççççççççççç¤ç¥ç¦çªç¬ç¯ç²ç´ç¸ç¹ç¿çççççç"
    "ççççççççç ç¡ç¢ç£ç¤ç¥ç«ç¬ç­ç®ç¯ç°ç±ç²ç³ç´çµç¸ç¹ç¼ç½ç¾çççççççççç"
    "ççççççç¢ç£ç¤ç¦ç§ç¨çªç«ç°ç±ç´ç¹ç¼ç¿ççççççççççççççç ç¢ç¤ç¥ç¦ç©"
    "çªç«ç­ç°ç³ç´çµç¸ç¼ç¾ç¿çççççççççç£ç«ç¯ç¸ç»ç½ç¾ç¿çççççççççççç"
    "ççç¤ç¦ç­ç®ç±ç²ç´ç¿ççççççççççççççççççç¥ç¦ç®ç¯ç±ç²ç´ç·ç¸ç¹ç¼ç¾"
    "çççççççççççç ç¢ç¦ç¨ç©ç¬ç­ç¯çµç¶ç·ç¸çºç¼ççççççççççç¡ç¢ç£ç¥ç¦"
    "ç¨ç«ç¬ç¹ç½ç¾ç¿ççççççççççç ç¢ç¥ç§ç©çªç«ç¬ç­ç°ç³çµç»ç½ç¿çççççç¢ç£ç¥"
    "ç§ç©ç«ç¬ç­ç®ç°ç³ç¶ç¸ç»ç¼ç¾ç¿ç ç ç ç ç ç ç ç ç ç ç ç ç ç ç ç ç ç ç  ç £ç ¥ç §ç «ç ¬ç ­ç ®"
    "ç °ç ´ç µç ·ç ¸ç ¹ç ºç »ç ¼ç ¾ç¡ç¡ç¡ç¡ç¡ç¡ç¡ç¡ç¡ç¡ç¡ç¡ç¡ç¡ç¡ç¡ç¡ç¡ªç¡«ç¡¬ç¡­ç¡®ç¡¼ç¡¿ç¢ç¢ç¢ç¢ç¢ç¢"
    "ç¢ç¢ç¢ç¢ç¢ç¢ç¢ç¢ç¢ç¢ç¢¡ç¢£ç¢¥ç¢§ç¢¨ç¢°ç¢±ç¢²ç¢³ç¢´ç¢¶ç¢¹ç¢¾ç£ç£ç£ç£ç£ç£ç£ç£ç£ç£ç£ç£¡ç£¨ç£¬ç£²ç£´ç£·"
    "ç£¹ç£»ç¤ç¤ç¤ç¤ç¤ç¤´ç¤µç¤ºç¤¼ç¤¾ç¥ç¥ç¥ç¥ç¥ç¥ç¥ç¥ç¥ç¥ç¥ç¥ç¥ç¥ç¥ç¥ç¥ç¥ç¥ç¥ç¥ç¥ç¥ ç¥¢ç¥¥ç¥§ç¥¨ç¥­"
    "ç¥¯ç¥²ç¥·ç¥¸ç¥ºç¥¼ç¥¾ç¦ç¦ç¦ç¦ç¦ç¦ç¦ç¦ç¦ç¦ç¦ç¦ç¦¤ç¦§ç¦³ç¦¹ç¦ºç¦»ç¦½ç¦¾ç§ç§ç§ç§ç§ç§ç§ç§ç§ç§ç§ç§ç§£"
    "ç§¤ç§¦ç§§ç§©ç§«ç§¬ç§­ç§¯ç§°ç§¸ç§»ç§½ç§¾ç¨ç¨ç¨ç¨ç¨ç¨ç¨ç¨ç¨ç¨ç¨ç¨ç¨ç¨ç¨ ç¨£ç¨³ç¨·ç¨¹ç¨»ç¨¼ç¨½ç¨¿ç©ç©ç©ç©"
    "ç©ç©ç©ç©°ç©´ç©¶ç©·ç©¸ç©¹ç©ºç©¿çªçªçªçªçªçªçªçªçªçªçªçªçªçªçªçªçªçªçª çª£çª¥çª¦çª¨çª¬çª­çª³çª¸çª¿ç«"
    "ç«ç«ç«ç«ç«ç«ç« ç«£ç«¥ç«¦ç««ç«­ç«¯ç«¹ç«ºç«½ç«¿ç¬ç¬ç¬ç¬ç¬ç¬ç¬ç¬ç¬ç¬ç¬ç¬ç¬ç¬ ç¬¤ç¬¥ç¬¦ç¬¨ç¬ªç¬«ç¬¬ç¬®ç¬¯"
    "ç¬±ç¬³ç¬¸ç¬ºç¬¼ç¬¾ç­ç­ç­ç­ç­ç­ç­ç­ç­ç­ç­ç­ç­ç­ç­ç­ç­ç­ ç­¢ç­¤ç­¥ç­¦ç­®ç­±ç­²ç­µç­¶ç­·ç­¹ç­»ç­¼ç­¾ç®ç®"
    "ç®ç®ç®ç®ç®ç®ç®ç®ç®¡ç®¢ç®¦ç®§ç®¨ç®©ç®ªç®«ç®¬ç®­ç®±ç®´ç®¸ç¯ç¯ç¯ç¯ç¯ç¯ç¯ç¯ç¯ç¯¡ç¯¥ç¯¦ç¯ªç¯®ç¯¯ç¯±ç¯·ç¯¼ç¯¾"
    "ç°ç°ç°ç°ç°ç°ç°ç°ç°ç°ç° ç°§ç°ªç°°ç°¸ç°¿ç±ç±ç±ç±¥ç±³ç±´ç±»ç±¼ç±½ç²ç²ç²ç²ç²ç²ç²ç²ç²ç²ç²¢ç²¤ç²¥ç²ªç²®"
    "ç²±ç²²ç²³ç²¹ç²¼ç²½ç²¾ç²¿ç³ç³ç³ç³ç³ç³ç³ç³ç³ç³ç³ç³ç³ç³ç³ ç³¨ç³¯ç³µç³»ç´ç´ ç´¢ç´§ç´«ç´¯çµçµ®çµ·ç¶¦ç¶®ç¸ ç¸¢"
    "ç¸»ç¹ç¹ç¹çºçºçº çº¡çº¢çº£çº¤çº¥çº¦çº§çº¨çº©çºªçº«çº¬çº­çº®çº¯çº°çº±çº²çº³çº´çºµçº¶çº·çº¸çº¹çººçº»çº¼çº½çº¾çº¿ç»ç»"
    "ç»ç»ç»ç»ç»ç»ç»ç»ç»ç»ç»ç»ç»ç»ç»ç»ç»ç»ç»ç»ç»ç»ç»ç»ç»ç»ç»ç»ç»ç»ç» ç»¡ç»¢ç»£ç»¤ç»¥ç»¦ç»§ç»¨ç»©"
    "ç»ªç»«ç»­ç»®ç»¯ç»°ç»±ç»²ç»³ç»´ç»µç»¶ç»·ç»¸ç»¹ç»ºç»»ç»¼ç»½ç»¾ç»¿ç¼ç¼ç¼ç¼ç¼ç¼ç¼ç¼ç¼ç¼ç¼ç¼ç¼ç¼ç¼ç¼ç¼ç¼ç¼"
    "ç¼ç¼ç¼ç¼ç¼ç¼ç¼ç¼ç¼ç¼ç¼ ç¼¡ç¼¢ç¼£ç¼¤ç¼¥ç¼¦ç¼§ç¼¨ç¼©ç¼ªç¼«ç¼¬ç¼­ç¼®ç¼¯ç¼°ç¼±ç¼²ç¼³ç¼´ç¼µç¼¶ç¼¸ç¼ºç½ç½ç½ç½ç½"
    "ç½ç½ç½ç½ç½ç½ç½ç½¡ç½¢ç½¨ç½©ç½ªç½®ç½±ç½²ç½´ç½¶ç½¹ç½½ç½¾ç¾ç¾ç¾ç¾ç¾ç¾ç¾ç¾ç¾ç¾ç¾ç¾ç¾ç¾¡ç¾¤ç¾§ç¾¯ç¾°ç¾±ç¾²"
    "ç¾¸ç¾¹ç¾¼ç¾½ç¾¿ç¿ç¿ç¿ç¿ç¿ç¿ç¿ç¿ç¿ç¿ç¿ç¿ç¿ç¿ç¿ç¿ç¿ ç¿¡ç¿¥ç¿¦ç¿©ç¿®ç¿¯ç¿°ç¿±ç¿³ç¿·ç¿»ç¿¼ç¿¾èèèèè"
    "èèèèèèèèèèèèèèèèè è¢è¤è¥è¦è§è¨è©èªè°è±è³èµè¶è·è¸è»è½è¿èèèèè"
    "èèèèèèè©èªè±è¿èèèèèèèèèèèèèèè è¡è¢è¤è¥è©èªè«è­è®è¯è±è²è´è·è¸"
    "èºè¼è½è¾è¿èèèèèèèèèèèèèèèèèèè è¡è£è¤è¥è§è¨è©èªè«è¬è­è¯è°è±è²è³"
    "è´è¶è¸èºè¼è½èèèèèèèèèèèèèèèèèè©è¬è¯è±è²è¶è¸è¾è¿èèèèèèèè"
    "èèèèèèè è¥è§è¨è©è­è®è¯è°è±è´è¹èºè»è¼è½è¾è¿èèèèèèèèèèèè¦è¨è³èºè»"
    "èèèèèèèèè£è§èªè¬è­è³è´è»è¼è¾èèèèèèèèèèèèèèèè è¢è£è¥èªè«è¬"
    "è­è¯è°è±è²è³è´èµè¶è·è¸è¹è»è¾èèèèèèèèèèè¨è®è¯è°è²è³è´èºè½è¾è¿èèèèè"
    "èèèèèèèèèèèèè è¡è£è¤è¥è¦è¨è©èªè«è¬è­è®è¯è°è±è³è´è·è¸è¹è¼è½è¾èèèè"
    "èèèèèèèèèèèèèèèèèèè è¡è£è¤è¥è¦è§è«è¯è±è´è·è¹è»è¾èèèèèèè"
    "èèèèèèèèèèèèèèèè§è¨è«è¬è­è¯è±è³è´èµè¶è¸è¹èºè¼è½èèèèèèèèè"
    "èèèèèèèèèèèè è¡è£è¤è¥è¦è§è¨è©èªè«è¬è­è®è¯è·è¸è»è¼è½èèèèèèèèè"
    "èèèè è¨è©èªè«è°è±è²è³è´è¶è·è¸è¹èºè¼è½è¿èèèèèèèèèèèèèèèèè è¡è¥"
    "è©èªè°è±è²è¹è¼è½èèèèèèèèèèèèèèè£è¤è¥è¦è§è¨è©è±è³è¸è¹è¼è½èèèèè"
    "èèèèè¡è£è©è«è¬è­è°è±è³è´èµè¶è¸èºèèèèèèèèèèèèèè¡è¨è¯è±è²è´è¸è¹èº"
    "è»è½è¿èèèèèèèèèèèèèèè è¢è£è¥è¦è¬è°è¼è¿èèèèèèèèèèè¡è«è¬è·"
    "è¸è¹èºè»è¼è½èèèèèèèèè¤è¨è°è²è´è¹èºè»è¾èèèèèèèè¢è¤è¨èªè®è¯è°è³è·è¸"
    "è¹è¿èèèèèèèèè è¤è¦è¨è©è»è¿èèèèè§è©è¸è¼èèèèèèèèèè¢è¤è«è¬è®è±"
    "è·è¸è¹èºè»è¼è½è¾è¿èèèèèèèèèèèèèè£è¤è§è¨è©èªè¬è¯è°è±è²è´è¶èºèèèè"
    "èèèèèèèèèèèèèèè¤è©è­è®è°è±è²è³è´è¸è¹è¾èèèèèèèèèèèèèè"
    "èèèèè¡è¢è£è¥è©è®è±è´è·è»è¾è¿èèèèèèèèèè è£è¤è¥è®è°è²è´è¶è»è¼è½è¾èè"
    "èèèèèèè è£è¨è«è¬è­è¯è±è³èµèºè½èèèèèèèèè è¥èªè«è®è¹è¾è è è è è è è ¡"
    "è ¢è ²è ¹è ¼è¡è¡è¡è¡è¡è¡è¡è¡è¡è¡è¡è¡ è¡¡è¡¢è¡£è¡¥è¡¨è¡©è¡«è¡¬è¡®è¡°è¡²è¡·è¡½è¡¾è¡¿è¢è¢è¢è¢è¢è¢è¢è¢è¢"
    "è¢è¢è¢è¢¢è¢¤è¢ªè¢«è¢­è¢¯è¢±è¢·è¢¼è£è£è£è£è£è£è£è£è£è£è£è£è£è£è£¢è££è£¤è£¥è£¨è£°è£±è£³è£´è£¸è£¹è£¼è£¾è¤"
    "è¤è¤è¤è¤è¤è¤è¤è¤è¤è¤¡è¤¥è¤ªè¤«è¤¯è¤°è¤´è¤¶è¥è¥è¥è¥è¥è¥è¥è¥¦è¥«è¥»è¥¿è¦è¦è¦è§è§è§è§è§è§è§è§è§"
    "è§è§è§è§è§è§è§è§è§è§è§è§è§è§£è§¥è§¦è§«è§­è§¯è§±è§³è§¿è¨è¨è¨è¨è¨¾è©è©è©¹èªèªèªè¬è­¦è­¬è®¡è®¢è®£è®¤"
    "è®¥è®¦è®§è®¨è®©è®ªè®«è®­è®®è®¯è®°è®±è®²è®³è®´è®µè®¶è®·è®¸è®¹è®ºè®»è®¼è®½è®¾è®¿è¯è¯è¯è¯è¯è¯è¯è¯è¯è¯è¯è¯è¯è¯"
    "è¯è¯è¯è¯è¯è¯è¯è¯è¯è¯è¯è¯è¯è¯è¯è¯è¯è¯è¯ è¯¡è¯¢è¯£è¯¤è¯¥è¯¦è¯§è¯¨è¯©è¯«è¯¬è¯­è¯®è¯¯è¯°è¯±è¯²è¯³è¯´è¯µè¯·"
    "è¯¸è¯¹è¯ºè¯»è¯¼è¯½è¯¾è¯¿è°è°è°è°è°è°è°è°è°è°è°è°è°è°è°è°è°è°è°è°è°è°è°è°è°è°è°è°è°è°è° è°¡"
    "è°¢è°£è°¤è°¥è°¦è°§è°¨è°©è°ªè°«è°¬è°­è°®è°¯è°°è°±è°²è°³è°´è°µè°¶è°·è°¼è°¿è±è±è±è±è±è±è±è±¡è±¢è±¨è±ªè±«è±®è±³è±¸è±¹"
    "è±ºè²è²è²è²è²è²è²è²è´è´è´è´¡è´¢è´£è´¤è´¥è´¦è´§è´¨è´©è´ªè´«è´¬è´­è´®è´¯è´°è´±è´²è´³è´´è´µè´¶è´·è´¸è´¹è´ºè´»è´¼"
    "è´½è´¾è´¿èµèµèµèµèµèµèµèµèµèµèµèµèµèµèµèµèµèµèµèµèµèµèµèµèµèµèµèµèµèµèµèµèµ èµ¡èµ¢èµ£èµ¤"
    "èµ¦èµ§èµªèµ«èµ­èµ°èµ³èµ´èµµèµ¶èµ·è¶è¶è¶è¶è¶è¶è¶è¶è¶£è¶¯è¶±è¶³è¶´è¶µè¶¸è¶ºè¶¼è¶¾è¶¿è·è·è·è·è·è·è·è·è·è·"
    "è·è·è·è·è·è·è·è·£è·¤è·¨è·ªè·¬è·¯è·±è·³è·µè·¶è··è·¸è·¹è·ºè·»è·½è¸è¸è¸è¸è¸è¸è¸è¸è¸è¸è¸¢è¸£è¸¦è¸©è¸ªè¸¬è¸®"
    "è¸¯è¸±è¸µè¸¶è¸¹è¸ºè¸½è¹è¹è¹è¹è¹è¹è¹è¹è¹è¹è¹è¹è¹è¹è¹è¹è¹¢è¹¦è¹©è¹¬è¹­è¹¯è¹°è¹²è¹´è¹¶è¹¼è¹½è¹¾è¹¿èºèºèº"
    "èºèºèºèºèºèº«èº¬èº¯èº²èººè½¦è½§è½¨è½©è½ªè½«è½¬è½­è½®è½¯è½°è½±è½²è½³è½´è½µè½¶è½·è½¸è½¹è½ºè½»è½¼è½½è½¾è½¿è¾è¾è¾è¾"
    "è¾è¾è¾è¾è¾è¾è¾è¾è¾è¾è¾è¾è¾è¾è¾è¾è¾è¾è¾è¾è¾è¾è¾è¾è¾è¾è¾è¾£è¾¨è¾©è¾«è¾°è¾±è¾¹è¾½è¾¾è¾¿è¿è¿è¿"
    "è¿è¿è¿è¿è¿è¿è¿è¿è¿è¿è¿è¿è¿è¿è¿è¿è¿¢è¿¤è¿¥è¿¦è¿¨è¿©è¿ªè¿«è¿­è¿®è¿°è¿³è¿·è¿¸è¿¹è¿ºè¿½ééééééé"
    "ééééééééééééééééé é¡é¢é¦é­é®é¯é´éµé¶é¸é»é¼é¾éééééééééé"
    "ééééé¢é£é¥é¨é­é®é´éµé¹é½é¿éééééééééééééé é¡é¢é£é¦é¨éªé¬é®é¯é°é±"
    "é²é³é´éµé¶é¸é¹éºé»é½é¾é¿éééééééééééééééééé¡é¢é¤é¦é§é¨éªé«é­é¯é´"
    "é¸é½é¾é¿éééééééééééééé é¢é£é«é¯é±é¹éééééééééééééééé"
    "éé¡é¢é£é¤é¥é¦é©éªé¬é®é¯é°é±é²é´éµé¶é·é¸é¹éºé½é¾é¿éééééééééééé¢é¨éªé­"
    "é®é¯é´éµéºé¾ééééééééééé´éé®ééé¾éªéééé¾é«éééééééééééé"
    "éééééééééééééé é¡é¢é£é¤é¥é¦é§é¨é©éªé«é¬é­é®é¯é°é±é²é³é´éµé·é¹éºé»é¼"
    "é½é¾é¿éééééééééééééééééééééééééééééé é¡é¢é£é¤é¥é§é¨"
    "é©éªé«é¬é­é®é¯é°é±é²é³é´éµé¶é·é¸é¹éºé»é¼é½é¾é¿ééééééééééééééééé"
    "éééééééééééééééé¡é¢é£é¤é¥é¦é§é¨é©éªé«é¬é­é®é¯é°é±é²é³é´éµé¶é·é¸é¹"
    "éºé»é¼é½é¾é¿ééééééééééééééééééééééééééééééé é¡é¢é£"
    "é¤é¥é¦é§é¨é©éªé«é¬é­é®é¯é°é±é²é³é´éµé¶é¿é¨é©éªé«é­é®é¯é°é±é²é³é´éµé¶é·é¸é¹éºé»é¼"
    "é½é¾é¿ééééééééééééééééééééééééééééé¡éªé®é±é²é³é´éµé¶"
    "é»é¼é½é¿éééééééééééééééééééé¡é¢é¤é§é¨é©éªé¬é²é´éµé¶é·éééé"
    "éééééééééé§é©é°é³é¶é¹éºé¼é½é¾ééééééééééééééé é¨é©éªé¯é±é³"
    "é¶é·é¹é¾éééééééééééééééé¨éªé­é°é²é¸é¹é¾ééééééé é¡é¢é¥é©é¬é°"
    "é³é´é¶é¸éºé¼é½é¿ééééééééé é¡é£é§é¨é«é¬é­é®é¯é²é³é´éé¦é§é¨é©éªé«é¬é­é³éµ"
    "é¶é¡µé¡¶é¡·é¡¸é¡¹é¡ºé¡»é¡¼é¡½é¡¾é¡¿é¢é¢é¢é¢é¢é¢é¢é¢é¢é¢é¢é¢é¢é¢é¢é¢é¢é¢é¢é¢é¢é¢é¢é¢é¢é¢é¢é¢"
    "é¢é¢é¢ é¢¡é¢¢é¢¤é¢¥é¢¦é¢§é£é£é£é£é£é£é£é£é£é£é£é£é£é£§é£¨é¤é¤é¤®é¥é¥é¥¥é¥§é¥¨é¥©é¥ªé¥«é¥¬é¥­é¥®é¥¯é¥°"
    "é¥±é¥²é¥³é¥´é¥µé¥¶é¥·é¥¸é¥¹é¥ºé¥»é¥¼é¥½é¥¿é¦é¦é¦é¦é¦é¦é¦é¦é¦é¦é¦é¦é¦é¦é¦é¦é¦é¦é¦é¦é¦é¦é¦é¦é¦é¦¥"
    "é¦§é¦¨é©¬é©­é©®é©¯é©°é©±é©²é©³é©´é©µé©¶é©·é©¸é©¹é©ºé©»é©¼é©½é©¾é©¿éªéªéªéªéªéªéªéªéªéªéªéªéªéªéªéªéªéª"
    "éªéªéªéªéªéªéªéªéªéªéªéªéªéª éª¡éª¢éª£éª¤éª¥éª¦éª§éª¨éª°éª±éª¶éª·éª¸éªºéª¼é«é«é«é«é«é«é«é«é«é«é«"
    "é«¡é«¢é«¦é««é«­é«¯é«¹é«»é«½é¬é¬é¬é¬é¬é¬é¬é¬£é¬¯é¬²é¬¶é¬·é¬»é¬¼é­é­é­é­é­é­é­é­é­é­é­é­é­é­é±¼é±½é±¾"
    "é±¿é²é²é²é²é²é²é²é²é²é²é²é²é²é²é²é²é²é²é²é²é²é²é²é²é²é²é²é²é²é²é² é²¡é²¢é²£é²¤é²¥é²¦é²§é²¨"
    "é²©é²ªé²«é²¬é²­é²®é²¯é²°é²±é²²é²³é²´é²µé²·é²¸é²¹é²ºé²»é²¼é²½é²¾é²¿é³é³é³é³é³é³é³é³é³é³é³é³é³é³é³é³é³é³"
    "é³é³é³é³é³é³é³é³é³é³é³é³é³ é³¡é³¢é³£é³¤é¸é¸ é¸¡é¸¢é¸£é¸¤é¸¥é¸¦é¸§é¸¨é¸©é¸ªé¸«é¸¬é¸­é¸®é¸¯é¸°é¸±é¸²é¸³é¸µé¸¶"
    "é¸·é¸¸é¸¹é¸ºé¸»é¸¼é¸½é¸¾é¸¿é¹é¹é¹é¹é¹é¹é¹é¹é¹é¹é¹é¹é¹é¹é¹é¹é¹é¹é¹é¹é¹é¹é¹é¹é¹é¹é¹é¹é¹é¹é¹"
    "é¹ é¹¡é¹¢é¹£é¹¤é¹¦é¹§é¹¨é¹©é¹ªé¹«é¹¬é¹­é¹®é¹¯é¹°é¹±é¹²é¹³é¹´é¹¾é¹¿éºéºéºéºéºéºéºéºéºéºéºéº¦éº¸éº¹éº»éº½éº¾é»"
    "é»é»é»é»é»é»é»é»é»é»é»é»é» é»¡é»¢é»¥é»§é»©é»ªé»¯é»¹é»»é»¼é»¾é¼é¼é¼é¼é¼é¼é¼é¼é¼ é¼¢é¼©é¼«é¼¬é¼¯é¼±é¼·"
    "é¼¹é¼»é¼½é¼¾é½é½é½é½é½é½¿é¾é¾é¾é¾é¾é¾é¾é¾é¾é¾é¾é¾é¾é¾é¾é¾é¾é¾ é¾¢é¿é¿é¿ããã®ãããã¦ã"
    "ãã¹ãã ã ã¤ã¥ã§ã§ã§ã«°ã¬ã¬ã¬ã­ã­ã®¾ã°ã³ã³ã³ã´ãµã¶²ã¸ã¸ãºã»¬ã½ã¿ ää®ääää¹ääää¡"
    "ä²ääää¨ä«ä¬äääªä´ä£ää¢ºä¢¼ä£ä¥½ä¦ä²ä² ä²¢ä´ä´ä´ä´ä´ä´ä´ä¶®ð ¤ð ¶ð ³ð¡ð¡ð£ð£²ð£²ð£¸£ð¤§ð¤©½"
    "ð¤«ð¥²ð¥¢ð¥¨ð¥»ð¦¡ð¦ð¦¶ð¦¼ð¦­ð¦°¡ð§¿¹ð¨ð¨¸ð¨ð¨ ð¨­ð¨±ð¨±ð¨±ð¨±ð¨ºð©½¾ð©¾ð©¾ðªðª£»ðª¤ðª¨°ðª¨¶ðª©ðª¾¢ð«§ð«¨ð«·ð«¸ð«­ð«ð«£ð«¯"
    "ð«²ð«½ð«ð«ð«ð«¡ð«§ð«¯ð«¶ð«¹ð«ð«ð«¶ð«®ð«¯ð«³ð«§ð«´ð«ð«ð«¦ð«§ð«¨ð«ªð«¬ð«ð«ð«­ð«­ð«©ð«ð«¦ð«¹ð«¼ð« ð« ð« ð«¢¸ð««ð«­"
    "ð«­¢ð«­¼ð«®ð«°ð«µ·ð«¶ð«··ð«¸©ð¬©ð¬ªð¬©ð¬ð¬ð¬ð¬¹ð¬¼ð¬ð¬¤ð¬ð¬ð¬¡ð¬¤ð¬ð¬ð¬ð¬ð¬ð¬ð¬¡ð¬©ð¬«ð¬¬ð¬­ð¬¯ð¬ð¬ð¬ð¬¬ð¬¯ð¬"
    "ð¬ð¬½ð¬£ð¬£ð¬£¡ð¬£³ð¬¤ð¬¤ð¬¤ð¬¨ð¬¨ð¬©½ð¬ª©ð¬¬©ð¬¬­ð¬¬®ð¬¬±ð¬¬¸ð¬¬¹ð¬¬»ð¬¬¿ð¬­ð¬­ð¬­ð¬­ð¬­ð¬­¤ð¬­©ð¬­¬ð¬­¯ð¬­³ð¬­¶ð¬­¸ð¬­¼ð¬®±ð¬®¿ð¬¯ð¬¯ð¬±ð¬±"
    "ð¬³µð¬³¶ð¬³½ð¬³¿ð¬´ð¬´ð¬´ð¬¶ð¬¶ð¬¶ð¬¶ð¬¶ð¬¶ ð¬¶¨ð¬¶­ð¬¶®ð¬·ð¬¸ð¬¸ð¬¸£ð¬¸¦ð¬¸ªð¬¹¼ð¬ºð¬º"
)
CN_CHARS_EXT = "å¶è¯¶å±å§é£å±"

CN_CHARS = CN_CHARS_COMMON + CN_CHARS_EXT
IN_CH_CHARS = {c: True for c in CN_CHARS}

EN_CHARS = string.ascii_letters + string.digits
IN_EN_CHARS = {c: True for c in EN_CHARS}

VALID_CHARS = CN_CHARS + EN_CHARS + " "
IN_VALID_CHARS = {c: True for c in VALID_CHARS}


# ================================================================================ #
#                                    basic class
# ================================================================================ #
class ChineseChar(object):
    """
    ä¸­æå­ç¬¦
    æ¯ä¸ªå­ç¬¦å¯¹åºç®ä½åç¹ä½,
    e.g. ç®ä½ = 'è´', ç¹ä½ = 'è² '
    è½¬æ¢æ¶å¯è½¬æ¢ä¸ºç®ä½æç¹ä½
    """

    def __init__(self, simplified, traditional):
        self.simplified = simplified
        self.traditional = traditional
        # self.__repr__ = self.__str__

    def __str__(self):
        return self.simplified or self.traditional or None

    def __repr__(self):
        return self.__str__()


class ChineseNumberUnit(ChineseChar):
    """
    ä¸­ææ°å­/æ°ä½å­ç¬¦
    æ¯ä¸ªå­ç¬¦é¤ç¹ç®ä½å¤è¿æä¸ä¸ªé¢å¤çå¤§åå­ç¬¦
    e.g. 'é' å 'é¸'
    """

    def __init__(self, power, simplified, traditional, big_s, big_t):
        super(ChineseNumberUnit, self).__init__(simplified, traditional)
        self.power = power
        self.big_s = big_s
        self.big_t = big_t

    def __str__(self):
        return "10^{}".format(self.power)

    @classmethod
    def create(cls, index, value, numbering_type=NUMBERING_TYPES[1], small_unit=False):
        if small_unit:
            return ChineseNumberUnit(
                power=index + 1, simplified=value[0], traditional=value[1], big_s=value[1], big_t=value[1]
            )
        elif numbering_type == NUMBERING_TYPES[0]:
            return ChineseNumberUnit(
                power=index + 8, simplified=value[0], traditional=value[1], big_s=value[0], big_t=value[1]
            )
        elif numbering_type == NUMBERING_TYPES[1]:
            return ChineseNumberUnit(
                power=(index + 2) * 4, simplified=value[0], traditional=value[1], big_s=value[0], big_t=value[1]
            )
        elif numbering_type == NUMBERING_TYPES[2]:
            return ChineseNumberUnit(
                power=pow(2, index + 3), simplified=value[0], traditional=value[1], big_s=value[0], big_t=value[1]
            )
        else:
            raise ValueError("Counting type should be in {0} ({1} provided).".format(NUMBERING_TYPES, numbering_type))


class ChineseNumberDigit(ChineseChar):
    """
    ä¸­ææ°å­å­ç¬¦
    """

    def __init__(self, value, simplified, traditional, big_s, big_t, alt_s=None, alt_t=None):
        super(ChineseNumberDigit, self).__init__(simplified, traditional)
        self.value = value
        self.big_s = big_s
        self.big_t = big_t
        self.alt_s = alt_s
        self.alt_t = alt_t

    def __str__(self):
        return str(self.value)

    @classmethod
    def create(cls, i, v):
        return ChineseNumberDigit(i, v[0], v[1], v[2], v[3])


class ChineseMath(ChineseChar):
    """
    ä¸­ææ°ä½å­ç¬¦
    """

    def __init__(self, simplified, traditional, symbol, expression=None):
        super(ChineseMath, self).__init__(simplified, traditional)
        self.symbol = symbol
        self.expression = expression
        self.big_s = simplified
        self.big_t = traditional


CC, CNU, CND, CM = ChineseChar, ChineseNumberUnit, ChineseNumberDigit, ChineseMath


class NumberSystem(object):
    """
    ä¸­ææ°å­ç³»ç»
    """

    pass


class MathSymbol(object):
    """
    ç¨äºä¸­ææ°å­ç³»ç»çæ°å­¦ç¬¦å· (ç¹/ç®ä½), e.g.
    positive = ['æ­£', 'æ­£']
    negative = ['è´', 'è² ']
    point = ['ç¹', 'é»']
    """

    def __init__(self, positive, negative, point):
        self.positive = positive
        self.negative = negative
        self.point = point

    def __iter__(self):
        for v in self.__dict__.values():
            yield v


# class OtherSymbol(object):
#     """
#     å¶ä»ç¬¦å·
#     """
#
#     def __init__(self, sil):
#         self.sil = sil
#
#     def __iter__(self):
#         for v in self.__dict__.values():
#             yield v


# ================================================================================ #
#                                    basic utils
# ================================================================================ #
def create_system(numbering_type=NUMBERING_TYPES[1]):
    """
    æ ¹æ®æ°å­ç³»ç»ç±»åè¿ååå»ºç¸åºçæ°å­ç³»ç»ï¼é»è®¤ä¸º mid
    NUMBERING_TYPES = ['low', 'mid', 'high']: ä¸­ææ°å­ç³»ç»ç±»å
        low:  'å' = 'äº¿' * 'å' = $10^{9}$,  'äº¬' = 'å' * 'å', etc.
        mid:  'å' = 'äº¿' * 'ä¸' = $10^{12}$, 'äº¬' = 'å' * 'ä¸', etc.
        high: 'å' = 'äº¿' * 'äº¿' = $10^{16}$, 'äº¬' = 'å' * 'å', etc.
    è¿åå¯¹åºçæ°å­ç³»ç»
    """

    # chinese number units of 'äº¿' and larger
    all_larger_units = zip(LARGER_CHINESE_NUMERING_UNITS_SIMPLIFIED, LARGER_CHINESE_NUMERING_UNITS_TRADITIONAL)
    larger_units = [CNU.create(i, v, numbering_type, False) for i, v in enumerate(all_larger_units)]
    # chinese number units of 'å, ç¾, å, ä¸'
    all_smaller_units = zip(SMALLER_CHINESE_NUMERING_UNITS_SIMPLIFIED, SMALLER_CHINESE_NUMERING_UNITS_TRADITIONAL)
    smaller_units = [CNU.create(i, v, small_unit=True) for i, v in enumerate(all_smaller_units)]
    # digis
    chinese_digis = zip(CHINESE_DIGIS, CHINESE_DIGIS, BIG_CHINESE_DIGIS_SIMPLIFIED, BIG_CHINESE_DIGIS_TRADITIONAL)
    digits = [CND.create(i, v) for i, v in enumerate(chinese_digis)]
    digits[0].alt_s, digits[0].alt_t = ZERO_ALT, ZERO_ALT
    digits[1].alt_s, digits[1].alt_t = ONE_ALT, ONE_ALT
    digits[2].alt_s, digits[2].alt_t = TWO_ALTS[0], TWO_ALTS[1]

    # symbols
    positive_cn = CM(POSITIVE[0], POSITIVE[1], "+", lambda x: x)
    negative_cn = CM(NEGATIVE[0], NEGATIVE[1], "-", lambda x: -x)
    point_cn = CM(POINT[0], POINT[1], ".", lambda x, y: float(str(x) + "." + str(y)))
    # sil_cn = CM(SIL[0], SIL[1], '-', lambda x, y: float(str(x) + '-' + str(y)))
    system = NumberSystem()
    system.units = smaller_units + larger_units
    system.digits = digits
    system.math = MathSymbol(positive_cn, negative_cn, point_cn)
    # system.symbols = OtherSymbol(sil_cn)
    return system


def chn2num(chinese_string, numbering_type=NUMBERING_TYPES[1]):
    def get_symbol(char, system):
        for u in system.units:
            if char in [u.traditional, u.simplified, u.big_s, u.big_t]:
                return u
        for d in system.digits:
            if char in [d.traditional, d.simplified, d.big_s, d.big_t, d.alt_s, d.alt_t]:
                return d
        for m in system.math:
            if char in [m.traditional, m.simplified]:
                return m

    def string2symbols(chinese_string, system):
        int_string, dec_string = chinese_string, ""
        for p in [system.math.point.simplified, system.math.point.traditional]:
            if p in chinese_string:
                int_string, dec_string = chinese_string.split(p)
                break
        return [get_symbol(c, system) for c in int_string], [get_symbol(c, system) for c in dec_string]

    def correct_symbols(integer_symbols, system):
        """
        ä¸ç¾å« to ä¸ç¾å«å
        ä¸äº¿ä¸åä¸ç¾ä¸ to ä¸äº¿ ä¸åä¸ ä¸ç¾ä¸
        """

        if integer_symbols and isinstance(integer_symbols[0], CNU):
            if integer_symbols[0].power == 1:
                integer_symbols = [system.digits[1]] + integer_symbols

        if len(integer_symbols) > 1:
            if isinstance(integer_symbols[-1], CND) and isinstance(integer_symbols[-2], CNU):
                integer_symbols.append(CNU(integer_symbols[-2].power - 1, None, None, None, None))

        result = []
        unit_count = 0
        for s in integer_symbols:
            if isinstance(s, CND):
                result.append(s)
                unit_count = 0
            elif isinstance(s, CNU):
                current_unit = CNU(s.power, None, None, None, None)
                unit_count += 1

            if unit_count == 1:
                result.append(current_unit)
            elif unit_count > 1:
                for i in range(len(result)):
                    if isinstance(result[-i - 1], CNU) and result[-i - 1].power < current_unit.power:
                        result[-i - 1] = CNU(result[-i - 1].power + current_unit.power, None, None, None, None)
        return result

    def compute_value(integer_symbols):
        """
        Compute the value.
        When current unit is larger than previous unit, current unit * all previous units will be used as all previous units.
        e.g. 'ä¸¤åä¸' = 2000 * 10000 not 2000 + 10000
        """
        value = [0]
        last_power = 0
        for s in integer_symbols:
            if isinstance(s, CND):
                value[-1] = s.value
            elif isinstance(s, CNU):
                value[-1] *= pow(10, s.power)
                if s.power > last_power:
                    value[:-1] = list(map(lambda v: v * pow(10, s.power), value[:-1]))
                    last_power = s.power
                value.append(0)
        return sum(value)

    system = create_system(numbering_type)
    int_part, dec_part = string2symbols(chinese_string, system)
    int_part = correct_symbols(int_part, system)
    int_str = str(compute_value(int_part))
    dec_str = "".join([str(d.value) for d in dec_part])
    if dec_part:
        return "{0}.{1}".format(int_str, dec_str)
    else:
        return int_str


def num2chn(
    number_string,
    numbering_type=NUMBERING_TYPES[1],
    big=False,
    traditional=False,
    alt_zero=False,
    alt_one=False,
    alt_two=True,
    use_zeros=True,
    use_units=True,
):
    def get_value(value_string, use_zeros=True):
        striped_string = value_string.lstrip("0")

        # record nothing if all zeros
        if not striped_string:
            return []

        # record one digits
        elif len(striped_string) == 1:
            if use_zeros and len(value_string) != len(striped_string):
                return [system.digits[0], system.digits[int(striped_string)]]
            else:
                return [system.digits[int(striped_string)]]

        # recursively record multiple digits
        else:
            result_unit = next(u for u in reversed(system.units) if u.power < len(striped_string))
            result_string = value_string[: -result_unit.power]
            return get_value(result_string) + [result_unit] + get_value(striped_string[-result_unit.power :])

    system = create_system(numbering_type)

    int_dec = number_string.split(".")
    if len(int_dec) == 1:
        int_string = int_dec[0]
        dec_string = ""
    elif len(int_dec) == 2:
        int_string = int_dec[0]
        dec_string = int_dec[1]
    else:
        raise ValueError("invalid input num string with more than one dot: {}".format(number_string))

    if use_units and len(int_string) > 1:
        result_symbols = get_value(int_string)
    else:
        result_symbols = [system.digits[int(c)] for c in int_string]
    dec_symbols = [system.digits[int(c)] for c in dec_string]
    if dec_string:
        result_symbols += [system.math.point] + dec_symbols

    if alt_two:
        liang = CND(2, system.digits[2].alt_s, system.digits[2].alt_t, system.digits[2].big_s, system.digits[2].big_t)
        for i, v in enumerate(result_symbols):
            if isinstance(v, CND) and v.value == 2:
                next_symbol = result_symbols[i + 1] if i < len(result_symbols) - 1 else None
                previous_symbol = result_symbols[i - 1] if i > 0 else None
                if isinstance(next_symbol, CNU) and isinstance(previous_symbol, (CNU, type(None))):
                    if next_symbol.power != 1 and ((previous_symbol is None) or (previous_symbol.power != 1)):
                        result_symbols[i] = liang

    # if big is True, 'ä¸¤' will not be used and `alt_two` has no impact on output
    if big:
        attr_name = "big_"
        if traditional:
            attr_name += "t"
        else:
            attr_name += "s"
    else:
        if traditional:
            attr_name = "traditional"
        else:
            attr_name = "simplified"

    result = "".join([getattr(s, attr_name) for s in result_symbols])

    # if not use_zeros:
    #     result = result.strip(getattr(system.digits[0], attr_name))

    if alt_zero:
        result = result.replace(getattr(system.digits[0], attr_name), system.digits[0].alt_s)

    if alt_one:
        result = result.replace(getattr(system.digits[1], attr_name), system.digits[1].alt_s)

    for i, p in enumerate(POINT):
        if result.startswith(p):
            return CHINESE_DIGIS[0] + result

    # ^10, 11, .., 19
    if (
        len(result) >= 2
        and result[1] in [SMALLER_CHINESE_NUMERING_UNITS_SIMPLIFIED[0], SMALLER_CHINESE_NUMERING_UNITS_TRADITIONAL[0]]
        and result[0] in [CHINESE_DIGIS[1], BIG_CHINESE_DIGIS_SIMPLIFIED[1], BIG_CHINESE_DIGIS_TRADITIONAL[1]]
    ):
        result = result[1:]

    return result


# ================================================================================ #
#                          different types of rewriters
# ================================================================================ #
class Cardinal:
    """
    CARDINALç±»
    """

    def __init__(self, cardinal=None, chntext=None):
        self.cardinal = cardinal
        self.chntext = chntext

    def chntext2cardinal(self):
        return chn2num(self.chntext)

    def cardinal2chntext(self):
        return num2chn(self.cardinal)


class Digit:
    """
    DIGITç±»
    """

    def __init__(self, digit=None, chntext=None):
        self.digit = digit
        self.chntext = chntext

    # def chntext2digit(self):
    #     return chn2num(self.chntext)

    def digit2chntext(self):
        return num2chn(self.digit, alt_two=False, use_units=False)


class TelePhone:
    """
    TELEPHONEç±»
    """

    def __init__(self, telephone=None, raw_chntext=None, chntext=None):
        self.telephone = telephone
        self.raw_chntext = raw_chntext
        self.chntext = chntext

    # def chntext2telephone(self):
    #     sil_parts = self.raw_chntext.split('<SIL>')
    #     self.telephone = '-'.join([
    #         str(chn2num(p)) for p in sil_parts
    #     ])
    #     return self.telephone

    def telephone2chntext(self, fixed=False):
        if fixed:
            sil_parts = self.telephone.split("-")
            self.raw_chntext = "<SIL>".join([num2chn(part, alt_two=False, use_units=False) for part in sil_parts])
            self.chntext = self.raw_chntext.replace("<SIL>", "")
        else:
            sp_parts = self.telephone.strip("+").split()
            self.raw_chntext = "<SP>".join([num2chn(part, alt_two=False, use_units=False) for part in sp_parts])
            self.chntext = self.raw_chntext.replace("<SP>", "")
        return self.chntext


class Fraction:
    """
    FRACTIONç±»
    """

    def __init__(self, fraction=None, chntext=None):
        self.fraction = fraction
        self.chntext = chntext

    def chntext2fraction(self):
        denominator, numerator = self.chntext.split("åä¹")
        return chn2num(numerator) + "/" + chn2num(denominator)

    def fraction2chntext(self):
        numerator, denominator = self.fraction.split("/")
        return num2chn(denominator) + "åä¹" + num2chn(numerator)


class Date:
    """
    DATEç±»
    """

    def __init__(self, date=None, chntext=None):
        self.date = date
        self.chntext = chntext

    # def chntext2date(self):
    #     chntext = self.chntext
    #     try:
    #         year, other = chntext.strip().split('å¹´', maxsplit=1)
    #         year = Digit(chntext=year).digit2chntext() + 'å¹´'
    #     except ValueError:
    #         other = chntext
    #         year = ''
    #     if other:
    #         try:
    #             month, day = other.strip().split('æ', maxsplit=1)
    #             month = Cardinal(chntext=month).chntext2cardinal() + 'æ'
    #         except ValueError:
    #             day = chntext
    #             month = ''
    #         if day:
    #             day = Cardinal(chntext=day[:-1]).chntext2cardinal() + day[-1]
    #     else:
    #         month = ''
    #         day = ''
    #     date = year + month + day
    #     self.date = date
    #     return self.date

    def date2chntext(self):
        date = self.date
        try:
            year, other = date.strip().split("å¹´", 1)
            year = Digit(digit=year).digit2chntext() + "å¹´"
        except ValueError:
            other = date
            year = ""
        if other:
            try:
                month, day = other.strip().split("æ", 1)
                month = Cardinal(cardinal=month).cardinal2chntext() + "æ"
            except ValueError:
                day = date
                month = ""
            if day:
                day = Cardinal(cardinal=day[:-1]).cardinal2chntext() + day[-1]
        else:
            month = ""
            day = ""
        chntext = year + month + day
        self.chntext = chntext
        return self.chntext


class Money:
    """
    MONEYç±»
    """

    def __init__(self, money=None, chntext=None):
        self.money = money
        self.chntext = chntext

    # def chntext2money(self):
    #     return self.money

    def money2chntext(self):
        money = self.money
        pattern = re.compile(r"(\d+(\.\d+)?)")
        matchers = pattern.findall(money)
        if matchers:
            for matcher in matchers:
                money = money.replace(matcher[0], Cardinal(cardinal=matcher[0]).cardinal2chntext())
        self.chntext = money
        return self.chntext


class Percentage:
    """
    PERCENTAGEç±»
    """

    def __init__(self, percentage=None, chntext=None):
        self.percentage = percentage
        self.chntext = chntext

    def chntext2percentage(self):
        return chn2num(self.chntext.strip().strip("ç¾åä¹")) + "%"

    def percentage2chntext(self):
        return "ç¾åä¹" + num2chn(self.percentage.strip().strip("%"))


def normalize_nsw(raw_text):
    text = "^" + raw_text + "$"

    # è§èåæ¥æ
    pattern = re.compile(r"\D+((([089]\d|(19|20)\d{2})å¹´)?(\d{1,2}æ(\d{1,2}[æ¥å·])?)?)")
    matchers = pattern.findall(text)
    if matchers:
        # print('date')
        for matcher in matchers:
            text = text.replace(matcher[0], Date(date=matcher[0]).date2chntext(), 1)

    # è§èåéé±
    pattern = re.compile(r"\D+((\d+(\.\d+)?)[å¤ä½å ]?" + CURRENCY_UNITS + r"(\d" + CURRENCY_UNITS + r"?)?)")
    matchers = pattern.findall(text)
    if matchers:
        # print('money')
        for matcher in matchers:
            text = text.replace(matcher[0], Money(money=matcher[0]).money2chntext(), 1)

    # è§èååºè¯/ææºå·ç 
    # ææº
    # http://www.jihaoba.com/news/show/13680
    # ç§»å¨ï¼139ã138ã137ã136ã135ã134ã159ã158ã157ã150ã151ã152ã188ã187ã182ã183ã184ã178ã198
    # èéï¼130ã131ã132ã156ã155ã186ã185ã176
    # çµä¿¡ï¼133ã153ã189ã180ã181ã177
    pattern = re.compile(r"\D((\+?86 ?)?1([38]\d|5[0-35-9]|7[678]|9[89])\d{8})\D")
    matchers = pattern.findall(text)
    if matchers:
        # print('telephone')
        for matcher in matchers:
            text = text.replace(matcher[0], TelePhone(telephone=matcher[0]).telephone2chntext(), 1)
    # åºè¯
    pattern = re.compile(r"\D((0(10|2[1-3]|[3-9]\d{2})-?)?[1-9]\d{6,7})\D")
    matchers = pattern.findall(text)
    if matchers:
        # print('fixed telephone')
        for matcher in matchers:
            text = text.replace(matcher[0], TelePhone(telephone=matcher[0]).telephone2chntext(fixed=True), 1)

    # è§èååæ°
    pattern = re.compile(r"(\d+/\d+)")
    matchers = pattern.findall(text)
    if matchers:
        # print('fraction')
        for matcher in matchers:
            text = text.replace(matcher, Fraction(fraction=matcher).fraction2chntext(), 1)

    # è§èåç¾åæ°
    text = text.replace("ï¼", "%")
    pattern = re.compile(r"(\d+(\.\d+)?%)")
    matchers = pattern.findall(text)
    if matchers:
        # print('percentage')
        for matcher in matchers:
            text = text.replace(matcher[0], Percentage(percentage=matcher[0]).percentage2chntext(), 1)

    # è§èåçº¯æ°+éè¯
    pattern = re.compile(r"(\d+(\.\d+)?)[å¤ä½å ]?" + COM_QUANTIFIERS)
    matchers = pattern.findall(text)
    if matchers:
        # print('cardinal+quantifier')
        for matcher in matchers:
            text = text.replace(matcher[0], Cardinal(cardinal=matcher[0]).cardinal2chntext(), 1)

    # è§èåæ°å­ç¼å·
    pattern = re.compile(r"(\d{4,32})")
    matchers = pattern.findall(text)
    if matchers:
        # print('digit')
        for matcher in matchers:
            text = text.replace(matcher, Digit(digit=matcher).digit2chntext(), 1)

    # è§èåçº¯æ°
    pattern = re.compile(r"(\d+(\.\d+)?)")
    matchers = pattern.findall(text)
    if matchers:
        # print('cardinal')
        for matcher in matchers:
            text = text.replace(matcher[0], Cardinal(cardinal=matcher[0]).cardinal2chntext(), 1)

    # restore P2P, O2O, B2C, B2B etc
    pattern = re.compile(r"(([a-zA-Z]+)äº([a-zA-Z]+))")
    matchers = pattern.findall(text)
    if matchers:
        # print('particular')
        for matcher in matchers:
            text = text.replace(matcher[0], matcher[1] + "2" + matcher[2], 1)

    return text.lstrip("^").rstrip("$")


def remove_erhua(text):
    """
    å»é¤å¿åé³è¯ä¸­çå¿:
    ä»å¥³å¿å¨é£è¾¹å¿ -> ä»å¥³å¿å¨é£è¾¹
    """

    new_str = ""
    while re.search("å¿", text):
        a = re.search("å¿", text).span()
        remove_er_flag = 0

        if ER_WHITELIST_PATTERN.search(text):
            b = ER_WHITELIST_PATTERN.search(text).span()
            if b[0] <= a[0]:
                remove_er_flag = 1

        if remove_er_flag == 0:
            new_str = new_str + text[0 : a[0]]
            text = text[a[1] :]
        else:
            new_str = new_str + text[0 : b[1]]
            text = text[b[1] :]

    text = new_str + text
    return text


def remove_space(text):
    tokens = text.split()
    new = []
    for k, t in enumerate(tokens):
        if k != 0:
            if IN_EN_CHARS.get(tokens[k - 1][-1]) and IN_EN_CHARS.get(t[0]):
                new.append(" ")
        new.append(t)
    return "".join(new)


class TextNorm:
    def __init__(
        self,
        to_banjiao: bool = False,
        to_upper: bool = False,
        to_lower: bool = False,
        remove_fillers: bool = False,
        remove_erhua: bool = False,
        check_chars: bool = False,
        remove_space: bool = False,
        cc_mode: str = "",
    ):
        self.to_banjiao = to_banjiao
        self.to_upper = to_upper
        self.to_lower = to_lower
        self.remove_fillers = remove_fillers
        self.remove_erhua = remove_erhua
        self.check_chars = check_chars
        self.remove_space = remove_space

        self.cc = None
        if cc_mode:
            from opencc import OpenCC  # Open Chinese Convert: pip install opencc

            self.cc = OpenCC(cc_mode)

    def __call__(self, text):
        if self.cc:
            text = self.cc.convert(text)

        if self.to_banjiao:
            text = text.translate(QJ2BJ_TRANSFORM)

        if self.to_upper:
            text = text.upper()

        if self.to_lower:
            text = text.lower()

        if self.remove_fillers:
            for c in FILLER_CHARS:
                text = text.replace(c, "")

        if self.remove_erhua:
            text = remove_erhua(text)

        text = normalize_nsw(text)

        text = text.translate(PUNCS_TRANSFORM)

        if self.check_chars:
            for c in text:
                if not IN_VALID_CHARS.get(c):
                    print(f"WARNING: illegal char {c} in: {text}", file=sys.stderr)
                    return ""

        if self.remove_space:
            text = remove_space(text)

        return text


if __name__ == "__main__":
    p = argparse.ArgumentParser()

    # normalizer options
    p.add_argument("--to_banjiao", action="store_true", help="convert quanjiao chars to banjiao")
    p.add_argument("--to_upper", action="store_true", help="convert to upper case")
    p.add_argument("--to_lower", action="store_true", help="convert to lower case")
    p.add_argument("--remove_fillers", action="store_true", help='remove filler chars such as "å, å"')
    p.add_argument("--remove_erhua", action="store_true", help='remove erhua chars such as "ä»å¥³å¿å¨é£è¾¹å¿ -> ä»å¥³å¿å¨é£è¾¹"')
    p.add_argument("--check_chars", action="store_true", help="skip sentences containing illegal chars")
    p.add_argument("--remove_space", action="store_true", help="remove whitespace")
    p.add_argument(
        "--cc_mode", choices=["", "t2s", "s2t"], default="", help="convert between traditional to simplified"
    )

    # I/O options
    p.add_argument("--log_interval", type=int, default=10000, help="log interval in number of processed lines")
    p.add_argument("--has_key", action="store_true", help="will be deprecated, set --format ark instead")
    p.add_argument("--format", type=str, choices=["txt", "ark", "tsv"], default="txt", help="input format")
    p.add_argument("ifile", help="input filename, assume utf-8 encoding")
    p.add_argument("ofile", help="output filename")

    args = p.parse_args()

    if args.has_key:
        args.format = "ark"

    normalizer = TextNorm(
        to_banjiao=args.to_banjiao,
        to_upper=args.to_upper,
        to_lower=args.to_lower,
        remove_fillers=args.remove_fillers,
        remove_erhua=args.remove_erhua,
        check_chars=args.check_chars,
        remove_space=args.remove_space,
        cc_mode=args.cc_mode,
    )

    normalizer = TextNorm(
        to_banjiao=args.to_banjiao,
        to_upper=args.to_upper,
        to_lower=args.to_lower,
        remove_fillers=args.remove_fillers,
        remove_erhua=args.remove_erhua,
        check_chars=args.check_chars,
        remove_space=args.remove_space,
        cc_mode=args.cc_mode,
    )

    ndone = 0
    with open(args.ifile, "r", encoding="utf8") as istream, open(args.ofile, "w+", encoding="utf8") as ostream:
        if args.format == "tsv":
            reader = csv.DictReader(istream, delimiter="\t")
            assert "TEXT" in reader.fieldnames
            print("\t".join(reader.fieldnames), file=ostream)

            for item in reader:
                text = item["TEXT"]

                if text:
                    text = normalizer(text)

                if text:
                    item["TEXT"] = text
                    print("\t".join([item[f] for f in reader.fieldnames]), file=ostream)

                ndone += 1
                if ndone % args.log_interval == 0:
                    print(f"text norm: {ndone} lines done.", file=sys.stderr, flush=True)
        else:
            for l in istream:
                key, text = "", ""
                if args.format == "ark":  # KALDI archive, line format: "key text"
                    cols = l.strip().split(maxsplit=1)
                    key, text = cols[0], cols[1] if len(cols) == 2 else ""
                else:
                    text = l.strip()

                if text:
                    text = normalizer(text)

                if text:
                    if args.format == "ark":
                        print(key + "\t" + text, file=ostream)
                    else:
                        print(text, file=ostream)

                ndone += 1
                if ndone % args.log_interval == 0:
                    print(f"text norm: {ndone} lines done.", file=sys.stderr, flush=True)
    print(f"text norm: {ndone} lines done in total.", file=sys.stderr, flush=True)


================================================================================
# File: auralis/models/xttsv2/components/tts/layers/xtts/perceiver_encoder.py
# Path: /home/marco/PycharmProjects/betterVoiceCraft/Auralis/src/auralis/models/xttsv2/components/tts/layers/xtts/perceiver_encoder.py
================================================================================

# Adapted from https://github.com/lucidrains/naturalspeech2-pytorch/blob/659bec7f7543e7747e809e950cc2f84242fbeec7/naturalspeech2_pytorch/naturalspeech2_pytorch.py#L532

from collections import namedtuple
from functools import wraps

import torch
import torch.nn.functional as F
from einops.layers.torch import Rearrange
from torch import nn, einsum
from einops import rearrange, repeat
from functools import wraps
from collections import namedtuple
from packaging import version


def exists(x):
    return x is not None

def default(val, d):
    return val if exists(val) else d() if callable(d) else d

def once(fn):
    called = False

    @wraps(fn)
    def inner(x):
        nonlocal called
        if called:
            return
        called = True
        return fn(x)

    return inner


print_once = once(print)

# main class


class Attend(nn.Module):
    def __init__(self, dropout=0.0, causal=False, use_flash=False):
        super().__init__()
        self.dropout = dropout
        self.attn_dropout = nn.Dropout(dropout)

        self.causal = causal
        self.register_buffer("mask", None, persistent=False)

        self.use_flash = use_flash
        assert not (use_flash and version.parse(torch.__version__) < version.parse("2.0.0")), \
            "For flash attention, pytorch 2.0+ is required"
        self.config = namedtuple("EfficientAttentionConfig", ["enable_flash", "enable_math", "enable_mem_efficient"])
        self.cpu_config = self.config(True, True, True)
        self.cuda_config = None

        if torch.cuda.is_available() and use_flash:
            device_properties = torch.cuda.get_device_properties(torch.device("cuda"))
            if device_properties.major == 8 and device_properties.minor == 0:
                print_once("A100 GPU detected, using flash attention if input tensor is on cuda")
                self.cuda_config = self.config(True, False, False)
            else:
                print_once("Non-A100 GPU detected, using math or mem efficient attention if input tensor is on cuda")
                self.cuda_config = self.config(False, True, True)

    def get_mask(self, n, device):
        if exists(self.mask) and self.mask.shape[-1] >= n:
            return self.mask[:n, :n]

        mask = torch.ones((n, n), device=device, dtype=torch.bool).triu(1)
        self.register_buffer("mask", mask, persistent=False)
        return mask

    def flash_attn(self, q, k, v, mask=None):
        with torch.inference_mode():
            _, heads, q_len, _, k_len, is_cuda = *q.shape, k.shape[-2], q.is_cuda

            if k.ndim == 3:
                k = rearrange(k, "b ... -> b 1 ...").expand_as(q)
            if v.ndim == 3:
                v = rearrange(v, "b ... -> b 1 ...").expand_as(q)

            if exists(mask):
                mask = rearrange(mask, "b j -> b 1 1 j").expand(-1, heads, q_len, -1)

            config = self.cuda_config if is_cuda else self.cpu_config
            with torch.backends.cuda.sdp_kernel(**config._asdict()):
                out = F.scaled_dot_product_attention(
                    q, k, v, attn_mask=mask,
                    dropout_p=self.dropout if self.training else 0.0,
                    is_causal=self.causal
                )
            return out

    def forward(self, q, k, v, mask=None):
        with torch.inference_mode():
            n, device = q.shape[-2], q.device
            scale = q.shape[-1] ** -0.5

            if self.use_flash:
                out = self.flash_attn(q, k, v, mask=mask)
                return out

            kv_einsum_eq = "b j d" if k.ndim == 3 else "b h j d"
            sim = einsum(f"b h i d, {kv_einsum_eq} -> b h i j", q, k) * scale

            if exists(mask):
                mask = rearrange(mask, "b j -> b 1 1 j")
                sim = sim.masked_fill(~mask, -torch.finfo(sim.dtype).max)
                del mask

            if self.causal:
                causal_mask = self.get_mask(n, device)
                sim = sim.masked_fill(causal_mask, -torch.finfo(sim.dtype).max)
                del causal_mask

            attn = sim.softmax(dim=-1)
            attn = self.attn_dropout(attn)
            del sim

            out = einsum(f"b h i j, {kv_einsum_eq} -> b h i d", attn, v)
            del attn
            return out


def Sequential(*mods):
    return nn.Sequential(*filter(exists, mods))


class RMSNorm(nn.Module):
    def __init__(self, dim, scale=True, dim_cond=None):
        super().__init__()
        self.cond = exists(dim_cond)
        self.to_gamma_beta = nn.Linear(dim_cond, dim * 2) if self.cond else None

        self.scale = dim**0.5
        self.gamma = nn.Parameter(torch.ones(dim)) if scale else None

    def forward(self, x, cond=None):
        with torch.inference_mode():
            gamma = self.gamma if exists(self.gamma) else 1
            out = F.normalize(x, dim=-1) * self.scale * gamma
            if not self.cond:
                return out
            assert exists(cond)
            gamma, beta = self.to_gamma_beta(cond).chunk(2, dim=-1)
            gamma, beta = map(lambda t: rearrange(t, "b d -> b 1 d"), (gamma, beta))
            out = out * gamma + beta
            del gamma, beta
            return out


class CausalConv1d(nn.Conv1d):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        (kernel_size,) = self.kernel_size
        (dilation,) = self.dilation
        (stride,) = self.stride

        assert stride == 1
        self.causal_padding = dilation * (kernel_size - 1)

    def forward(self, x):
        with torch.inference_mode():
            causal_padded_x = F.pad(x, (self.causal_padding, 0), value=0.0)
            out = super().forward(causal_padded_x)
            del causal_padded_x
            return out


class GEGLU(nn.Module):
    def forward(self, x):
        with torch.inference_mode():
            x, gate = x.chunk(2, dim=-1)
            out = F.gelu(gate) * x
            del gate
            return out


def FeedForward(dim, mult=4, causal_conv=False):
    dim_inner = int(dim * mult * 2 / 3)

    conv = None
    if causal_conv:
        conv = nn.Sequential(
            Rearrange("b n d -> b d n"),
            CausalConv1d(dim_inner, dim_inner, 3),
            Rearrange("b d n -> b n d"),
        )

    return Sequential(nn.Linear(dim, dim_inner * 2), GEGLU(), conv, nn.Linear(dim_inner, dim))


class Attention(nn.Module):
    def __init__(
        self,
        dim,
        *,
        dim_context=None,
        causal=False,
        dim_head=64,
        heads=8,
        dropout=0.0,
        use_flash=False,
        cross_attn_include_queries=False,
    ):
        super().__init__()
        self.scale = dim_head**-0.5
        self.heads = heads
        self.cross_attn_include_queries = cross_attn_include_queries
        dim_inner = dim_head * heads
        dim_context = default(dim_context, dim)

        self.attend = Attend(causal=causal, dropout=dropout, use_flash=use_flash)
        self.to_q = nn.Linear(dim, dim_inner, bias=False)
        self.to_kv = nn.Linear(dim_context, dim_inner * 2, bias=False)
        self.to_out = nn.Linear(dim_inner, dim, bias=False)

    def forward(self, x, context=None, mask=None):
        with torch.inference_mode():
            h, has_context = self.heads, exists(context)
            context = default(context, x)
            if has_context and self.cross_attn_include_queries:
                context = torch.cat((x, context), dim=-2)

            q = self.to_q(x)
            kv = self.to_kv(context)
            k, v = kv.chunk(2, dim=-1)
            del kv
            q, k, v = map(lambda t: rearrange(t, "b n (h d) -> b h n d", h=h), (q, k, v))

            out = self.attend(q, k, v, mask=mask)
            del q, k, v
            out = rearrange(out, "b h n d -> b n (h d)")
            out = self.to_out(out)
            return out


class PerceiverResampler(nn.Module):
    def __init__(
        self,
        *,
        dim,
        depth=2,
        dim_context=None,
        num_latents=32,
        dim_head=64,
        heads=8,
        ff_mult=4,
        use_flash_attn=False,
    ):
        super().__init__()
        dim_context = default(dim_context, dim)

        self.proj_context = nn.Linear(dim_context, dim) if dim_context != dim else nn.Identity()

        self.latents = nn.Parameter(torch.randn(num_latents, dim))
        nn.init.normal_(self.latents, std=0.02)

        self.layers = nn.ModuleList([])
        for _ in range(depth):
            self.layers.append(
                nn.ModuleList(
                    [
                        Attention(
                            dim=dim,
                            dim_head=dim_head,
                            heads=heads,
                            use_flash=use_flash_attn,
                            cross_attn_include_queries=True,
                        ),
                        FeedForward(dim=dim, mult=ff_mult),
                    ]
                )
            )

        self.norm = RMSNorm(dim)

    def forward(self, x, mask=None):
        with torch.inference_mode():
            batch = x.shape[0]
            x = self.proj_context(x)
            latents = repeat(self.latents, "n d -> b n d", b=batch)

            for attn, ff in self.layers:
                lat_out = attn(latents, x, mask=mask)
                latents = lat_out.add_(latents)
                ff_out = ff(latents)
                latents = ff_out.add_(latents)
                del lat_out, ff_out

            out = self.norm(latents)
            del latents
            return out


================================================================================
# File: auralis/models/xttsv2/components/tts/layers/xtts/latent_encoder.py
# Path: /home/marco/PycharmProjects/betterVoiceCraft/Auralis/src/auralis/models/xttsv2/components/tts/layers/xtts/latent_encoder.py
================================================================================

# ported from: Originally ported from: https://github.com/neonbjb/tortoise-tts

import math

import torch
from torch import nn
from torch.nn import functional as F


class GroupNorm32(nn.GroupNorm):
    def forward(self, x):
        with torch.inference_mode():
            out = super().forward(x.float()).type(x.dtype)
            return out

def conv_nd(dims, *args, **kwargs):
    if dims == 1:
        return nn.Conv1d(*args, **kwargs)
    elif dims == 2:
        return nn.Conv2d(*args, **kwargs)
    elif dims == 3:
        return nn.Conv3d(*args, **kwargs)
    raise ValueError(f"unsupported dimensions: {dims}")


def normalization(channels):
    groups = 32
    if channels <= 16:
        groups = 8
    elif channels <= 64:
        groups = 16
    while channels % groups != 0:
        groups = groups // 2
    assert groups > 2
    return GroupNorm32(groups, channels)


def zero_module(module):
    for p in module.parameters():
        p.detach().zero_()
    return module


class QKVAttention(nn.Module):
    def __init__(self, n_heads):
        super().__init__()
        self.n_heads = n_heads

    def forward(self, qkv, mask=None, qk_bias=0):
        """
        Apply QKV attention.

        :param qkv: an [N x (H * 3 * C) x T] tensor of Qs, Ks, and Vs.
        :return: an [N x (H * C) x T] tensor after attention.
        """
        with torch.inference_mode():
            bs, width, length = qkv.shape
            assert width % (3 * self.n_heads) == 0
            ch = width // (3 * self.n_heads)
            q, k, v = qkv.reshape(bs * self.n_heads, ch * 3, length).split(ch, dim=1)

            scale = 1 / math.sqrt(math.sqrt(ch))
            weight = torch.einsum("bct,bcs->bts", q * scale, k * scale)
            weight.add_(qk_bias)
            del q, k

            if mask is not None:
                mask = mask.repeat(self.n_heads, 1, 1)
                weight.masked_fill_(mask.logical_not(), -torch.inf)
                del mask

            weight = torch.softmax(weight.float(), dim=-1).type(weight.dtype)
            a = torch.einsum("bts,bcs->bct", weight, v)
            del weight, v

            return a.reshape(bs, -1, length)


class AttentionBlock(nn.Module):
    """An attention block that allows spatial positions to attend to each other."""

    def __init__(
        self,
        channels,
        num_heads=1,
        num_head_channels=-1,
        out_channels=None,
        do_activation=False,
    ):
        super().__init__()
        self.channels = channels
        out_channels = channels if out_channels is None else out_channels
        self.do_activation = do_activation
        if num_head_channels == -1:
            self.num_heads = num_heads
        else:
            assert (
                channels % num_head_channels == 0
            ), f"q,k,v channels {channels} is not divisible by num_head_channels {num_head_channels}"
            self.num_heads = channels // num_head_channels
        self.norm = normalization(channels)
        self.qkv = conv_nd(1, channels, out_channels * 3, 1)
        self.attention = QKVAttention(self.num_heads)

        self.x_proj = nn.Identity() if out_channels == channels else conv_nd(1, channels, out_channels, 1)
        self.proj_out = zero_module(conv_nd(1, out_channels, out_channels, 1))

    def forward(self, x, mask=None, qk_bias=0):
        with torch.inference_mode():
            b, c, *spatial = x.shape
            if mask is not None:
                if len(mask.shape) == 2:
                    mask = mask.unsqueeze(0).expand(b, -1, -1)
                if mask.shape[1] != x.shape[-1]:
                    mask = mask[:, : x.shape[-1], : x.shape[-1]]

            x_in = x
            x = x.reshape(b, c, -1)
            x = self.norm(x)
            if self.do_activation:
                x = F.silu(x)

            qkv = self.qkv(x)
            attn_out = self.attention(qkv, mask=mask, qk_bias=qk_bias)
            del qkv
            h = self.proj_out(attn_out)
            del attn_out
            xp = self.x_proj(x)
            out = xp.add_(h)
            del xp, h, x

            out = out.reshape(b, out.shape[1], *spatial)
            return out


class ConditioningEncoder(nn.Module):
    def __init__(
        self,
        spec_dim,
        embedding_dim,
        attn_blocks=6,
        num_attn_heads=4,
    ):
        super().__init__()
        attn = []
        self.init = nn.Conv1d(spec_dim, embedding_dim, kernel_size=1)
        for a in range(attn_blocks):
            attn.append(AttentionBlock(embedding_dim, num_attn_heads))
        self.attn = nn.Sequential(*attn)
        self.dim = embedding_dim

    def forward(self, x):
        with torch.inference_mode():
            # x: (b, 80, s)
            h = self.init(x)
            h = self.attn(h)
            return h


================================================================================
# File: auralis/models/xttsv2/components/tts/layers/xtts/hifigan_decoder.py
# Path: /home/marco/PycharmProjects/betterVoiceCraft/Auralis/src/auralis/models/xttsv2/components/tts/layers/xtts/hifigan_decoder.py
================================================================================

import torch
import torchaudio
from torch import nn
from torch.nn import Conv1d, ConvTranspose1d
from torch.nn import functional as F
from torch.nn.utils.parametrizations import weight_norm
from torch.nn.utils.parametrize import remove_parametrizations

from .......common.utilities import load_fsspec

LRELU_SLOPE = 0.1


def get_padding(k, d):
    return int((k * d - d) / 2)


class ResBlock1(torch.nn.Module):
    """Residual Block Type 1. It has 3 convolutional layers in each convolutional block.

    Network::

        x -> lrelu -> conv1_1 -> conv1_2 -> conv1_3 -> z -> lrelu -> conv2_1 -> conv2_2 -> conv2_3 -> o -> + -> o
        |--------------------------------------------------------------------------------------------------|


    Args:
        channels (int): number of hidden channels for the convolutional layers.
        kernel_size (int): size of the convolution filter in each layer.
        dilations (list): list of dilation value for each conv layer in a block.
    """

    def __init__(self, channels, kernel_size=3, dilation=(1, 3, 5)):
        super().__init__()
        self.convs1 = nn.ModuleList(
            [
                weight_norm(
                    Conv1d(
                        channels,
                        channels,
                        kernel_size,
                        1,
                        dilation=dilation[i],
                        padding=get_padding(kernel_size, dilation[i]),
                    )
                )
                for i in range(3)
            ]
        )

        self.convs2 = nn.ModuleList(
            [
                weight_norm(
                    Conv1d(
                        channels,
                        channels,
                        kernel_size,
                        1,
                        dilation=1,
                        padding=get_padding(kernel_size, 1),
                    )
                )
                for _ in range(3)
            ]
        )

    def forward(self, x):
        """
        Args:
            x (Tensor): input tensor.
        Returns:
            Tensor: output tensor.
        Shapes:
            x: [B, C, T]
        """
        for c1, c2 in zip(self.convs1, self.convs2):
            xt = F.leaky_relu(x, LRELU_SLOPE)
            xt = c1(xt)
            xt = F.leaky_relu(xt, LRELU_SLOPE)
            xt = c2(xt)
            x = x.add_(xt)  # in place
            del xt
        return x

    def remove_weight_norm(self):
        for l in self.convs1:
            remove_parametrizations(l, "weight")
        for l in self.convs2:
            remove_parametrizations(l, "weight")


class ResBlock2(torch.nn.Module):
    """Residual Block Type 2. It has 1 convolutional layers in each convolutional block.

    Network::

        x -> lrelu -> conv1-> -> z -> lrelu -> conv2-> o -> + -> o
        |---------------------------------------------------|


    Args:
        channels (int): number of hidden channels for the convolutional layers.
        kernel_size (int): size of the convolution filter in each layer.
        dilations (list): list of dilation value for each conv layer in a block.
    """

    def __init__(self, channels, kernel_size=3, dilation=(1, 3)):
        super().__init__()
        self.convs = nn.ModuleList(
            [
                weight_norm(
                    Conv1d(
                        channels,
                        channels,
                        kernel_size,
                        1,
                        dilation=dilation[i],
                        padding=get_padding(kernel_size, dilation[i]),
                    )
                )
                for i in range(2)
            ]
        )

    def forward(self, x):
        with torch.inference_mode():
            for c in self.convs:
                xt = F.leaky_relu(x, LRELU_SLOPE)
                xt = c(xt)
                x = x.add_(xt) # in place
                del xt
            return x

    def remove_weight_norm(self):
        for l in self.convs:
            remove_parametrizations(l, "weight")


class HifiganGenerator(torch.nn.Module):
    def __init__(
        self,
        in_channels,
        out_channels,
        resblock_type,
        resblock_dilation_sizes,
        resblock_kernel_sizes,
        upsample_kernel_sizes,
        upsample_initial_channel,
        upsample_factors,
        inference_padding=5,
        cond_channels=0,
        conv_pre_weight_norm=True,
        conv_post_weight_norm=True,
        conv_post_bias=True,
        cond_in_each_up_layer=False,
    ):
        r"""HiFiGAN Generator with Multi-Receptive Field Fusion (MRF)

        Network:
            x -> lrelu -> upsampling_layer -> resblock1_k1x1 -> z1 -> + -> z_sum / #resblocks -> lrelu -> conv_post_7x1 -> tanh -> o
                                                 ..          -> zI ---|
                                              resblockN_kNx1 -> zN ---'

        Args:
            in_channels (int): number of input tensor channels.
            out_channels (int): number of output tensor channels.
            resblock_type (str): type of the `ResBlock`. '1' or '2'.
            resblock_dilation_sizes (List[List[int]]): list of dilation values in each layer of a `ResBlock`.
            resblock_kernel_sizes (List[int]): list of kernel sizes for each `ResBlock`.
            upsample_kernel_sizes (List[int]): list of kernel sizes for each transposed convolution.
            upsample_initial_channel (int): number of channels for the first upsampling layer. This is divided by 2
                for each consecutive upsampling layer.
            upsample_factors (List[int]): upsampling factors (stride) for each upsampling layer.
            inference_padding (int): constant padding applied to the input at inference time. Defaults to 5.
        """
        super().__init__()
        self.inference_padding = inference_padding
        self.num_kernels = len(resblock_kernel_sizes)
        self.num_upsamples = len(upsample_factors)
        self.cond_in_each_up_layer = cond_in_each_up_layer

        # initial upsampling layers
        self.conv_pre = weight_norm(Conv1d(in_channels, upsample_initial_channel, 7, 1, padding=3))
        resblock = ResBlock1 if resblock_type == "1" else ResBlock2
        # upsampling layers
        self.ups = nn.ModuleList()
        for i, (u, k) in enumerate(zip(upsample_factors, upsample_kernel_sizes)):
            self.ups.append(
                weight_norm(
                    ConvTranspose1d(
                        upsample_initial_channel // (2**i),
                        upsample_initial_channel // (2 ** (i + 1)),
                        k,
                        u,
                        padding=(k - u) // 2,
                    )
                )
            )
        # MRF blocks
        self.resblocks = nn.ModuleList()
        for i in range(len(self.ups)):
            ch = upsample_initial_channel // (2 ** (i + 1))
            for k, d in zip(resblock_kernel_sizes, resblock_dilation_sizes):
                self.resblocks.append(resblock(ch, k, d))
        # post convolution layer
        self.conv_post = weight_norm(Conv1d(ch, out_channels, 7, 1, padding=3, bias=conv_post_bias))
        if cond_channels > 0:
            self.cond_layer = nn.Conv1d(cond_channels, upsample_initial_channel, 1)

        if not conv_pre_weight_norm:
            remove_parametrizations(self.conv_pre, "weight")

        if not conv_post_weight_norm:
            remove_parametrizations(self.conv_post, "weight")

        if self.cond_in_each_up_layer:
            self.conds = nn.ModuleList()
            for i in range(len(self.ups)):
                ch = upsample_initial_channel // (2 ** (i + 1))
                self.conds.append(nn.Conv1d(cond_channels, ch, 1))

    def forward(self, x, g=None):
        """
        Args:
            x (Tensor): feature input tensor.
            g (Tensor): global conditioning input tensor.

        Returns:
            Tensor: output waveform.

        Shapes:
            x: [B, C, T]
            Tensor: [B, 1, T]
        """
        with torch.inference_mode():
            with torch.no_grad():
                with torch.amp.autocast(device_type='cuda', dtype=torch.float16):
                    if x.dim() == 2:
                        x = x.unsqueeze(0)
                    x = self.conv_pre(x)
                    if hasattr(self, "cond_layer"):
                        x.add_(self.cond_layer(g))

                    for i in range(self.num_upsamples):
                        x = F.leaky_relu(x, LRELU_SLOPE, inplace=True)
                        x = self.ups[i](x)

                        if self.cond_in_each_up_layer:
                            x.add_(self.conds[i](g))

                        z_sum = 0
                        for j in range(self.num_kernels):
                            rb_out = self.resblocks[i * self.num_kernels + j](x)
                            # rb_out Ã¨ un riferimento, non serve del immediato qui
                            z_sum += rb_out.float()
                            del rb_out
                        # Ora normalizziamo
                        x = z_sum / self.num_kernels
                        del z_sum
                    x = F.leaky_relu(x, inplace=True)
                    x = self.conv_post(x)
                    x = torch.tanh(x)
                    return x

    @torch.no_grad()
    def inference(self, c):
        """
        Args:
            x (Tensor): conditioning input tensor.

        Returns:
            Tensor: output waveform.

        Shapes:
            x: [B, C, T]
            Tensor: [B, 1, T]
        """
        c = c.to(self.conv_pre.weight.device)
        c = F.pad(c, (self.inference_padding, self.inference_padding), "replicate")
        return self.forward(c)

    def remove_weight_norm(self):
        print("Removing weight norm...")
        for l in self.ups:
            remove_parametrizations(l, "weight")
        for l in self.resblocks:
            l.remove_weight_norm()
        remove_parametrizations(self.conv_pre, "weight")
        remove_parametrizations(self.conv_post, "weight")

    def load_checkpoint(self, config, checkpoint_path, eval=False, cache=False):
        state = torch.load(checkpoint_path, map_location=torch.device("cpu"))
        self.load_state_dict(state["model"])
        if eval:
            self.eval()
            assert not self.training
            self.remove_weight_norm()


class SELayer(nn.Module):
    def __init__(self, channel, reduction=8):
        super(SELayer, self).__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.fc = nn.Sequential(
            nn.Linear(channel, channel // reduction),
            nn.ReLU(inplace=True),
            nn.Linear(channel // reduction, channel),
            nn.Sigmoid(),
        )

    def forward(self, x):
        y = self.avg_pool(x).view(x.size(0), x.size(1))
        y = self.fc(y).view(x.size(0), x.size(1), 1, 1)
        x = x * y
        return x


class SEBasicBlock(nn.Module):
    expansion = 1

    def __init__(self, inplanes, planes, stride=1, downsample=None, reduction=8):
        super(SEBasicBlock, self).__init__()
        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=3, stride=stride, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(planes)
        self.relu = nn.ReLU(inplace=True)
        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(planes)
        self.se = SELayer(planes, reduction)
        self.downsample = downsample

    def forward(self, x):
        residual = x

        x = self.conv1(x)
        x = self.relu(x)
        x = self.bn1(x)

        x = self.conv2(x)
        x = self.bn2(x)
        x = self.se(x)

        if self.downsample is not None:
            residual = self.downsample(residual)

        x += residual
        x = self.relu(x)
        return x


def set_init_dict(model_dict, checkpoint_state, c):
    # Partial initialization: if there is a mismatch with new and old layer, it is skipped.
    for k, v in checkpoint_state.items():
        if k not in model_dict:
            print(f" | > Layer missing in the model definition: {k}")
    pretrained_dict = {k: v for k, v in checkpoint_state.items() if k in model_dict}
    # 2. filter out different size layers
    pretrained_dict = {k: v for k, v in pretrained_dict.items() if v.numel() == model_dict[k].numel()}
    # 3. skip reinit layers
    if c.has("reinit_layers") and c.reinit_layers is not None:
        for reinit_layer_name in c.reinit_layers:
            pretrained_dict = {k: v for k, v in pretrained_dict.items() if reinit_layer_name not in k}
    # 4. overwrite entries in the existing state dict
    model_dict.update(pretrained_dict)
    print(f" | > {len(pretrained_dict)} / {len(model_dict)} layers are restored.")
    return model_dict


class PreEmphasis(nn.Module):
    def __init__(self, coefficient=0.97):
        super().__init__()
        self.coefficient = coefficient
        self.register_buffer("filter", torch.tensor([-self.coefficient, 1.0], dtype=torch.float32).view(1, 1, -1))

    def forward(self, x):
        assert len(x.size()) == 2

        x = torch.nn.functional.pad(x.unsqueeze(1), (1, 0), "reflect")
        x = torch.nn.functional.conv1d(x, self.filter).squeeze(1)
        return x


class ResNetSpeakerEncoder(nn.Module):
    """This is copied from ð¸TTS to remove it from the dependencies."""

    # pylint: disable=W0102
    def __init__(
        self,
        input_dim=64,
        proj_dim=512,
        layers=[3, 4, 6, 3],
        num_filters=[32, 64, 128, 256],
        encoder_type="ASP",
        log_input=False,
        use_torch_spec=False,
        audio_config=None,
    ):
        super(ResNetSpeakerEncoder, self).__init__()

        self.encoder_type = encoder_type
        self.input_dim = input_dim
        self.log_input = log_input
        self.use_torch_spec = use_torch_spec
        self.audio_config = audio_config
        self.proj_dim = proj_dim

        self.conv1 = nn.Conv2d(1, num_filters[0], kernel_size=3, stride=1, padding=1)
        self.relu = nn.ReLU(inplace=True)
        self.bn1 = nn.BatchNorm2d(num_filters[0])

        self.inplanes = num_filters[0]
        self.layer1 = self.create_layer(SEBasicBlock, num_filters[0], layers[0])
        self.layer2 = self.create_layer(SEBasicBlock, num_filters[1], layers[1], stride=(2, 2))
        self.layer3 = self.create_layer(SEBasicBlock, num_filters[2], layers[2], stride=(2, 2))
        self.layer4 = self.create_layer(SEBasicBlock, num_filters[3], layers[3], stride=(2, 2))

        self.instancenorm = nn.InstanceNorm1d(input_dim)

        if self.use_torch_spec:
            self.torch_spec = torch.nn.Sequential(
                PreEmphasis(audio_config["preemphasis"]),
                torchaudio.transforms.MelSpectrogram(
                    sample_rate=audio_config["sample_rate"],
                    n_fft=audio_config["fft_size"],
                    win_length=audio_config["win_length"],
                    hop_length=audio_config["hop_length"],
                    window_fn=torch.hamming_window,
                    n_mels=audio_config["num_mels"],
                ),
            )

        else:
            self.torch_spec = None

        outmap_size = int(self.input_dim / 8)

        self.attention = nn.Sequential(
            nn.Conv1d(num_filters[3] * outmap_size, 128, kernel_size=1),
            nn.ReLU(inplace=True),
            nn.BatchNorm1d(128),
            nn.Conv1d(128, num_filters[3] * outmap_size, kernel_size=1),
            nn.Softmax(dim=2),
        )

        if self.encoder_type == "SAP":
            out_dim = num_filters[3] * outmap_size
        elif self.encoder_type == "ASP":
            out_dim = num_filters[3] * outmap_size * 2
        else:
            raise ValueError("Undefined encoder")

        self.fc = nn.Linear(out_dim, proj_dim)

        self._init_layers()

    def _init_layers(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode="fan_out", nonlinearity="relu")
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)

    def create_layer(self, block, planes, blocks, stride=1):
        downsample = None
        if stride != 1 or self.inplanes != planes * block.expansion:
            downsample = nn.Sequential(
                nn.Conv2d(self.inplanes, planes * block.expansion, kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(planes * block.expansion),
            )

        layers = [block(self.inplanes, planes, stride, downsample)]
        self.inplanes = planes * block.expansion
        layers.extend(block(self.inplanes, planes) for _ in range(1, blocks))

        return nn.Sequential(*layers)

    # pylint: disable=R0201
    def new_parameter(self, *size):
        out = nn.Parameter(torch.FloatTensor(*size))
        nn.init.xavier_normal_(out)
        return out

    def forward(self, x, l2_norm=False):
        """Forward pass of the model.

        Args:
            x (Tensor): Raw waveform signal or spectrogram frames. If input is a waveform, `torch_spec` must be `True`
                to compute the spectrogram on-the-fly.
            l2_norm (bool): Whether to L2-normalize the outputs.

        Shapes:
            - x: :math:`(N, 1, T_{in})` or :math:`(N, D_{spec}, T_{in})`
        """
        x.squeeze_(1)
        # if you torch spec compute it otherwise use the mel spec computed by the AP
        if self.use_torch_spec:
            x = self.torch_spec(x)

        if self.log_input:
            x.add_(1e-6).log_()
        x = self.instancenorm(x).unsqueeze(1)

        x = self.conv1(x)
        x = self.relu(x)
        x = self.bn1(x)

        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.layer4(x)

        x = x.reshape(x.size(0), -1, x.size(-1))

        w = self.attention(x)

        if self.encoder_type == "SAP":
            x = torch.sum(x * w, dim=2)
        elif self.encoder_type == "ASP":
            mu = torch.sum(x * w, dim=2)
            sg = torch.sqrt((torch.sum((x ** 2) * w, dim=2) - mu ** 2).clamp(min=1e-5))
            x = torch.cat((mu, sg), 1)

        x = x.view(x.size()[0], -1)
        x = self.fc(x)

        if l2_norm:
            x = torch.nn.functional.normalize(x, p=2, dim=1)
        return x

    def load_checkpoint(
        self,
        checkpoint_path: str,
        eval: bool = False,
        use_cuda: bool = False,
        criterion=None,
        cache=False,
    ):
        state = load_fsspec(checkpoint_path, map_location=torch.device("cpu"), cache=cache)
        try:
            self.load_state_dict(state["model"])
            print(" > Model fully restored. ")
        except (KeyError, RuntimeError) as error:
            # If eval raise the error
            if eval:
                raise error

            print(" > Partial model initialization.")
            model_dict = self.state_dict()
            model_dict = set_init_dict(model_dict, state["model"])
            self.load_state_dict(model_dict)
            del model_dict

        # load the criterion for restore_path
        if criterion is not None and "criterion" in state:
            try:
                criterion.load_state_dict(state["criterion"])
            except (KeyError, RuntimeError) as error:
                print(" > Criterion load ignored because of:", error)

        if use_cuda:
            self.cuda()
            if criterion is not None:
                criterion = criterion.cuda()

        if eval:
            self.eval()
            assert not self.training

        if not eval:
            return criterion, state["step"]
        return criterion


class HifiDecoder(torch.nn.Module):
    def __init__(
        self,
        input_sample_rate=22050,
        output_sample_rate=24000,
        output_hop_length=256,
        ar_mel_length_compression=1024,
        decoder_input_dim=1024,
        resblock_type_decoder="1",
        resblock_dilation_sizes_decoder=[[1, 3, 5], [1, 3, 5], [1, 3, 5]],
        resblock_kernel_sizes_decoder=[3, 7, 11],
        upsample_rates_decoder=[8, 8, 2, 2],
        upsample_initial_channel_decoder=512,
        upsample_kernel_sizes_decoder=[16, 16, 4, 4],
        d_vector_dim=512,
        cond_d_vector_in_each_upsampling_layer=True,
        speaker_encoder_audio_config={
            "fft_size": 512,
            "win_length": 400,
            "hop_length": 160,
            "sample_rate": 16000,
            "preemphasis": 0.97,
            "num_mels": 64,
        },
    ):
        super().__init__()
        self.input_sample_rate = input_sample_rate
        self.output_sample_rate = output_sample_rate
        self.output_hop_length = output_hop_length
        self.ar_mel_length_compression = ar_mel_length_compression
        self.speaker_encoder_audio_config = speaker_encoder_audio_config
        self.waveform_decoder = HifiganGenerator(
            decoder_input_dim,
            1,
            resblock_type_decoder,
            resblock_dilation_sizes_decoder,
            resblock_kernel_sizes_decoder,
            upsample_kernel_sizes_decoder,
            upsample_initial_channel_decoder,
            upsample_rates_decoder,
            inference_padding=0,
            cond_channels=d_vector_dim,
            conv_pre_weight_norm=False,
            conv_post_weight_norm=False,
            conv_post_bias=False,
            cond_in_each_up_layer=cond_d_vector_in_each_upsampling_layer,
        )
        self.speaker_encoder = ResNetSpeakerEncoder(
            input_dim=64,
            proj_dim=512,
            log_input=True,
            use_torch_spec=True,
            audio_config=speaker_encoder_audio_config,
        )

    @property
    def device(self):
        return next(self.parameters()).device

    def forward(self, latents, g=None):
        """
        Args:
            x (Tensor): feature input tensor (GPT latent).
            g (Tensor): global conditioning input tensor.

        Returns:
            Tensor: output waveform.

        Shapes:
            x: [B, C, T]
            Tensor: [B, 1, T]
        """

        with torch.inference_mode():
            # Interpolazione
            z = F.interpolate(
                latents.transpose(1, 2),
                scale_factor=self.ar_mel_length_compression / self.output_hop_length,
                mode="linear",
                align_corners=False,
            )
            z = z.squeeze(1)
            # ulteriore upsampling
            if self.output_sample_rate != self.input_sample_rate:
                z = F.interpolate(
                    z.unsqueeze(0),
                    scale_factor=self.output_sample_rate / self.input_sample_rate,
                    mode="linear",
                    align_corners=False,
                ).squeeze(0)

            o = self.waveform_decoder(z, g=g)
            del z
            return o

    @torch.no_grad()
    def inference(self, c, g):
        """
        Args:
            x (Tensor): feature input tensor (GPT latent).
            g (Tensor): global conditioning input tensor.

        Returns:
            Tensor: output waveform.

        Shapes:
            x: [B, C, T]
            Tensor: [B, 1, T]
        """
        return self.forward(c, g=g)

    def load_checkpoint(self, checkpoint_path, eval=False):  # pylint: disable=unused-argument, redefined-builtin
        state = load_fsspec(checkpoint_path, map_location=torch.device("cpu"))
        # remove unused keys
        state = state["model"]
        for key in list(state.keys()):
            if "waveform_decoder." not in key and "speaker_encoder." not in key:
                del state[key]

        self.load_state_dict(state)
        if eval:
            self.eval()
            assert not self.training
            self.waveform_decoder.remove_weight_norm()


================================================================================
# File: auralis/core/tts.py
# Path: /home/marco/PycharmProjects/betterVoiceCraft/Auralis/src/auralis/core/tts.py
================================================================================

#  Copyright (c) 2024 Astramind. Licensed under Apache License, Version 2.0.
import asyncio
import gc
import json
import logging
import os
import queue
import threading
import uuid
from functools import partial
from typing import AsyncGenerator, Optional, Dict, Union, Generator, List

import objgraph
import torch

from auralis.common.scheduling.orchestrator import Orchestrator
from huggingface_hub import hf_hub_download

from auralis.common.definitions.dto.output import TTSOutput
from auralis.common.definitions.dto.requests import TTSRequest
from auralis.common.logging.logger import setup_logger, set_vllm_logging_level
from auralis.common.metrics.performance import track_generation
from auralis.models.base import BaseAsyncTTSEngine

logger = setup_logger(__file__)

class TTS:
    def __init__(self, scheduler_max_concurrency: int = None, vllm_logging_level=logging.DEBUG):
        """
        Initialize the TTS object, which is the main entry point to the entire library.

        Parameters
        ----------
        scheduler_max_concurrency: int
            (DEPRECATED) The number of concurrent requests that can be processed by the scheduler.
            Please pass it to the method `from_pretrained` instead.
        vllm_logging_level: int
            The logging level for the VLLM model.
        """
        set_vllm_logging_level(vllm_logging_level)

        self.orchestrator: Optional[Orchestrator] = None
        self.tts_engine: Optional[BaseAsyncTTSEngine] = None
        if scheduler_max_concurrency is not None:
            logger.warning("scheduler_max_concurrency passed as a TTS argument is deprecated and will be removed "
                           "in future releases, please pass it to the method from_pretrained")
        self.concurrency = scheduler_max_concurrency # kept for backwards compatibility
        self.logger = setup_logger(__file__)
        self._async = None

    @staticmethod
    def _split_requests(request: TTSRequest, max_length: int = 100000) -> List[TTSRequest]:
        """Split a request into multiple chunks."""
        if len(request.text) <= max_length:
            return [request]

        text_chunks = [request.text[i:i + max_length]
                      for i in range(0, len(request.text), max_length)]

        return [
            (copy := request.copy(), setattr(copy, 'text', chunk), setattr(copy, 'request_id', uuid.uuid4().hex))[0]
            for chunk in text_chunks
        ]

    def _start_orchestrator(self):
        """Starts the orchestrator for request scheduling."""
        self.orchestrator = Orchestrator(self.tts_engine)

    def _non_streaming_sync_wrapper(self, requests):
        try:
            future = asyncio.to_thread(self._process_requests, requests)
            return future.result()
        except Exception as e:
            logger.error(f"Error in non-streaming sync wrapper: {e}")
            raise

    def _streaming_sync_wrapper(self, requests):
        """Synchronous wrapper for streaming requests."""
        q = queue.Queue()
        try:
            async def process_single_request(request):
                try:
                    async for chunk in self.orchestrator.run(request=request):
                        if isinstance(chunk, torch.Tensor):
                            chunk = chunk.detach()
                        q.put(chunk)
                except Exception as e:
                    q.put(e)

            async def produce():
                try:
                    for sub_request in requests:
                        await process_single_request(sub_request)
                except Exception as e:
                    q.put(e)
                finally:
                    q.put(None)

            future = asyncio.run_coroutine_threadsafe(produce(), self.loop)

            def sync_generator():
                try:
                    while True:
                        item = q.get()
                        if item is None:
                            break
                        if isinstance(item, Exception):
                            raise item
                        yield item
                    future.result()
                except Exception as e:
                    future.cancel()
                    raise e
                finally:
                    # Clean up queue
                    while not q.empty():
                        try:
                            q.get_nowait()
                        except queue.Empty:
                            break
                    torch.cuda.empty_cache()

            return sync_generator()
        except Exception as e:
            logger.error(f"Error in streaming sync wrapper: {e}")
            raise

    async def _process_requests(self, requests):
        """Process requests and combine the results."""
        chunks = []
        try:
            for sub_request in requests:
                async for chunk in self.orchestrator.run(request=sub_request):
                    if isinstance(chunk, torch.Tensor):
                        chunk = chunk.detach()
                    chunks.append(chunk)
            return TTSOutput.combine_outputs(chunks)
        finally:
            chunks.clear()
            torch.cuda.empty_cache()

    async def _process_multiple_requests(self, requests: List[TTSRequest], results: Optional[List] = None) -> Optional[TTSOutput]:
        output_queues = None
        try:
            output_queues = [asyncio.Queue() for _ in requests] if results is not None else None

            async def process_subrequest(idx, sub_request, queue: Optional[asyncio.Queue] = None):
                chunks = []
                try:
                    async for chunk in self.orchestrator.run(request=sub_request):
                        if isinstance(chunk, torch.Tensor):
                            chunk = chunk.detach()
                        chunks.append(chunk)
                        if queue is not None:
                            await queue.put(chunk)
                    if queue is not None:
                        await queue.put(None)
                    return chunks
                finally:
                    chunks.clear()

            tasks = [
                asyncio.create_task(
                    process_subrequest(
                        idx,
                        sub_request,
                        output_queues[idx] if output_queues else None
                    )
                )
                for idx, sub_request in enumerate(requests)
            ]

            if results is not None:
                for idx, queue in enumerate(output_queues):
                    try:
                        while True:
                            chunk = await queue.get()
                            if chunk is None:
                                break
                            results[idx].append(chunk)
                    finally:
                        while not queue.empty():
                            await queue.get()
                return None
            else:
                try:
                    all_chunks = await asyncio.gather(*tasks)
                    complete_audio = [chunk for chunks in all_chunks for chunk in chunks]
                    return TTSOutput.combine_outputs(complete_audio)
                finally:
                    complete_audio.clear()
        finally:
            if output_queues:
                for q in output_queues:
                    while not q.empty():
                        await q.get()
            torch.cuda.empty_cache()

    def from_pretrained(self, model_name_or_path: str, **kwargs):
        """
        Load a pretrained model.

        This method loads a TTS model from a specified path or from the Hugging Face Hub.
        It determines the model type from the configuration file and initializes the
        appropriate model class using the ModelRegistry. It also sets up the scheduler
        concurrency and starts the request orchestrator.

        Args:
            model_name_or_path (str): The path to the model directory or the model identifier
                                       on the Hugging Face Hub.
            **kwargs: Additional keyword arguments to pass to the model's `from_pretrained` method.

        Returns:
            TTS: The initialized TTS object, ready to generate speech.

        Raises:
            ValueError: If the model configuration cannot be loaded or if the model type is
                        not supported.
        """
        from auralis.models.registry import ModelRegistry

        try:
            with open(os.path.join(model_name_or_path, 'config.json'), 'r') as f:
                config = json.load(f)

        except FileNotFoundError:
            try:
                config_path = hf_hub_download(repo_id=model_name_or_path, filename='config.json')
                with open(config_path, 'r') as f:
                    config = json.load(f)

            except Exception as e:
                raise ValueError(f"Could not load model from {model_name_or_path} neither locally or online: {e}")
        if kwargs.get('scheduler_max_concurrency', None) is None:
            kwargs['scheduler_max_concurrency'] =  self.concurrency

        self.tts_engine = ModelRegistry.get_model_class(
            config['model_type']).from_pretrained(model_name_or_path, **kwargs)

        self.tts_engine.info = ModelRegistry.get_model_info(config['model_type'])

        self._start_orchestrator()

        return self

    async def prepare_for_streaming_generation(self, request: TTSRequest):
        """
        Prepare the TTS engine for streaming generation.

        This method configures the TTS engine with the necessary conditioning
        based on the provided TTSRequest. It retrieves audio conditioning
        data from the speaker files if the configuration requires speaker
        embeddings or GPT-like decoder conditioning.

        Args:
            request: The TTSRequest containing speaker files for audio
                     conditioning.

        Returns:
            A partial function configured with the generation context,
            including the GPT conditional latent and speaker embeddings,
            if applicable.
        """
        try:
            conditioning_config = self.tts_engine.conditioning_config
            if conditioning_config.speaker_embeddings or conditioning_config.gpt_like_decoder_conditioning:
                gpt_cond_latent, speaker_embeddings = await self.tts_engine.get_audio_conditioning(request.speaker_files)
                if isinstance(gpt_cond_latent, torch.Tensor):
                    gpt_cond_latent = gpt_cond_latent.detach()
                if isinstance(speaker_embeddings, torch.Tensor):
                    speaker_embeddings = speaker_embeddings.detach()
                return partial(self.tts_engine.get_generation_context,
                             gpt_cond_latent=gpt_cond_latent,
                             speaker_embeddings=speaker_embeddings)
        except Exception as e:
            logger.error(f"Error in prepare_for_streaming_generation: {e}")
            raise

    async def generate_speech_async(self, request: TTSRequest) -> Union[AsyncGenerator[TTSOutput, None], TTSOutput]:
        """
        Asynchronous speech generation method.

        This method can be used to generate speech asynchronously. It will split the request
        into multiple subrequests and run them in parallel.

        Args:
            request: The TTSRequest to generate speech for.

        Returns:
            A generator of TTSOutput instances if `request.stream` is `True`, otherwise a single
            TTSOutput instance.
        """
        if self._async == False:
            raise RuntimeError("This instance was not created for async generation.")

        self._async = True

        async def process_chunks():
            """Process chunks and yield them as they are generated."""
            chunks = []
            try:
                async for chunk in self.orchestrator.run(request=request):
                    if request.stream:
                        yield chunk
                    else:
                        chunks.append(chunk)

                if not request.stream:
                    yield TTSOutput.combine_outputs(chunks)
            except Exception as e:
                self.logger.error(f"Error during speech generation: {e}")
                raise
            finally:
                chunks.clear()
                torch.cuda.empty_cache()

        if request.stream:
            return process_chunks()
        else:
            async for result in process_chunks():
                return result

    def generate_speech(self, request: TTSRequest) -> Union[Generator[TTSOutput, None, None], TTSOutput]:
        """
        Synchronous speech generation method.

        This method can be used to generate speech synchronously. It will split the request
        into multiple subrequests and run them in parallel.

        Args:
            request: The TTSRequest to generate speech for.

        Returns:
            A generator of TTSOutput instances if `request.stream` is `True`, otherwise a single
            TTSOutput instance.
        """
        if self._async == True:
            raise RuntimeError("This instance was created for async generation.")

        self._async = False
        requests = self._split_requests(request)

        try:
            if request.stream:
                return self._streaming_sync_wrapper(requests)
            else:
                return self._non_streaming_sync_wrapper(requests)
        except Exception as e:
            logger.error(f"Error in generate_speech: {e}")
            raise



    async def shutdown(self):
        try:
            if self.orchestrator:
                await self.orchestrator.shutdown()
            if self.tts_engine and hasattr(self.tts_engine, 'shutdown'):
                await self.tts_engine.shutdown()
            self.loop.call_soon_threadsafe(self.loop.stop())
            self.loop_thread.join()
        finally:
            self.orchestrator = None
            self.tts_engine = None
            torch.cuda.empty_cache()


================================================================================
# File: auralis/core/__init__.py
# Path: /home/marco/PycharmProjects/betterVoiceCraft/Auralis/src/auralis/core/__init__.py
================================================================================

#  Copyright (c) 2024 Astramind. Licensed under Apache License, Version 2.0.

